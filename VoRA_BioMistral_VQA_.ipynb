{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuwamMKefali/BioMistral-VoRA-VQA-VISION-LANGUAGE-PROJECT-/blob/main/VoRA_BioMistral_VQA_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43d806ef",
      "metadata": {
        "id": "43d806ef"
      },
      "source": [
        "# üëÅÔ∏è **VoRA + BioMistral for Medical VQA**\n",
        "**Author:** Luwam Major Kefali  \n",
        "Supervised by: **Dr. Giacomo Frisoni**\n",
        "             , **Prof. Gianluca Moro**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KIXTZni0a0cs",
      "metadata": {
        "id": "KIXTZni0a0cs"
      },
      "source": [
        "In this notebook, we train a VoRA adapter to inject visual question answering (VQA) capabilities into\n",
        "the biomedical language model BioMistral-7B using the VQA-RAD radiology QA dataset. VoRA (Vision\n",
        "as LoRA) is a novel method proposed by Han et al. (2025) that converts a text-only LLM into a\n",
        "multimodal LLM by integrating visual understanding through Low-Rank Adaptation (LoRA) layers\n",
        ".\n",
        "\n",
        "Unlike conventional multimodal models that attach a separate vision encoder (e.g. a ViT) and a\n",
        "connector module to an LLM, VoRA internalizes vision processing within the LLM via LoRA, preserving\n",
        "the original model‚Äôs architecture and language abilities . BioMistral-7B, introduced by Labrak et al.\n",
        "(2024), is a 7-billion-parameter biomedical LLM based on Mistral that has been further pre-trained on\n",
        "medical literature, achieving strong performance on medical QA tasks . By combining VoRA with\n",
        "BioMistral, we aim to enable the model to answer visual questions about medical images (like chest Xrays)\n",
        "while maintaining its domain language expertise."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ekd65TekaUJO",
      "metadata": {
        "id": "Ekd65TekaUJO"
      },
      "source": [
        "![vora pipeline.JPG](data:image/jpeg;base64,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)\n",
        "\n",
        "Figure from the paper"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HS7-qE-qbOKI",
      "metadata": {
        "id": "HS7-qE-qbOKI"
      },
      "source": [
        "Our use case is a biomedical VQA: given a chest X-ray image and a question (e.g. ‚ÄúWhat abnormality is\n",
        "seen in the left lung?‚Äù), the model should output a helpful answer. We will use the VQA-RAD dataset (Lau\n",
        "et al., 2018), which contains ~3,500 question-answer pairs on ~315 radiology images (mostly X-rays)\n",
        ".\n",
        "\n",
        "This dataset is small, so leveraging pre-trained knowledge is crucial: BioMistral provides medical\n",
        "language understanding , and VoRA‚Äôs ViT distillation injects general visual knowledge, reducing the\n",
        "need for massive vision-language data . By the end of training, our BioMistral+VoRA model should\n",
        "be able to interpret chest X-ray images and answer questions about them, all while running as a single\n",
        "unified model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5PNeAZSL01SN",
      "metadata": {
        "id": "5PNeAZSL01SN"
      },
      "source": [
        "### üîß Setup: Install VoRA and Dependencies\n",
        "\n",
        "We install HuggingFace Transformers, PEFT (for LoRA), and other dependencies.\n",
        "We then clone the official VoRA repo and install it locally in editable mode (`-e`).\n",
        "\n",
        "This sets up everything needed to use the VoRA approach on BioMistral, as described in (Han et al., 2025).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wxafcYvY7F6t",
      "metadata": {
        "id": "wxafcYvY7F6t"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "\n",
        "os.environ[\"PYTORCH_SDP_DISABLE_FLASH_ATTN\"] = \"1\"\n",
        "os.environ[\"PYTORCH_SDP_DISABLE_MEM_EFF_ATTENTION\"] = \"1\"\n",
        "os.environ[\"PYTORCH_SDP_DISABLE_CUDA_SDPA\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46900352",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "46900352",
        "outputId": "8f494ef6-c6fb-41c0-ff9f-ecb0710c1a42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.55.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.17.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.46.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "fatal: destination path 'VoRA' already exists and is not an empty directory.\n",
            "\n",
            "Usage:   \n",
            "  pip3 install [options] <requirement specifier> [package-index-options] ...\n",
            "  pip3 install [options] -r <requirements file> [package-index-options] ...\n",
            "  pip3 install [options] [-e] <vcs project url> ...\n",
            "  pip3 install [options] [-e] <local project path> ...\n",
            "  pip3 install [options] <archive url/path> ...\n",
            "\n",
            "-e option requires 1 argument\n",
            "/content/VoRA\n"
          ]
        }
      ],
      "source": [
        "# Install VoRA and required libraries\n",
        "!pip install -U transformers accelerate peft einops datasets bitsandbytes\n",
        "!git clone https://github.com/Hon-Wong/VoRA.git\n",
        "%pip install -q -e\n",
        "%cd VoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g2WRX-q4bssL",
      "metadata": {
        "id": "g2WRX-q4bssL"
      },
      "source": [
        "After running the above, we see the packages being installed and the VoRA repository cloned.\n",
        "The VoRA repo contains the implementation of the approach described in Han et al. (2025), including\n",
        "model classes and training utilities.\n",
        "\n",
        "\n",
        "*   **VoRA** repository code will give us VoRAForCausalLM- a model wrapper that integrated an LLM with vision embedding and LoRA, as per the VoRA architecture\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vRXgC3XEpwK0",
      "metadata": {
        "id": "vRXgC3XEpwK0"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/VoRA\")\n",
        "print(sys.path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KodAAXaD1D82",
      "metadata": {
        "id": "KodAAXaD1D82"
      },
      "source": [
        "# **üì• Data Preparation: Download VQA-RAD Dataset**\n",
        "\n",
        "We use HuggingFace's `datasets` library to download the VQA-RAD dataset, which contains visual question answering samples on radiology images (Lau et al., 2018), and is split into training and test sets. Each sample includes an image (chest Xray\n",
        "or other radiograph), a question, and an answer (often a short phrase). We will use the training set\n",
        "to fine-tune our model.\n",
        "\n",
        "This will be our training and test dataset for enabling BioMistral+VoRA to understand medical images, we are practically giving \"eyes\" for medical Biomistral.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2845482c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223,
          "referenced_widgets": [
            "fe91fee482cf4dd48db8d6995e3b7393",
            "1da3504f837b44da85683ba108875a52",
            "e8a1027638b0475c8e19cd46e6cb7d81",
            "4d557a816e6347d99330bbcd1577070a",
            "63283258f75c4328899ca59eae50336f",
            "bcd0148fd6a046829f8d9c36814b676e",
            "79aee285671848af8c72d85dff0bd4fa",
            "eff2c2883fea4869b36093f705724de0",
            "ed9fa821cd1b4f6896c79ed25130b238",
            "31c7a4829d824fd5baf67b01e02fd45d",
            "f32c33fe2ec04c0baf8bf7dab08cf38a",
            "a94f4d810ad6497fb2c8880f61192ce5",
            "79092b168d1140e1b136a8eb2c74682d",
            "642a1fafce5e4dc799589be123e6c41c",
            "919ce805855b4b73911931c62378f617",
            "6e8beb4246a14d3699ed1dfaad4b999a",
            "316391425d374cc68549c92800e039be",
            "f68dc7cb5728457dadf8bf8d3192e441",
            "b84934f916df492690f416a1f5822993",
            "a718a2bc3ec745a88c75301cb834b426",
            "8a703158f5cd48268b96afa4b6dbe20a",
            "65c8b5421efc4c45bf41038040a703b8"
          ]
        },
        "id": "2845482c",
        "outputId": "3f9975cf-061a-41ac-c3c4-6a74117e1a4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‚Äòvqa_rad‚Äô: File exists\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/1793 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fe91fee482cf4dd48db8d6995e3b7393"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/451 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a94f4d810ad6497fb2c8880f61192ce5"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Download VQA-RAD dataset\n",
        "from datasets import load_dataset\n",
        "ds = load_dataset(\"flaviagiammarino/vqa-rad\")\n",
        "%mkdir vqa_rad\n",
        "ds.save_to_disk(\"vqa_rad\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NQsL-r7rdtP3",
      "metadata": {
        "id": "NQsL-r7rdtP3"
      },
      "source": [
        "VQA-RAD is a relatively small dataset in the medical domain, which underscores the importance of\n",
        "**transfer learning** for this we rely on BioMistral‚Äôs prior knowledge of medical text and VoRA‚Äôs visual prior\n",
        "injection to make the most of this limited data. Each question in VQA-RAD is directly related to an image\n",
        "(e.g., *‚ÄúAre regions of the brain infarcted?‚Äù* referring to a brain MRI, with answer *‚Äúyes‚Äù*). The questions can\n",
        "be about identifying findings, counting, or verifying the presence of certain anomalies in the image.\n",
        "This variety makes it a good testbed for a multimodal medical model, albeit on a small scale."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "klUosz3x11_3",
      "metadata": {
        "id": "klUosz3x11_3"
      },
      "source": [
        "### üß± Import Core Libraries and VoRA Modules\n",
        "\n",
        "We set up imports for:\n",
        "- HuggingFace tools (tokenizer, model, PEFT LoRA)\n",
        "- Torch and torchvision\n",
        "- Custom VoRA model classes (`VoRAForCausalLM`, `VoRAConfig`)\n",
        "- Utilities for data loading and vision preprocessing\n",
        "\n",
        "We also register the VoRA repo path so we can import its modules.\n",
        "\n",
        "References: (Han et al., 2025)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ab191d7",
      "metadata": {
        "id": "7ab191d7"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import json\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from models.modeling_vora import VoRAForCausalLM\n",
        "from models.configuration_vora import VoRAConfig\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tC9C4TOse61s",
      "metadata": {
        "id": "tC9C4TOse61s"
      },
      "source": [
        "This sets up all dependencies. Notably, `VoRAForCausalLM` and `VoRAConfig` are the core classes\n",
        "from the VoRA implementation: - `VoRAConfig` will hold configuration for the base LLM, vision\n",
        "embedding, LoRA hyperparameters, etc. - `VoRAForCausalLM` is a model class that likely extends\n",
        "Hugging Face‚Äôs PreTrainedModel and internally builds an LLM with integrated LoRA and a vision\n",
        "embedding layer as described in the VoRA paper ."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VhwbGa0919P6",
      "metadata": {
        "id": "VhwbGa0919P6"
      },
      "source": [
        "#‚öôÔ∏è **Configure VoRA + BioMistral-7B**\n",
        "\n",
        "We define the configuration for:\n",
        "- Base LLM (`BioMistral/BioMistral-7B`)\n",
        "- Vision embedding settings\n",
        "- LoRA rank `r=128` (used in transformer layers)\n",
        "\n",
        "This config will be passed to `VoRAForCausalLM` later to build the model.\n",
        "\n",
        "Reference: LoRA + vision settings from (Han et al., 2025)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ytzZMFFxvPDo",
      "metadata": {
        "id": "ytzZMFFxvPDo"
      },
      "outputs": [],
      "source": [
        "# Initialize vora\n",
        "vora_config = VoRAConfig(\n",
        "    llm=\"BioMistral/BioMistral-7B\",\n",
        "    image_size=448,\n",
        "    patch_size=14,\n",
        "    vision_embedding=\"AIMv2Embedding\",\n",
        "    vision_embedding_intermediate_size=768,\n",
        "    vision_attention_mask=\"bidirectional\",\n",
        "    rms_norm_eps=1e-6,\n",
        "    lora={\n",
        "        \"r\": 128,\n",
        "        \"target_modules\": [\"self_attn.q_proj\",\n",
        "                           \"self_attn.k_proj\",\n",
        "                           \"self_attn.v_proj\",\n",
        "                           \"self_attn.o_proj\",\n",
        "                           \"mlp.gate_proj\",\n",
        "                           \"mlp.down_proj\",\n",
        "                           \"mlp.up_proj\"],\n",
        "        \"lora_alpha\": 32,\n",
        "        \"lora_dropout\": 0.1\n",
        "    },\n",
        "    **{\n",
        "        \"skip_aux_cls\": False,\n",
        "        \"reuse_aux_vision_embedding_layers\": \"\",\n",
        "        \"torch_dtype\": \"bfloat16\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i57EKKCIgy-4",
      "metadata": {
        "id": "i57EKKCIgy-4"
      },
      "source": [
        "Overall, this configuration tells VoRA to build a model consisting of BioMistral-7B plus an image\n",
        "embedding module and LoRA adapters for vision. Importantly, **the base BioMistral weights will be\n",
        "kept frozen** (with the LoRA weights being the only trainable parts) . This strategy preserves the\n",
        "language capability of BioMistral (ensuring it doesn‚Äôt ‚Äúunlearn‚Äù medical knowledge), while the LoRA\n",
        "layers learn to interpret visual data and integrate it with the textual context. As Han et al. (2025) note,\n",
        "freezing the LLM and using separate low-rank pathways for vision avoids the modality conflict that can\n",
        "destabilize training when introducing new modalities . And because LoRA adds relatively few\n",
        "parameters compared to the 7B base, the final model remains lightweight, with LoRA merged, it‚Äôs still\n",
        "effectively a 7B model at inference time ."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gWtnuB9RbfrS",
      "metadata": {
        "id": "gWtnuB9RbfrS"
      },
      "source": [
        "## **quantize maybe**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Yz_nlg5KhIhp",
      "metadata": {
        "id": "Yz_nlg5KhIhp"
      },
      "source": [
        "### Hardware check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "o9MckEeSdnXx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9MckEeSdnXx",
        "outputId": "31461947-fc10-4f67-d5f1-5516ec5b7653"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "Current device: 0\n",
            "Device name: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"Current device:\", torch.cuda.current_device())\n",
        "print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PpU1Qf052kx-",
      "metadata": {
        "id": "PpU1Qf052kx-"
      },
      "source": [
        "### Load VoRA Model with BioMistral Backbone\n",
        "\n",
        "We instantiate the VoRA-wrapped BioMistral-7B model using our config.\n",
        "\n",
        "This internally:\n",
        "- Loads BioMistral in 4-bit\n",
        "- Adds LoRA adapters to attention and MLP layers\n",
        "- Prepares vision embedding layers (but image input will be patched externally)\n",
        "- The tokenizer from BioMistral (Mistral-style)\n",
        "- Add `<image>` token for visual placeholder\n",
        "\n",
        "This allows us to insert visual tokens directly into text input during multimodal training.\n",
        "\n",
        "Reference: (Han et al., 2025), (Dettmers et al., 2023 ‚Äì QLoRA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EilMUFbHv0B_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EilMUFbHv0B_",
        "outputId": "d0d02e4d-5aed-4612-ffe2-e00cf74270bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO|tokenization_utils_base.py:2067] 2025-08-05 22:40:39,474 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--BioMistral--BioMistral-7B/snapshots/9a11e1ffa817c211cbb52ee1fb312dc6b61b40a5/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2067] 2025-08-05 22:40:39,474 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--BioMistral--BioMistral-7B/snapshots/9a11e1ffa817c211cbb52ee1fb312dc6b61b40a5/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2067] 2025-08-05 22:40:39,475 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2067] 2025-08-05 22:40:39,475 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--BioMistral--BioMistral-7B/snapshots/9a11e1ffa817c211cbb52ee1fb312dc6b61b40a5/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2067] 2025-08-05 22:40:39,476 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--BioMistral--BioMistral-7B/snapshots/9a11e1ffa817c211cbb52ee1fb312dc6b61b40a5/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2067] 2025-08-05 22:40:39,477 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|configuration_utils.py:752] 2025-08-05 22:40:39,701 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--BioMistral--BioMistral-7B/snapshots/9a11e1ffa817c211cbb52ee1fb312dc6b61b40a5/config.json\n",
            "[INFO|configuration_utils.py:817] 2025-08-05 22:40:39,706 >> Model config MistralConfig {\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"head_dim\": null,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.55.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model vocab size: 32001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO|modeling_utils.py:1308] 2025-08-05 22:40:39,821 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--BioMistral--BioMistral-7B/snapshots/9a11e1ffa817c211cbb52ee1fb312dc6b61b40a5/pytorch_model.bin\n",
            "[INFO|configuration_utils.py:1098] 2025-08-05 22:40:39,826 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "[INFO|safetensors_conversion.py:61] 2025-08-05 22:40:39,942 >> Attempting to create safetensors variant\n",
            "[INFO|safetensors_conversion.py:24] 2025-08-05 22:40:40,177 >> Attempting to convert .bin model on the fly to safetensors.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4024734703.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#Instantiate model with config(no weights will be loaded yet)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVoRAForCausalLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvora_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model Loaded successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/VoRA/models/modeling_vora.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# -------------- Setup LLM ---------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVoraGenerationMixin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    601\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   5059\u001b[0m                 \u001b[0moffload_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5060\u001b[0m                 \u001b[0merror_msgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5061\u001b[0;31m             \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5062\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5063\u001b[0m                 \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[1;32m   5522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5523\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5524\u001b[0;31m                 \u001b[0m_error_msgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisk_offload_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcpu_offload_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_shard_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5525\u001b[0m                 \u001b[0merror_msgs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0m_error_msgs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload_shard_file\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    972\u001b[0m     \u001b[0;31m# Skip it with fsdp on ranks other than 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_fsdp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_local_dist_rank_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_quantized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m         disk_offload_index, cpu_offload_index = _load_state_dict_into_meta_model(\n\u001b[0m\u001b[1;32m    975\u001b[0m             \u001b[0mmodel_to_load\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\u001b[0m\n\u001b[1;32m    842\u001b[0m             \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcasting_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m                 \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasting_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mto_contiguous\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m                 \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Load tokenizer with <image> token\n",
        "tokenizer = AutoTokenizer.from_pretrained(vora_config.llm, trust_remote_code=True)\n",
        "tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<image>\"]})\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "model_vocab_size = len(tokenizer)\n",
        "print(\"Model vocab size:\", model_vocab_size)\n",
        "\n",
        "#Instantiate model with config(no weights will be loaded yet)\n",
        "model = VoRAForCausalLM(vora_config)\n",
        "print(\"Model Loaded successfully!\")\n",
        "\n",
        "# To ensure embedding resizing affects the real layer\n",
        "model.get_input_embeddings = lambda: model.llm.model.embed_tokens\n",
        "model.set_input_embeddings = lambda value: setattr(model.llm.model, \"embed_tokens\", value)\n",
        "\n",
        "# Resize after adding special tokens\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "print(\"Tokenization set up successful!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wPT5YW6GSLfV",
      "metadata": {
        "id": "wPT5YW6GSLfV"
      },
      "outputs": [],
      "source": [
        "# Force resize lm_head to match\n",
        "new_vocab_size = model.llm.model.embed_tokens.weight.shape[0]\n",
        "\n",
        "# Resize lm_head\n",
        "old_lm_head = model.llm.lm_head\n",
        "new_lm_head = torch.nn.Linear(old_lm_head.in_features, new_vocab_size, bias=False)\n",
        "new_lm_head.weight.data[:old_lm_head.out_features] = old_lm_head.weight.data\n",
        "model.llm.lm_head = new_lm_head\n",
        "\n",
        "# thhen Update config at every level\n",
        "model.config.vocab_size = new_vocab_size\n",
        "model.llm.config.vocab_size = new_vocab_size\n",
        "model.llm.model.config.vocab_size = new_vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lE_Vt52RSR85",
      "metadata": {
        "id": "lE_Vt52RSR85"
      },
      "outputs": [],
      "source": [
        "print(\"LM head shape:\", model.llm.lm_head.weight.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n10O92s4hbgv",
      "metadata": {
        "id": "n10O92s4hbgv"
      },
      "source": [
        "Explanation:- We load a tokenizer for\n",
        "BioMistral-7B. The tokenizer is based on the Mistral/LLaMA tokenizer (32k vocab, using the same\n",
        "sentencepiece as LLaMA-2). We add a special token <image> to the tokenizer. This token will serve as\n",
        "a placeholder in text to indicate where an image is present. By adding it to the tokenizer‚Äôs vocabulary,\n",
        "we ensure the model can accept this token without treating it as unknown. We do this before loading\n",
        "the model so that the model‚Äôs embedding matrix can be resized to include the new token. (We also set the pad token to EOS  to avoid any issues with missing pad_token).\n",
        "\n",
        "\n",
        "trust_remote_code=True is set because BioMistral might have custom code. In\n",
        "practice, we don‚Äôt explicitly call from_pretrained here because VoRAForCausalLM handles\n",
        "it internally when we instantiate it with vora_config . The print statement \" Model loaded\n",
        "successfully!\" is just a placeholder confirmation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CJ7L8JOcPh9X",
      "metadata": {
        "id": "CJ7L8JOcPh9X"
      },
      "source": [
        "Below We inspect the model layers for sanity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "F6Ep5XeePgGI",
      "metadata": {
        "id": "F6Ep5XeePgGI"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DnCMAXpw26Vi",
      "metadata": {
        "id": "DnCMAXpw26Vi"
      },
      "source": [
        "### ‚úÖ Sanity Check: Tokenizer and Image Token ID\n",
        "\n",
        "We check:\n",
        "- That the `<image>` token was added correctly\n",
        "- That it has a valid token ID\n",
        "- That `pad_token_id` is set (to prevent model crash)\n",
        "\n",
        "This ensures smooth token replacement when inserting visual embeddings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eZxOKmi8heDP",
      "metadata": {
        "id": "eZxOKmi8heDP"
      },
      "source": [
        "We also added the <image> token to the tokenizer. Let‚Äôs double-check that this token is recognized:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "napEOrUrL7GK",
      "metadata": {
        "id": "napEOrUrL7GK"
      },
      "outputs": [],
      "source": [
        "print(model.llm.get_input_embeddings())\n",
        "print(\"Embed shape:\", model.llm.model.embed_tokens.weight.shape)\n",
        "print(\"LM head shape:\", model.llm.lm_head.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q6aCW11DVTSi",
      "metadata": {
        "id": "q6aCW11DVTSi"
      },
      "outputs": [],
      "source": [
        "print(\" pad_token_id:\", tokenizer.pad_token_id)\n",
        "assert isinstance(tokenizer.pad_token_id, int), \"pad_token_id is not set!\"\n",
        "# Verify tokenizer recognizes the <image> token\n",
        "token = \"<image>\"\n",
        "token_id = tokenizer.convert_tokens_to_ids(token)\n",
        "print(f\"Token: {token}\")\n",
        "print(f\"Token ID: {token_id}\")\n",
        "# Confirm it's actually in the vocab\n",
        "assert token_id != tokenizer.unk_token_id\n",
        "\" <image> token is not recognized (maps to [UNK])\"\n",
        "print(\" <image> token successfully added to tokenizer.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MPzkzYTfgsUV",
      "metadata": {
        "id": "MPzkzYTfgsUV"
      },
      "outputs": [],
      "source": [
        "# to debug what Vora has\n",
        "print(type(model))\n",
        "print(dir(model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RUdZjyeqN86E",
      "metadata": {
        "id": "RUdZjyeqN86E"
      },
      "outputs": [],
      "source": [
        "model.gradient_checkpointing_enable()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bp0Com-03DOT",
      "metadata": {
        "id": "bp0Com-03DOT"
      },
      "source": [
        "### üìä Count Parameters (Trainable vs Frozen)\n",
        "\n",
        "We confirm that:\n",
        "- The full model is ~7B parameters\n",
        "- Only a small % (~6.23%) are trainable (LoRA + vision embedding)\n",
        "\n",
        "This validates our use of **parameter-efficient fine-tuning** via LoRA, as in (Han et al., 2025).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97nzrmrC3J6O",
      "metadata": {
        "id": "97nzrmrC3J6O"
      },
      "outputs": [],
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Total parameters: {total_params/1e6:.1f}M\")\n",
        "print(f\"Trainable parameters: {trainable_params/1e6:.1f}M\")\n",
        "print(f\"Trainable %: {100 * trainable_params/total_params:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "S8x2CSu1WZRn",
      "metadata": {
        "id": "S8x2CSu1WZRn"
      },
      "source": [
        "# **Data wrapper**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dC7YH-eN3dcX",
      "metadata": {
        "id": "dC7YH-eN3dcX"
      },
      "source": [
        "### üß± VQARADDataset Class\n",
        "\n",
        "Wraps VQA-RAD into a PyTorch-compatible dataset.\n",
        "\n",
        "- Adds `<image>` token at the start of each question\n",
        "- Converts X-ray image to tensor (resized + normalized)\n",
        "- Returns image and text fields\n",
        "\n",
        "Note: VoRA expects `frames`, `n_frames`, and question with `<image>` token (Han et al., 2025).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gcGS-n1c6yF-",
      "metadata": {
        "id": "gcGS-n1c6yF-"
      },
      "source": [
        "(### VoRAWrappedDataset: Tokenization + Preprocessing\n",
        "\n",
        "Prepares each example for VoRA fine-tuning:\n",
        "\n",
        "- Tokenizes question (with `<image>`)\n",
        "- Finds the token position of `<image>`\n",
        "- Tokenizes answer as labels\n",
        "- Keeps raw image tensor (`frames`) for model‚Äôs vision branch\n",
        "\n",
        "This matches the expected input structure for `VoRATrainer`, enabling it to inject vision tokens inside the model. )  **will try this part again.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hVgSmQW43oRX",
      "metadata": {
        "id": "hVgSmQW43oRX"
      },
      "source": [
        "### üß™ Create Train and Test Dataset Objects\n",
        "\n",
        "We now create PyTorch-compatible train and test datasets from VQA-RAD.\n",
        "\n",
        "Note: each sample returns:\n",
        "- image tensor (`frames`)\n",
        "- question with `<image>` token\n",
        "- raw answer text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XeX_fHvRYo2G",
      "metadata": {
        "id": "XeX_fHvRYo2G"
      },
      "outputs": [],
      "source": [
        "!pip install colorlog --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ga0yGHgIVnQo",
      "metadata": {
        "id": "ga0yGHgIVnQo"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from train.train import VoRATrainer, TrainingArguments\n",
        "from transformers import PreTrainedTokenizer\n",
        "\n",
        "image_token = \"<image>\"\n",
        "output_dir = \"./output_vora_biomistral_vqa_rad\"\n",
        "\n",
        "# Load dataset\n",
        "#vqa_dataset = load_dataset(\"flaviagiammarino/vqa-rad\")\n",
        "#vqa_dataset[\"train\"] = vqa_dataset[\"train\"].select(range(10))\n",
        "\n",
        "#  Preview sizes\n",
        "#print(\"Train size:\", len(vqa_dataset['train']))\n",
        "#print(\"Test size:\", len(vqa_dataset['test']))\n",
        "#print(\"Sample QA:\", vqa_dataset['train'][0]['question'], \"=>\", vqa_dataset['train'][0]['answer'])\n",
        "\n",
        "# Dataset class\n",
        "class VQARADDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer: PreTrainedTokenizer, split=\"train\", max_length=1024):\n",
        "        self.data = dataset[split]\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.split = split\n",
        "        #self.vision_token = vision_token\n",
        "        #self.num_image_tokens = num_image_tokens\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((448, 448)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                 [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "\n",
        "        image = item[\"image\"]\n",
        "        if not isinstance(image, Image.Image):\n",
        "            image = Image.open(image).convert(\"RGB\")\n",
        "\n",
        "        pixel_values = self.transform(image)\n",
        "\n",
        "        prompt = f\"{image_token} {item['question']}\"\n",
        "        full_input = f\"{prompt}\\n{item['answer'].strip()}\"\n",
        "\n",
        "\n",
        "        tokenized = self.tokenizer(\n",
        "        full_input,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=self.max_length,\n",
        "        return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        input_ids = tokenized.input_ids.squeeze(0)\n",
        "        attention_mask = tokenized.attention_mask.squeeze(0)\n",
        "\n",
        "        labels = input_ids.clone()\n",
        "        prompt_len = len(self.tokenizer(prompt, truncation=True, max_length=self.max_length)[\"input_ids\"])\n",
        "        labels[:prompt_len] = -100\n",
        "\n",
        "        return {\n",
        "            \"pixel_values\": pixel_values,\n",
        "            \"frames\": pixel_values,\n",
        "            \"n_frames\": 1,\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels,\n",
        "            \"vision_placeholder_index\": self.tokenizer.convert_tokens_to_ids(image_token),\n",
        "            \"prompt\": prompt,\n",
        "            \"gt\": item[\"answer\"],\n",
        "            \"question\": item[\"question\"]\n",
        "            }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "z1UIC8KJ4FVi",
      "metadata": {
        "id": "z1UIC8KJ4FVi"
      },
      "source": [
        "### üß© Collate Function for Multimodal Batching\n",
        "\n",
        "This custom `collate_fn`:\n",
        "- Tokenizes the question and answer\n",
        "- Finds the `<image>` token position\n",
        "- Replaces it with ViT patch embeddings\n",
        "- Constructs final `inputs_embeds` and `attention_mask`\n",
        "\n",
        "This enables VoRA to attend over both vision and text inside a unified sequence, as proposed in (Han et al., 2025, Sec. 3.3).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SroftRit63P4",
      "metadata": {
        "id": "SroftRit63P4"
      },
      "source": [
        "Combines batch elements into a dictionary by key.\n",
        "\n",
        "- Stacks tensors (e.g. `input_ids`, `frames`)\n",
        "- Leaves strings (`prompt`, `gt`) as-is for logging\n",
        "\n",
        "Used by `VoRATrainer` to build batches from the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y4OO4ba1V6nT",
      "metadata": {
        "id": "y4OO4ba1V6nT"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    return {\n",
        "        \"input_ids\": torch.stack([x[\"input_ids\"] for x in batch]),\n",
        "        \"attention_mask\": torch.stack([x[\"attention_mask\"] for x in batch]),\n",
        "        \"labels\": torch.stack([x[\"labels\"] for x in batch]),\n",
        "        \"pixel_values\": torch.stack([x[\"pixel_values\"] for x in batch]),\n",
        "        \"frames\": torch.stack([x[\"frames\"] for x in batch]),\n",
        "        \"n_frames\": [x[\"n_frames\"] for x in batch],\n",
        "        \"vision_placeholder_index\": batch[0][\"vision_placeholder_index\"],\n",
        "        \"prompt\": [x[\"prompt\"] for x in batch],\n",
        "        \"gt\": [x[\"gt\"] for x in batch],\n",
        "        \"question\": [x[\"question\"] for x in batch],\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6jX_mZ2q4OrS",
      "metadata": {
        "id": "6jX_mZ2q4OrS"
      },
      "source": [
        "### üîç Quick Check: Collate a Single Sample\n",
        "\n",
        "Let‚Äôs inspect one preprocessed example to confirm:\n",
        "- Visual embeddings inserted correctly\n",
        "- Tensor shapes make sense\n",
        "\n",
        "Useful for debugging before training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CfB5ph9lX-qZ",
      "metadata": {
        "id": "CfB5ph9lX-qZ"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"flaviagiammarino/vqa-rad\")\n",
        "train_dataset = VQARADDataset(dataset, tokenizer, split=\"train\")\n",
        "\n",
        "train_dataset.data = train_dataset.data.select(range(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "v5d9oaoIZC93",
      "metadata": {
        "id": "v5d9oaoIZC93"
      },
      "source": [
        "# **Train**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2vOzyduE-AQ6",
      "metadata": {
        "id": "2vOzyduE-AQ6"
      },
      "source": [
        "Before that we will do a quick check on the tokens and that the LM head has also been resized. It is also paramount to to check that the vocab sizes match, because during inference the size mismatch error pops up, basically rendering the trained mmodel unsable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WL2CXN32OjbL",
      "metadata": {
        "id": "WL2CXN32OjbL"
      },
      "outputs": [],
      "source": [
        "print(\"Embed tokens:\", model.llm.model.embed_tokens.weight.shape)\n",
        "print(\"LM head:\", model.llm.lm_head.weight.shape)\n",
        "print(\"Config vocab sizes:\")\n",
        "print(\"  model.config.vocab_size =\", model.config.vocab_size)\n",
        "print(\"  model.llm.config.vocab_size =\", model.llm.config.vocab_size)\n",
        "print(\"  model.llm.model.config.vocab_size =\", model.llm.model.config.vocab_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VZZRvZllYisq",
      "metadata": {
        "id": "VZZRvZllYisq"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=2,\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    remove_unused_columns=False,\n",
        "    report_to=[],\n",
        ")\n",
        "\n",
        "trainer = VoRATrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    data_collator=collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8yzty-9aY5hj",
      "metadata": {
        "id": "8yzty-9aY5hj"
      },
      "outputs": [],
      "source": [
        "trainer.train()\n",
        "trainer.save_model(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nJC3v85H7X5I",
      "metadata": {
        "id": "nJC3v85H7X5I"
      },
      "source": [
        "### üíæ Save Final Fine-Tuned Model\n",
        "\n",
        "`safe_save_model_for_hf_trainer()` ensures the final model (LoRA + vision layers) is written to disk in HuggingFace format.  \n",
        "We can now merge or load this model later for inference.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from train.train import safe_save_model_for_hf_trainer\n",
        "\n",
        "# Save model to disk\n",
        "safe_save_model_for_hf_trainer(trainer, output_dir)"
      ],
      "metadata": {
        "id": "F78wTIWJNMhd"
      },
      "id": "F78wTIWJNMhd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u0VIcgwYTTbZ",
      "metadata": {
        "id": "u0VIcgwYTTbZ"
      },
      "outputs": [],
      "source": [
        "for step_log in trainer.state.log_history:\n",
        "    print(step_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TPTAJINJrN6B",
      "metadata": {
        "id": "TPTAJINJrN6B"
      },
      "source": [
        "# **Dummy processor**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n0ruhitprNcT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "n0ruhitprNcT",
        "outputId": "9e30e336-7fcb-4d7f-9750-9ce04700359e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'output_dir' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2811802350.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCLIPImageProcessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mProcessorMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mVoRAProcessorHF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mProcessorMixin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'output_dir' is not defined"
          ]
        }
      ],
      "source": [
        "from transformers import CLIPImageProcessor, AutoTokenizer, ProcessorMixin\n",
        "\n",
        "class VoRAProcessorHF(ProcessorMixin):\n",
        "    attributes = [\"image_processor\", \"tokenizer\"]\n",
        "    image_processor_class = \"CLIPImageProcessor\"\n",
        "    tokenizer_class = \"AutoTokenizer\"\n",
        "\n",
        "# Load components\n",
        "image_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
        "\n",
        "# Create processor\n",
        "processor = VoRAProcessorHF(\n",
        "    image_processor=image_processor,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Save processor to tehsame model dir\n",
        "processor.save_pretrained(output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6aB6OANb7mjh",
      "metadata": {
        "id": "6aB6OANb7mjh"
      },
      "source": [
        "# üß† Inference Function: Ask VoRA a Question with an Image\n",
        "\n",
        "We:\n",
        "- Tokenize the input question (must include `<image>`)\n",
        "- Prepare image as tensor (resized + normalized)\n",
        "- Pass to VoRA model using `generate()`\n",
        "- Decode the output tokens into readable text\n",
        "\n",
        "VoRA replaces `<image>` internally with visual embeddings and uses LoRA-tuned layers to answer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g4RCSwgIo7Hu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4RCSwgIo7Hu",
        "outputId": "7e75456a-7719-45ae-ceae-316d92445c15",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO|image_processing_base.py:376] 2025-08-05 22:42:24,266 >> loading configuration file ./output_vora_biomistral_vqa_rad/preprocessor_config.json\n",
            "[INFO|tokenization_utils_base.py:2065] 2025-08-05 22:42:24,267 >> loading file tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2065] 2025-08-05 22:42:24,268 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2065] 2025-08-05 22:42:24,268 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2065] 2025-08-05 22:42:24,269 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2065] 2025-08-05 22:42:24,269 >> loading file tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2065] 2025-08-05 22:42:24,269 >> loading file chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2065] 2025-08-05 22:42:24,408 >> loading file tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2065] 2025-08-05 22:42:24,409 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2065] 2025-08-05 22:42:24,410 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2065] 2025-08-05 22:42:24,410 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2065] 2025-08-05 22:42:24,411 >> loading file tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2065] 2025-08-05 22:42:24,411 >> loading file chat_template.jinja\n",
            "[WARNING|configuration_utils.py:684] 2025-08-05 22:42:24,520 >> The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
            "[INFO|configuration_utils.py:750] 2025-08-05 22:42:24,522 >> loading configuration file ./output_vora_biomistral_vqa_rad/config.json\n",
            "[INFO|configuration_utils.py:817] 2025-08-05 22:42:24,524 >> Model config VoRAConfig {\n",
            "  \"architectures\": [\n",
            "    \"VoRAForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"configuration_vora.VoRAConfig\",\n",
            "    \"AutoModelForCausalLM\": \"modeling_vora.VoRAForCausalLM\"\n",
            "  },\n",
            "  \"aux_vision\": \"\",\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"head_dim\": null,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"image_size\": 448,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"llm\": \"BioMistral/BioMistral-7B\",\n",
            "  \"lora\": {\n",
            "    \"lora_alpha\": 32,\n",
            "    \"lora_dropout\": 0.1,\n",
            "    \"r\": 128,\n",
            "    \"target_modules\": [\n",
            "      \"self_attn.q_proj\",\n",
            "      \"self_attn.k_proj\",\n",
            "      \"self_attn.v_proj\",\n",
            "      \"self_attn.o_proj\",\n",
            "      \"mlp.gate_proj\",\n",
            "      \"mlp.down_proj\",\n",
            "      \"mlp.up_proj\"\n",
            "    ]\n",
            "  },\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"model_type\": \"vora\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"patch_size\": 14,\n",
            "  \"reuse_aux_vision_embedding_layers\": \"\",\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"skip_aux_cls\": false,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.55.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vision_attention_mask\": \"bidirectional\",\n",
            "  \"vision_embedding\": \"AIMv2Embedding\",\n",
            "  \"vision_embedding_intermediate_size\": 768,\n",
            "  \"vocab_size\": 32001\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:752] 2025-08-05 22:42:24,651 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--BioMistral--BioMistral-7B/snapshots/9a11e1ffa817c211cbb52ee1fb312dc6b61b40a5/config.json\n",
            "[INFO|configuration_utils.py:817] 2025-08-05 22:42:24,653 >> Model config MistralConfig {\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"head_dim\": null,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.55.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1308] 2025-08-05 22:42:24,754 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--BioMistral--BioMistral-7B/snapshots/9a11e1ffa817c211cbb52ee1fb312dc6b61b40a5/pytorch_model.bin\n",
            "[INFO|configuration_utils.py:1098] 2025-08-05 22:42:24,758 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "[INFO|safetensors_conversion.py:61] 2025-08-05 22:42:24,889 >> Attempting to create safetensors variant\n",
            "[INFO|safetensors_conversion.py:24] 2025-08-05 22:42:25,115 >> Attempting to convert .bin model on the fly to safetensors.\n",
            "[INFO|modeling_utils.py:5606] 2025-08-05 22:42:32,951 >> All model checkpoint weights were used when initializing MistralForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:5614] 2025-08-05 22:42:32,952 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at BioMistral/BioMistral-7B.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1053] 2025-08-05 22:42:33,118 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--BioMistral--BioMistral-7B/snapshots/9a11e1ffa817c211cbb52ee1fb312dc6b61b40a5/generation_config.json\n",
            "[INFO|configuration_utils.py:1098] 2025-08-05 22:42:33,119 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2067] 2025-08-05 22:42:33,430 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--BioMistral--BioMistral-7B/snapshots/9a11e1ffa817c211cbb52ee1fb312dc6b61b40a5/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2067] 2025-08-05 22:42:33,431 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--BioMistral--BioMistral-7B/snapshots/9a11e1ffa817c211cbb52ee1fb312dc6b61b40a5/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2067] 2025-08-05 22:42:33,431 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2067] 2025-08-05 22:42:33,431 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--BioMistral--BioMistral-7B/snapshots/9a11e1ffa817c211cbb52ee1fb312dc6b61b40a5/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2067] 2025-08-05 22:42:33,432 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--BioMistral--BioMistral-7B/snapshots/9a11e1ffa817c211cbb52ee1fb312dc6b61b40a5/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2067] 2025-08-05 22:42:33,432 >> loading file chat_template.jinja from cache at None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:69 >> Trainable parameters:\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.0.self_attn.q_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.0.self_attn.q_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.0.self_attn.q_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.0.self_attn.k_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.0.self_attn.k_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.0.self_attn.k_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.0.self_attn.v_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.0.self_attn.v_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.0.self_attn.v_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.0.self_attn.o_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.0.self_attn.o_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.0.self_attn.o_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.0.mlp.gate_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.0.mlp.gate_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.0.mlp.gate_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.0.mlp.up_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.0.mlp.up_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.0.mlp.up_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.0.mlp.down_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.0.mlp.down_proj.lora_A.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.0.mlp.down_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.1.self_attn.q_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.1.self_attn.q_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.1.self_attn.q_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.1.self_attn.k_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.1.self_attn.k_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.1.self_attn.k_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.1.self_attn.v_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.1.self_attn.v_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.1.self_attn.v_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.1.self_attn.o_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.1.self_attn.o_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.1.self_attn.o_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.1.mlp.gate_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.1.mlp.gate_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.1.mlp.gate_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.1.mlp.up_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.1.mlp.up_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.1.mlp.up_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.1.mlp.down_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.1.mlp.down_proj.lora_A.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.1.mlp.down_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.2.self_attn.q_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.2.self_attn.q_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.2.self_attn.q_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.2.self_attn.k_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.2.self_attn.k_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.2.self_attn.k_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.2.self_attn.v_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.2.self_attn.v_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.2.self_attn.v_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.2.self_attn.o_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.2.self_attn.o_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.2.self_attn.o_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.2.mlp.gate_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.2.mlp.gate_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.2.mlp.gate_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.2.mlp.up_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.2.mlp.up_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.2.mlp.up_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.2.mlp.down_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.2.mlp.down_proj.lora_A.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.2.mlp.down_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.3.self_attn.q_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.3.self_attn.q_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.3.self_attn.q_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.3.self_attn.k_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.3.self_attn.k_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.3.self_attn.k_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.3.self_attn.v_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.3.self_attn.v_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.3.self_attn.v_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.3.self_attn.o_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.3.self_attn.o_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.3.self_attn.o_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.3.mlp.gate_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.3.mlp.gate_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.3.mlp.gate_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.3.mlp.up_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.3.mlp.up_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.3.mlp.up_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.3.mlp.down_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.3.mlp.down_proj.lora_A.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.3.mlp.down_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.4.self_attn.q_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.4.self_attn.q_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.4.self_attn.q_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.4.self_attn.k_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.4.self_attn.k_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.4.self_attn.k_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.4.self_attn.v_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.4.self_attn.v_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.4.self_attn.v_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.4.self_attn.o_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.4.self_attn.o_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.4.self_attn.o_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.4.mlp.gate_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.4.mlp.gate_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.4.mlp.gate_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.4.mlp.up_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.4.mlp.up_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.4.mlp.up_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.4.mlp.down_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.4.mlp.down_proj.lora_A.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.4.mlp.down_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.5.self_attn.q_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.5.self_attn.q_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.5.self_attn.q_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.5.self_attn.k_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.5.self_attn.k_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.5.self_attn.k_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.5.self_attn.v_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.5.self_attn.v_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.5.self_attn.v_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.5.self_attn.o_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.5.self_attn.o_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.5.self_attn.o_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.5.mlp.gate_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.5.mlp.gate_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.5.mlp.gate_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.5.mlp.up_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.5.mlp.up_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.5.mlp.up_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.5.mlp.down_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.5.mlp.down_proj.lora_A.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.5.mlp.down_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.6.self_attn.q_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.6.self_attn.q_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.6.self_attn.q_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:31] models.modeling_vora:72 >> llm.model.layers.6.self_attn.k_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.6.self_attn.k_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.6.self_attn.k_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.6.self_attn.v_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.6.self_attn.v_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.6.self_attn.v_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.6.self_attn.o_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.6.self_attn.o_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.6.self_attn.o_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.6.mlp.gate_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.6.mlp.gate_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.6.mlp.gate_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.6.mlp.up_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.6.mlp.up_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.6.mlp.up_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.6.mlp.down_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.6.mlp.down_proj.lora_A.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.6.mlp.down_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.7.self_attn.q_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.7.self_attn.q_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.7.self_attn.q_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.7.self_attn.k_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.7.self_attn.k_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.7.self_attn.k_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.7.self_attn.v_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.7.self_attn.v_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.7.self_attn.v_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.7.self_attn.o_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.7.self_attn.o_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.7.self_attn.o_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.7.mlp.gate_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.7.mlp.gate_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.7.mlp.gate_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.7.mlp.up_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.7.mlp.up_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.7.mlp.up_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.7.mlp.down_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.7.mlp.down_proj.lora_A.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.7.mlp.down_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.8.self_attn.q_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.8.self_attn.q_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.8.self_attn.q_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.8.self_attn.k_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.8.self_attn.k_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.8.self_attn.k_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.8.self_attn.v_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.8.self_attn.v_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.8.self_attn.v_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.8.self_attn.o_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.8.self_attn.o_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.8.self_attn.o_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.8.mlp.gate_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.8.mlp.gate_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.8.mlp.gate_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.8.mlp.up_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.8.mlp.up_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.8.mlp.up_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.8.mlp.down_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.8.mlp.down_proj.lora_A.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.8.mlp.down_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.9.self_attn.q_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.9.self_attn.q_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.9.self_attn.q_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.9.self_attn.k_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.9.self_attn.k_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.9.self_attn.k_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.9.self_attn.v_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.9.self_attn.v_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.9.self_attn.v_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.9.self_attn.o_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.9.self_attn.o_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.9.self_attn.o_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.9.mlp.gate_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.9.mlp.gate_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.9.mlp.gate_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.9.mlp.up_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.9.mlp.up_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.9.mlp.up_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.9.mlp.down_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.9.mlp.down_proj.lora_A.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.9.mlp.down_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.10.self_attn.q_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.10.self_attn.q_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.10.self_attn.q_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.10.self_attn.k_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.10.self_attn.k_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.10.self_attn.k_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.10.self_attn.v_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.10.self_attn.v_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.10.self_attn.v_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.10.self_attn.o_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.10.self_attn.o_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.10.self_attn.o_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.10.mlp.gate_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.10.mlp.gate_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.10.mlp.gate_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.10.mlp.up_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.10.mlp.up_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.10.mlp.up_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.10.mlp.down_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.10.mlp.down_proj.lora_A.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.10.mlp.down_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.11.self_attn.q_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.11.self_attn.q_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.11.self_attn.q_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.11.self_attn.k_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.11.self_attn.k_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.11.self_attn.k_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.11.self_attn.v_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.11.self_attn.v_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.11.self_attn.v_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.11.self_attn.o_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.11.self_attn.o_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.11.self_attn.o_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.11.mlp.gate_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.11.mlp.gate_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.11.mlp.gate_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.11.mlp.up_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.11.mlp.up_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.11.mlp.up_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.11.mlp.down_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.11.mlp.down_proj.lora_A.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.11.mlp.down_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.12.self_attn.q_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.12.self_attn.q_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.12.self_attn.q_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.12.self_attn.k_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.12.self_attn.k_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.12.self_attn.k_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.12.self_attn.v_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.12.self_attn.v_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.12.self_attn.v_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.12.self_attn.o_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.12.self_attn.o_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.12.self_attn.o_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.12.mlp.gate_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.12.mlp.gate_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.12.mlp.gate_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.12.mlp.up_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.12.mlp.up_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.12.mlp.up_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.12.mlp.down_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.12.mlp.down_proj.lora_A.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.12.mlp.down_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.13.self_attn.q_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.13.self_attn.q_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.13.self_attn.q_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.13.self_attn.k_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.13.self_attn.k_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.13.self_attn.k_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.13.self_attn.v_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.13.self_attn.v_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.13.self_attn.v_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.13.self_attn.o_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.13.self_attn.o_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.13.self_attn.o_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.13.mlp.gate_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.13.mlp.gate_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.13.mlp.gate_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.13.mlp.up_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.13.mlp.up_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.13.mlp.up_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.13.mlp.down_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.13.mlp.down_proj.lora_A.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.13.mlp.down_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.14.self_attn.q_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.14.self_attn.q_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.14.self_attn.q_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.14.self_attn.k_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.14.self_attn.k_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.14.self_attn.k_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.14.self_attn.v_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.14.self_attn.v_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.14.self_attn.v_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.14.self_attn.o_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.14.self_attn.o_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.14.self_attn.o_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.14.mlp.gate_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.14.mlp.gate_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.14.mlp.gate_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.14.mlp.up_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.14.mlp.up_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.14.mlp.up_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.14.mlp.down_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.14.mlp.down_proj.lora_A.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.14.mlp.down_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.15.self_attn.q_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.15.self_attn.q_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.15.self_attn.q_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.15.self_attn.k_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.15.self_attn.k_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.15.self_attn.k_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.15.self_attn.v_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.15.self_attn.v_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.15.self_attn.v_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.15.self_attn.o_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.15.self_attn.o_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.15.self_attn.o_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.15.mlp.gate_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.15.mlp.gate_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.15.mlp.gate_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.15.mlp.up_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.15.mlp.up_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.15.mlp.up_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.15.mlp.down_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.15.mlp.down_proj.lora_A.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.15.mlp.down_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.16.self_attn.q_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.16.self_attn.q_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.16.self_attn.q_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.16.self_attn.k_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.16.self_attn.k_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.16.self_attn.k_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.16.self_attn.v_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.16.self_attn.v_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.16.self_attn.v_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.16.self_attn.o_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.16.self_attn.o_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.16.self_attn.o_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.16.mlp.gate_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.16.mlp.gate_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.16.mlp.gate_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.16.mlp.up_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.16.mlp.up_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.16.mlp.up_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.16.mlp.down_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.16.mlp.down_proj.lora_A.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.16.mlp.down_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.17.self_attn.q_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.17.self_attn.q_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.17.self_attn.q_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.17.self_attn.k_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.17.self_attn.k_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.17.self_attn.k_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.17.self_attn.v_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.17.self_attn.v_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.17.self_attn.v_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.17.self_attn.o_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.17.self_attn.o_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.17.self_attn.o_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.17.mlp.gate_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.17.mlp.gate_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.17.mlp.gate_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.17.mlp.up_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.17.mlp.up_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.17.mlp.up_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.17.mlp.down_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.17.mlp.down_proj.lora_A.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.17.mlp.down_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.18.self_attn.q_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.18.self_attn.q_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.18.self_attn.q_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.18.self_attn.k_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.18.self_attn.k_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.18.self_attn.k_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.18.self_attn.v_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.18.self_attn.v_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.18.self_attn.v_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.18.self_attn.o_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.18.self_attn.o_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.18.self_attn.o_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.18.mlp.gate_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.18.mlp.gate_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.18.mlp.gate_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.18.mlp.up_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.18.mlp.up_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.18.mlp.up_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.18.mlp.down_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.18.mlp.down_proj.lora_A.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.18.mlp.down_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.19.self_attn.q_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.19.self_attn.q_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.19.self_attn.q_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.19.self_attn.k_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.19.self_attn.k_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.19.self_attn.k_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.19.self_attn.v_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.19.self_attn.v_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.19.self_attn.v_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.19.self_attn.o_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.19.self_attn.o_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.19.self_attn.o_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.19.mlp.gate_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.19.mlp.gate_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.19.mlp.gate_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.19.mlp.up_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.19.mlp.up_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.19.mlp.up_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.19.mlp.down_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.19.mlp.down_proj.lora_A.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.19.mlp.down_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.20.self_attn.q_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.20.self_attn.q_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.20.self_attn.q_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.20.self_attn.k_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.20.self_attn.k_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.20.self_attn.k_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.20.self_attn.v_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.20.self_attn.v_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.20.self_attn.v_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.20.self_attn.o_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.20.self_attn.o_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.20.self_attn.o_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.20.mlp.gate_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.20.mlp.gate_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.20.mlp.gate_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.20.mlp.up_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.20.mlp.up_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.20.mlp.up_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.20.mlp.down_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.20.mlp.down_proj.lora_A.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.20.mlp.down_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.21.self_attn.q_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.21.self_attn.q_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.21.self_attn.q_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.21.self_attn.k_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.21.self_attn.k_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.21.self_attn.k_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.21.self_attn.v_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.21.self_attn.v_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.21.self_attn.v_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.21.self_attn.o_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.21.self_attn.o_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.21.self_attn.o_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.21.mlp.gate_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.21.mlp.gate_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.21.mlp.gate_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.21.mlp.up_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.21.mlp.up_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.21.mlp.up_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.21.mlp.down_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.21.mlp.down_proj.lora_A.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.21.mlp.down_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.22.self_attn.q_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.22.self_attn.q_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.22.self_attn.q_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.22.self_attn.k_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.22.self_attn.k_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.22.self_attn.k_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.22.self_attn.v_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.22.self_attn.v_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.22.self_attn.v_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.22.self_attn.o_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.22.self_attn.o_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.22.self_attn.o_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.22.mlp.gate_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.22.mlp.gate_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.22.mlp.gate_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.22.mlp.up_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.22.mlp.up_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.22.mlp.up_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.22.mlp.down_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.22.mlp.down_proj.lora_A.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.22.mlp.down_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.23.self_attn.q_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.23.self_attn.q_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.23.self_attn.q_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.23.self_attn.k_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.23.self_attn.k_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.23.self_attn.k_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.23.self_attn.v_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.23.self_attn.v_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.23.self_attn.v_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.23.self_attn.o_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.23.self_attn.o_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.23.self_attn.o_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.23.mlp.gate_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.23.mlp.gate_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.23.mlp.gate_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.23.mlp.up_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.23.mlp.up_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.23.mlp.up_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.23.mlp.down_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.23.mlp.down_proj.lora_A.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.23.mlp.down_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.24.self_attn.q_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.24.self_attn.q_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.24.self_attn.q_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.24.self_attn.k_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.24.self_attn.k_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.24.self_attn.k_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.24.self_attn.v_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.24.self_attn.v_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.24.self_attn.v_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.24.self_attn.o_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.24.self_attn.o_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.24.self_attn.o_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.24.mlp.gate_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.24.mlp.gate_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.24.mlp.gate_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.24.mlp.up_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.24.mlp.up_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.24.mlp.up_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.24.mlp.down_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.24.mlp.down_proj.lora_A.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.24.mlp.down_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.25.self_attn.q_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.25.self_attn.q_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.25.self_attn.q_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.25.self_attn.k_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.25.self_attn.k_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.25.self_attn.k_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.25.self_attn.v_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.25.self_attn.v_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.25.self_attn.v_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.25.self_attn.o_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.25.self_attn.o_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.25.self_attn.o_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.25.mlp.gate_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.25.mlp.gate_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.25.mlp.gate_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.25.mlp.up_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.25.mlp.up_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.25.mlp.up_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.25.mlp.down_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.25.mlp.down_proj.lora_A.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.25.mlp.down_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.26.self_attn.q_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.26.self_attn.q_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.26.self_attn.q_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.26.self_attn.k_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.26.self_attn.k_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.26.self_attn.k_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.26.self_attn.v_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.26.self_attn.v_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.26.self_attn.v_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.26.self_attn.o_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.26.self_attn.o_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.26.self_attn.o_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.26.mlp.gate_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.26.mlp.gate_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.26.mlp.gate_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.26.mlp.up_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.26.mlp.up_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.26.mlp.up_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.26.mlp.down_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.26.mlp.down_proj.lora_A.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.26.mlp.down_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.27.self_attn.q_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.27.self_attn.q_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.27.self_attn.q_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.27.self_attn.k_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.27.self_attn.k_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.27.self_attn.k_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.27.self_attn.v_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.27.self_attn.v_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.27.self_attn.v_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.27.self_attn.o_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.27.self_attn.o_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.27.self_attn.o_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.27.mlp.gate_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.27.mlp.gate_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.27.mlp.gate_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.27.mlp.up_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.27.mlp.up_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.27.mlp.up_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.27.mlp.down_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.27.mlp.down_proj.lora_A.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.27.mlp.down_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.28.self_attn.q_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.28.self_attn.q_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.28.self_attn.q_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.28.self_attn.k_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.28.self_attn.k_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.28.self_attn.k_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.28.self_attn.v_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.28.self_attn.v_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.28.self_attn.v_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.28.self_attn.o_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.28.self_attn.o_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.28.self_attn.o_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.28.mlp.gate_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.28.mlp.gate_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.28.mlp.gate_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.28.mlp.up_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.28.mlp.up_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.28.mlp.up_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.28.mlp.down_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.28.mlp.down_proj.lora_A.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.28.mlp.down_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.29.self_attn.q_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.29.self_attn.q_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.29.self_attn.q_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.29.self_attn.k_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.29.self_attn.k_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.29.self_attn.k_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.29.self_attn.v_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.29.self_attn.v_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.29.self_attn.v_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.29.self_attn.o_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.29.self_attn.o_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.29.self_attn.o_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.29.mlp.gate_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.29.mlp.gate_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.29.mlp.gate_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.29.mlp.up_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.29.mlp.up_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.29.mlp.up_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.29.mlp.down_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.29.mlp.down_proj.lora_A.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.29.mlp.down_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.30.self_attn.q_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.30.self_attn.q_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.30.self_attn.q_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.30.self_attn.k_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.30.self_attn.k_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.30.self_attn.k_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.30.self_attn.v_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.30.self_attn.v_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.30.self_attn.v_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.30.self_attn.o_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.30.self_attn.o_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.30.self_attn.o_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.30.mlp.gate_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.30.mlp.gate_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.30.mlp.gate_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.30.mlp.up_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.30.mlp.up_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.30.mlp.up_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.30.mlp.down_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.30.mlp.down_proj.lora_A.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.30.mlp.down_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.31.self_attn.q_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.31.self_attn.q_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.31.self_attn.q_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.31.self_attn.k_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.31.self_attn.k_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.31.self_attn.k_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.31.self_attn.v_proj.bias: 1024\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.31.self_attn.v_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.31.self_attn.v_proj.lora_B.weight: 131072\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.31.self_attn.o_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.31.self_attn.o_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.31.self_attn.o_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.31.mlp.gate_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.31.mlp.gate_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.31.mlp.gate_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.31.mlp.up_proj.bias: 14336\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.31.mlp.up_proj.lora_A.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.31.mlp.up_proj.lora_B.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.31.mlp.down_proj.bias: 4096\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.31.mlp.down_proj.lora_A.weight: 1835008\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> llm.model.layers.31.mlp.down_proj.lora_B.weight: 524288\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> vision_embedding.pos_embed: 786432\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> vision_embedding.patchifier.proj.weight: 451584\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> vision_embedding.patchifier.proj.bias: 768\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> vision_embedding.patchifier.norm.weight: 768\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:72 >> vision_embedding.out_proj.weight: 3145728\u001b[0m\n",
            "\u001b[32m[INFO|2025-08-05 22:43:32] models.modeling_vora:73 >> Total parameters: 7583037952\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO|modeling_utils.py:3321] 2025-08-05 22:43:32,478 >> You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
            "[WARNING|logging.py:328] 2025-08-05 22:43:33,549 >> The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VoRAForCausalLM(\n",
              "  (llm): MistralForCausalLM(\n",
              "    (model): MistralModel(\n",
              "      (embed_tokens): Embedding(32001, 4096)\n",
              "      (layers): ModuleList(\n",
              "        (0-31): 32 x MistralDecoderLayer(\n",
              "          (self_attn): MistralAttention(\n",
              "            (q_proj): LoRALayer(\n",
              "              in_features=4096, out_features=4096, bias=True\n",
              "              (lora_A): Linear(in_features=4096, out_features=128, bias=False)\n",
              "              (lora_B): Linear(in_features=128, out_features=4096, bias=False)\n",
              "            )\n",
              "            (k_proj): LoRALayer(\n",
              "              in_features=4096, out_features=1024, bias=True\n",
              "              (lora_A): Linear(in_features=4096, out_features=128, bias=False)\n",
              "              (lora_B): Linear(in_features=128, out_features=1024, bias=False)\n",
              "            )\n",
              "            (v_proj): LoRALayer(\n",
              "              in_features=4096, out_features=1024, bias=True\n",
              "              (lora_A): Linear(in_features=4096, out_features=128, bias=False)\n",
              "              (lora_B): Linear(in_features=128, out_features=1024, bias=False)\n",
              "            )\n",
              "            (o_proj): LoRALayer(\n",
              "              in_features=4096, out_features=4096, bias=True\n",
              "              (lora_A): Linear(in_features=4096, out_features=128, bias=False)\n",
              "              (lora_B): Linear(in_features=128, out_features=4096, bias=False)\n",
              "            )\n",
              "          )\n",
              "          (mlp): MistralMLP(\n",
              "            (gate_proj): LoRALayer(\n",
              "              in_features=4096, out_features=14336, bias=True\n",
              "              (lora_A): Linear(in_features=4096, out_features=128, bias=False)\n",
              "              (lora_B): Linear(in_features=128, out_features=14336, bias=False)\n",
              "            )\n",
              "            (up_proj): LoRALayer(\n",
              "              in_features=4096, out_features=14336, bias=True\n",
              "              (lora_A): Linear(in_features=4096, out_features=128, bias=False)\n",
              "              (lora_B): Linear(in_features=128, out_features=14336, bias=False)\n",
              "            )\n",
              "            (down_proj): LoRALayer(\n",
              "              in_features=14336, out_features=4096, bias=True\n",
              "              (lora_A): Linear(in_features=14336, out_features=128, bias=False)\n",
              "              (lora_B): Linear(in_features=128, out_features=4096, bias=False)\n",
              "            )\n",
              "            (act_fn): SiLU()\n",
              "          )\n",
              "          (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "          (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "        )\n",
              "      )\n",
              "      (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "      (rotary_emb): MistralRotaryEmbedding()\n",
              "    )\n",
              "    (lm_head): Linear(in_features=4096, out_features=32001, bias=False)\n",
              "  )\n",
              "  (vision_embedding): AIMv2Embedding(\n",
              "    (patchifier): AIMv2PatchEmbed(\n",
              "      (proj): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
              "      (norm): RMSNorm((768,), eps=1e-05)\n",
              "    )\n",
              "    (out_proj): Linear(in_features=768, out_features=4096, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "from transformers import AutoProcessor, AutoModelForCausalLM\n",
        "from transformers.modeling_utils import load_sharded_checkpoint\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "# Load processor and model from the output directory\n",
        "output_dir = \"./output_vora_biomistral_vqa_rad\"\n",
        "# processor = AutoProcessor.from_pretrained(output_dir, trust_remote_code=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(output_dir, trust_remote_code=True)\n",
        "tokenizer.add_special_tokens({'additional_special_tokens' : ['<image>']})\n",
        "\n",
        "#model = AutoModelForCausalLM.from_pretrained(output_dir, trust_remote_code=True).cuda().eval()\n",
        "\n",
        "#Load custom model\n",
        "config = VoRAConfig.from_pretrained(output_dir, trust_remote_code=True)\n",
        "model = VoRAForCausalLM(config)\n",
        "\n",
        "# Resize embedding to match the tokenizer to avoid size mismatch\n",
        "model.get_input_embeddings = lambda: model.llm.model.embed_tokens\n",
        "model.set_input_embeddings = lambda value: setattr(model.llm.model, \"embed_tokens\", value)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Resize lm_head to match\n",
        "with torch.no_grad():\n",
        "    old_lm_head = model.llm.lm_head\n",
        "    new_vocab_size = model.llm.model.embed_tokens.weight.shape[0]\n",
        "    new_lm_head = torch.nn.Linear(old_lm_head.in_features, new_vocab_size, bias=False)\n",
        "    new_lm_head.weight.data[:old_lm_head.out_features] = old_lm_head.weight.data\n",
        "    model.llm.lm_head = new_lm_head\n",
        "\n",
        "# Update all config vocab sizes like we did during training\n",
        "model.config.vocab_size = new_vocab_size\n",
        "model.llm.config.vocab_size = new_vocab_size\n",
        "model.llm.model.config.vocab_size = new_vocab_size\n",
        "\n",
        "# Now we load actual weights\n",
        "load_sharded_checkpoint(model, output_dir)\n",
        "model.eval().cuda()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ProcessorMixin, BatchFeature\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "class VoRAProcessor(ProcessorMixin):\n",
        "    attributes = [\"image_processor\", \"tokenizer\"]\n",
        "    image_processor_class = \"AutoImageProcessor\"\n",
        "    tokenizer_class = \"AutoTokenizer\"\n",
        "\n",
        "    def __init__(self, image_processor=None, tokenizer=None, image_token=\"<image>\", image_token_index=-200):\n",
        "        super().__init__(image_processor=image_processor, tokenizer=tokenizer)\n",
        "        self.image_token = image_token\n",
        "        self.image_token_index = image_token_index\n",
        "        self.target_image_size = 448\n",
        "\n",
        "    def expand2square(self, pil_img: Image.Image):\n",
        "        pil_img = pil_img.resize((self.target_image_size, self.target_image_size))\n",
        "        return pil_img\n",
        "\n",
        "    def tokenizer_vision_placeholder(self, text: str):\n",
        "        segments = text.split(self.image_token)\n",
        "        ids = []\n",
        "        for i, segment in enumerate(segments):\n",
        "            if i > 0:\n",
        "                ids.append(self.image_token_index)\n",
        "            ids.extend(self.tokenizer.encode(segment, add_special_tokens=False))\n",
        "\n",
        "        min_text_length = 10\n",
        "        if len(ids) < min_text_length:\n",
        "            ids += [self.tokenizer.pad_token_id] * (min_text_length - len(ids))\n",
        "        return ids\n",
        "\n",
        "    def __call__(self, images=None, text=None, return_tensors=\"pt\"):\n",
        "        # Process image\n",
        "        img = images[0].resize((448, 448))  # Exact size from config\n",
        "        image_inputs = self.image_processor([img], return_tensors=return_tensors)\n",
        "        image_inputs[\"frames\"] = image_inputs.pop(\"pixel_values\")\n",
        "        image_inputs[\"n_frames\"] = [1]\n",
        "        image_inputs[\"vision_placeholder_index\"] = self.image_token_index\n",
        "\n",
        "        # Process text\n",
        "        text_ids = self.tokenizer_vision_placeholder(text[0])  #\n",
        "        text_pad_length = 266 - 256 - len(text_ids)  # Total 266 - vision tokens - text tokens\n",
        "\n",
        "        input_ids = [text_ids + [self.tokenizer.pad_token_id] * text_pad_length]\n",
        "        attention_mask = [[1]*len(text_ids) + [0]*text_pad_length]\n",
        "\n",
        "        return BatchFeature(data={\n",
        "            **image_inputs,\n",
        "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
        "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
        "            \"vison_placeholder_mode\": torch.tensor([0])\n",
        "        })\n"
      ],
      "metadata": {
        "id": "e923FkWYfnLq"
      },
      "id": "e923FkWYfnLq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "0ftor4XEVmGb",
        "outputId": "9632a620-ed1f-4fcc-d34d-f446bfb04a3d"
      },
      "id": "0ftor4XEVmGb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e077d5f2-b1a3-4574-958b-0f16ffe64b00\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e077d5f2-b1a3-4574-958b-0f16ffe64b00\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 000001-1.jpg to 000001-1.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Processor type:\", type(processor))\n",
        "print(\"Has image processor:\", hasattr(processor, 'image_processor'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOmGUqZ509N_",
        "outputId": "267c1b9b-5f07-490d-8fb3-1e40fe4ee142"
      },
      "id": "nOmGUqZ509N_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processor type: <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>\n",
            "Has image processor: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vision encoder (ViT) for extracting image patches\n",
        "from transformers import AutoModel, AutoImageProcessor\n",
        "vit_encoder = AutoModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
        "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
        "\n",
        "vit_encoder.eval().to(\"cuda\")\n",
        "\n",
        "# Project patch embeddings to match LLM hidden size (4096)\n",
        "projection = torch.nn.Linear(768, 4096).to(\"cuda\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2469e29021454e4fad31abddfabdef8c",
            "de7af2dcea6c43e48e7c37c2cd23ed4c",
            "b6bc1f8c14e445a3a9a57132590f8ca4",
            "bea25870ebdb4f109ba1894294225105",
            "2ccd59d8936542e3a52ada363c8456c0",
            "2fcc984cdf4349f2a31032deff551e08",
            "e9c98662b75c4f028be5367e5b400625",
            "f356f2c6fa7f42858f5ba789c057b344",
            "129e26d0c5b14e618a74ac9e4f37a13a",
            "11ed5411b1f84ca68bd6fce51b124772",
            "33d6504a16c6424eb8249bbf65d9cc94"
          ]
        },
        "id": "Ase3lJEb35Jx",
        "outputId": "62dd5bef-5ac2-4943-d91c-ce77e6df77a4"
      },
      "id": "Ase3lJEb35Jx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO|configuration_utils.py:752] 2025-08-06 00:55:43,049 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--vit-base-patch16-224/snapshots/3f49326eb077187dfe1c2a2bb15fbd74e6ab91e3/config.json\n",
            "[INFO|configuration_utils.py:817] 2025-08-06 00:55:43,055 >> Model config ViTConfig {\n",
            "  \"architectures\": [\n",
            "    \"ViTForImageClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"encoder_stride\": 16,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"tench, Tinca tinca\",\n",
            "    \"1\": \"goldfish, Carassius auratus\",\n",
            "    \"2\": \"great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias\",\n",
            "    \"3\": \"tiger shark, Galeocerdo cuvieri\",\n",
            "    \"4\": \"hammerhead, hammerhead shark\",\n",
            "    \"5\": \"electric ray, crampfish, numbfish, torpedo\",\n",
            "    \"6\": \"stingray\",\n",
            "    \"7\": \"cock\",\n",
            "    \"8\": \"hen\",\n",
            "    \"9\": \"ostrich, Struthio camelus\",\n",
            "    \"10\": \"brambling, Fringilla montifringilla\",\n",
            "    \"11\": \"goldfinch, Carduelis carduelis\",\n",
            "    \"12\": \"house finch, linnet, Carpodacus mexicanus\",\n",
            "    \"13\": \"junco, snowbird\",\n",
            "    \"14\": \"indigo bunting, indigo finch, indigo bird, Passerina cyanea\",\n",
            "    \"15\": \"robin, American robin, Turdus migratorius\",\n",
            "    \"16\": \"bulbul\",\n",
            "    \"17\": \"jay\",\n",
            "    \"18\": \"magpie\",\n",
            "    \"19\": \"chickadee\",\n",
            "    \"20\": \"water ouzel, dipper\",\n",
            "    \"21\": \"kite\",\n",
            "    \"22\": \"bald eagle, American eagle, Haliaeetus leucocephalus\",\n",
            "    \"23\": \"vulture\",\n",
            "    \"24\": \"great grey owl, great gray owl, Strix nebulosa\",\n",
            "    \"25\": \"European fire salamander, Salamandra salamandra\",\n",
            "    \"26\": \"common newt, Triturus vulgaris\",\n",
            "    \"27\": \"eft\",\n",
            "    \"28\": \"spotted salamander, Ambystoma maculatum\",\n",
            "    \"29\": \"axolotl, mud puppy, Ambystoma mexicanum\",\n",
            "    \"30\": \"bullfrog, Rana catesbeiana\",\n",
            "    \"31\": \"tree frog, tree-frog\",\n",
            "    \"32\": \"tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui\",\n",
            "    \"33\": \"loggerhead, loggerhead turtle, Caretta caretta\",\n",
            "    \"34\": \"leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea\",\n",
            "    \"35\": \"mud turtle\",\n",
            "    \"36\": \"terrapin\",\n",
            "    \"37\": \"box turtle, box tortoise\",\n",
            "    \"38\": \"banded gecko\",\n",
            "    \"39\": \"common iguana, iguana, Iguana iguana\",\n",
            "    \"40\": \"American chameleon, anole, Anolis carolinensis\",\n",
            "    \"41\": \"whiptail, whiptail lizard\",\n",
            "    \"42\": \"agama\",\n",
            "    \"43\": \"frilled lizard, Chlamydosaurus kingi\",\n",
            "    \"44\": \"alligator lizard\",\n",
            "    \"45\": \"Gila monster, Heloderma suspectum\",\n",
            "    \"46\": \"green lizard, Lacerta viridis\",\n",
            "    \"47\": \"African chameleon, Chamaeleo chamaeleon\",\n",
            "    \"48\": \"Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis\",\n",
            "    \"49\": \"African crocodile, Nile crocodile, Crocodylus niloticus\",\n",
            "    \"50\": \"American alligator, Alligator mississipiensis\",\n",
            "    \"51\": \"triceratops\",\n",
            "    \"52\": \"thunder snake, worm snake, Carphophis amoenus\",\n",
            "    \"53\": \"ringneck snake, ring-necked snake, ring snake\",\n",
            "    \"54\": \"hognose snake, puff adder, sand viper\",\n",
            "    \"55\": \"green snake, grass snake\",\n",
            "    \"56\": \"king snake, kingsnake\",\n",
            "    \"57\": \"garter snake, grass snake\",\n",
            "    \"58\": \"water snake\",\n",
            "    \"59\": \"vine snake\",\n",
            "    \"60\": \"night snake, Hypsiglena torquata\",\n",
            "    \"61\": \"boa constrictor, Constrictor constrictor\",\n",
            "    \"62\": \"rock python, rock snake, Python sebae\",\n",
            "    \"63\": \"Indian cobra, Naja naja\",\n",
            "    \"64\": \"green mamba\",\n",
            "    \"65\": \"sea snake\",\n",
            "    \"66\": \"horned viper, cerastes, sand viper, horned asp, Cerastes cornutus\",\n",
            "    \"67\": \"diamondback, diamondback rattlesnake, Crotalus adamanteus\",\n",
            "    \"68\": \"sidewinder, horned rattlesnake, Crotalus cerastes\",\n",
            "    \"69\": \"trilobite\",\n",
            "    \"70\": \"harvestman, daddy longlegs, Phalangium opilio\",\n",
            "    \"71\": \"scorpion\",\n",
            "    \"72\": \"black and gold garden spider, Argiope aurantia\",\n",
            "    \"73\": \"barn spider, Araneus cavaticus\",\n",
            "    \"74\": \"garden spider, Aranea diademata\",\n",
            "    \"75\": \"black widow, Latrodectus mactans\",\n",
            "    \"76\": \"tarantula\",\n",
            "    \"77\": \"wolf spider, hunting spider\",\n",
            "    \"78\": \"tick\",\n",
            "    \"79\": \"centipede\",\n",
            "    \"80\": \"black grouse\",\n",
            "    \"81\": \"ptarmigan\",\n",
            "    \"82\": \"ruffed grouse, partridge, Bonasa umbellus\",\n",
            "    \"83\": \"prairie chicken, prairie grouse, prairie fowl\",\n",
            "    \"84\": \"peacock\",\n",
            "    \"85\": \"quail\",\n",
            "    \"86\": \"partridge\",\n",
            "    \"87\": \"African grey, African gray, Psittacus erithacus\",\n",
            "    \"88\": \"macaw\",\n",
            "    \"89\": \"sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita\",\n",
            "    \"90\": \"lorikeet\",\n",
            "    \"91\": \"coucal\",\n",
            "    \"92\": \"bee eater\",\n",
            "    \"93\": \"hornbill\",\n",
            "    \"94\": \"hummingbird\",\n",
            "    \"95\": \"jacamar\",\n",
            "    \"96\": \"toucan\",\n",
            "    \"97\": \"drake\",\n",
            "    \"98\": \"red-breasted merganser, Mergus serrator\",\n",
            "    \"99\": \"goose\",\n",
            "    \"100\": \"black swan, Cygnus atratus\",\n",
            "    \"101\": \"tusker\",\n",
            "    \"102\": \"echidna, spiny anteater, anteater\",\n",
            "    \"103\": \"platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus\",\n",
            "    \"104\": \"wallaby, brush kangaroo\",\n",
            "    \"105\": \"koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus\",\n",
            "    \"106\": \"wombat\",\n",
            "    \"107\": \"jellyfish\",\n",
            "    \"108\": \"sea anemone, anemone\",\n",
            "    \"109\": \"brain coral\",\n",
            "    \"110\": \"flatworm, platyhelminth\",\n",
            "    \"111\": \"nematode, nematode worm, roundworm\",\n",
            "    \"112\": \"conch\",\n",
            "    \"113\": \"snail\",\n",
            "    \"114\": \"slug\",\n",
            "    \"115\": \"sea slug, nudibranch\",\n",
            "    \"116\": \"chiton, coat-of-mail shell, sea cradle, polyplacophore\",\n",
            "    \"117\": \"chambered nautilus, pearly nautilus, nautilus\",\n",
            "    \"118\": \"Dungeness crab, Cancer magister\",\n",
            "    \"119\": \"rock crab, Cancer irroratus\",\n",
            "    \"120\": \"fiddler crab\",\n",
            "    \"121\": \"king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica\",\n",
            "    \"122\": \"American lobster, Northern lobster, Maine lobster, Homarus americanus\",\n",
            "    \"123\": \"spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish\",\n",
            "    \"124\": \"crayfish, crawfish, crawdad, crawdaddy\",\n",
            "    \"125\": \"hermit crab\",\n",
            "    \"126\": \"isopod\",\n",
            "    \"127\": \"white stork, Ciconia ciconia\",\n",
            "    \"128\": \"black stork, Ciconia nigra\",\n",
            "    \"129\": \"spoonbill\",\n",
            "    \"130\": \"flamingo\",\n",
            "    \"131\": \"little blue heron, Egretta caerulea\",\n",
            "    \"132\": \"American egret, great white heron, Egretta albus\",\n",
            "    \"133\": \"bittern\",\n",
            "    \"134\": \"crane\",\n",
            "    \"135\": \"limpkin, Aramus pictus\",\n",
            "    \"136\": \"European gallinule, Porphyrio porphyrio\",\n",
            "    \"137\": \"American coot, marsh hen, mud hen, water hen, Fulica americana\",\n",
            "    \"138\": \"bustard\",\n",
            "    \"139\": \"ruddy turnstone, Arenaria interpres\",\n",
            "    \"140\": \"red-backed sandpiper, dunlin, Erolia alpina\",\n",
            "    \"141\": \"redshank, Tringa totanus\",\n",
            "    \"142\": \"dowitcher\",\n",
            "    \"143\": \"oystercatcher, oyster catcher\",\n",
            "    \"144\": \"pelican\",\n",
            "    \"145\": \"king penguin, Aptenodytes patagonica\",\n",
            "    \"146\": \"albatross, mollymawk\",\n",
            "    \"147\": \"grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus\",\n",
            "    \"148\": \"killer whale, killer, orca, grampus, sea wolf, Orcinus orca\",\n",
            "    \"149\": \"dugong, Dugong dugon\",\n",
            "    \"150\": \"sea lion\",\n",
            "    \"151\": \"Chihuahua\",\n",
            "    \"152\": \"Japanese spaniel\",\n",
            "    \"153\": \"Maltese dog, Maltese terrier, Maltese\",\n",
            "    \"154\": \"Pekinese, Pekingese, Peke\",\n",
            "    \"155\": \"Shih-Tzu\",\n",
            "    \"156\": \"Blenheim spaniel\",\n",
            "    \"157\": \"papillon\",\n",
            "    \"158\": \"toy terrier\",\n",
            "    \"159\": \"Rhodesian ridgeback\",\n",
            "    \"160\": \"Afghan hound, Afghan\",\n",
            "    \"161\": \"basset, basset hound\",\n",
            "    \"162\": \"beagle\",\n",
            "    \"163\": \"bloodhound, sleuthhound\",\n",
            "    \"164\": \"bluetick\",\n",
            "    \"165\": \"black-and-tan coonhound\",\n",
            "    \"166\": \"Walker hound, Walker foxhound\",\n",
            "    \"167\": \"English foxhound\",\n",
            "    \"168\": \"redbone\",\n",
            "    \"169\": \"borzoi, Russian wolfhound\",\n",
            "    \"170\": \"Irish wolfhound\",\n",
            "    \"171\": \"Italian greyhound\",\n",
            "    \"172\": \"whippet\",\n",
            "    \"173\": \"Ibizan hound, Ibizan Podenco\",\n",
            "    \"174\": \"Norwegian elkhound, elkhound\",\n",
            "    \"175\": \"otterhound, otter hound\",\n",
            "    \"176\": \"Saluki, gazelle hound\",\n",
            "    \"177\": \"Scottish deerhound, deerhound\",\n",
            "    \"178\": \"Weimaraner\",\n",
            "    \"179\": \"Staffordshire bullterrier, Staffordshire bull terrier\",\n",
            "    \"180\": \"American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier\",\n",
            "    \"181\": \"Bedlington terrier\",\n",
            "    \"182\": \"Border terrier\",\n",
            "    \"183\": \"Kerry blue terrier\",\n",
            "    \"184\": \"Irish terrier\",\n",
            "    \"185\": \"Norfolk terrier\",\n",
            "    \"186\": \"Norwich terrier\",\n",
            "    \"187\": \"Yorkshire terrier\",\n",
            "    \"188\": \"wire-haired fox terrier\",\n",
            "    \"189\": \"Lakeland terrier\",\n",
            "    \"190\": \"Sealyham terrier, Sealyham\",\n",
            "    \"191\": \"Airedale, Airedale terrier\",\n",
            "    \"192\": \"cairn, cairn terrier\",\n",
            "    \"193\": \"Australian terrier\",\n",
            "    \"194\": \"Dandie Dinmont, Dandie Dinmont terrier\",\n",
            "    \"195\": \"Boston bull, Boston terrier\",\n",
            "    \"196\": \"miniature schnauzer\",\n",
            "    \"197\": \"giant schnauzer\",\n",
            "    \"198\": \"standard schnauzer\",\n",
            "    \"199\": \"Scotch terrier, Scottish terrier, Scottie\",\n",
            "    \"200\": \"Tibetan terrier, chrysanthemum dog\",\n",
            "    \"201\": \"silky terrier, Sydney silky\",\n",
            "    \"202\": \"soft-coated wheaten terrier\",\n",
            "    \"203\": \"West Highland white terrier\",\n",
            "    \"204\": \"Lhasa, Lhasa apso\",\n",
            "    \"205\": \"flat-coated retriever\",\n",
            "    \"206\": \"curly-coated retriever\",\n",
            "    \"207\": \"golden retriever\",\n",
            "    \"208\": \"Labrador retriever\",\n",
            "    \"209\": \"Chesapeake Bay retriever\",\n",
            "    \"210\": \"German short-haired pointer\",\n",
            "    \"211\": \"vizsla, Hungarian pointer\",\n",
            "    \"212\": \"English setter\",\n",
            "    \"213\": \"Irish setter, red setter\",\n",
            "    \"214\": \"Gordon setter\",\n",
            "    \"215\": \"Brittany spaniel\",\n",
            "    \"216\": \"clumber, clumber spaniel\",\n",
            "    \"217\": \"English springer, English springer spaniel\",\n",
            "    \"218\": \"Welsh springer spaniel\",\n",
            "    \"219\": \"cocker spaniel, English cocker spaniel, cocker\",\n",
            "    \"220\": \"Sussex spaniel\",\n",
            "    \"221\": \"Irish water spaniel\",\n",
            "    \"222\": \"kuvasz\",\n",
            "    \"223\": \"schipperke\",\n",
            "    \"224\": \"groenendael\",\n",
            "    \"225\": \"malinois\",\n",
            "    \"226\": \"briard\",\n",
            "    \"227\": \"kelpie\",\n",
            "    \"228\": \"komondor\",\n",
            "    \"229\": \"Old English sheepdog, bobtail\",\n",
            "    \"230\": \"Shetland sheepdog, Shetland sheep dog, Shetland\",\n",
            "    \"231\": \"collie\",\n",
            "    \"232\": \"Border collie\",\n",
            "    \"233\": \"Bouvier des Flandres, Bouviers des Flandres\",\n",
            "    \"234\": \"Rottweiler\",\n",
            "    \"235\": \"German shepherd, German shepherd dog, German police dog, alsatian\",\n",
            "    \"236\": \"Doberman, Doberman pinscher\",\n",
            "    \"237\": \"miniature pinscher\",\n",
            "    \"238\": \"Greater Swiss Mountain dog\",\n",
            "    \"239\": \"Bernese mountain dog\",\n",
            "    \"240\": \"Appenzeller\",\n",
            "    \"241\": \"EntleBucher\",\n",
            "    \"242\": \"boxer\",\n",
            "    \"243\": \"bull mastiff\",\n",
            "    \"244\": \"Tibetan mastiff\",\n",
            "    \"245\": \"French bulldog\",\n",
            "    \"246\": \"Great Dane\",\n",
            "    \"247\": \"Saint Bernard, St Bernard\",\n",
            "    \"248\": \"Eskimo dog, husky\",\n",
            "    \"249\": \"malamute, malemute, Alaskan malamute\",\n",
            "    \"250\": \"Siberian husky\",\n",
            "    \"251\": \"dalmatian, coach dog, carriage dog\",\n",
            "    \"252\": \"affenpinscher, monkey pinscher, monkey dog\",\n",
            "    \"253\": \"basenji\",\n",
            "    \"254\": \"pug, pug-dog\",\n",
            "    \"255\": \"Leonberg\",\n",
            "    \"256\": \"Newfoundland, Newfoundland dog\",\n",
            "    \"257\": \"Great Pyrenees\",\n",
            "    \"258\": \"Samoyed, Samoyede\",\n",
            "    \"259\": \"Pomeranian\",\n",
            "    \"260\": \"chow, chow chow\",\n",
            "    \"261\": \"keeshond\",\n",
            "    \"262\": \"Brabancon griffon\",\n",
            "    \"263\": \"Pembroke, Pembroke Welsh corgi\",\n",
            "    \"264\": \"Cardigan, Cardigan Welsh corgi\",\n",
            "    \"265\": \"toy poodle\",\n",
            "    \"266\": \"miniature poodle\",\n",
            "    \"267\": \"standard poodle\",\n",
            "    \"268\": \"Mexican hairless\",\n",
            "    \"269\": \"timber wolf, grey wolf, gray wolf, Canis lupus\",\n",
            "    \"270\": \"white wolf, Arctic wolf, Canis lupus tundrarum\",\n",
            "    \"271\": \"red wolf, maned wolf, Canis rufus, Canis niger\",\n",
            "    \"272\": \"coyote, prairie wolf, brush wolf, Canis latrans\",\n",
            "    \"273\": \"dingo, warrigal, warragal, Canis dingo\",\n",
            "    \"274\": \"dhole, Cuon alpinus\",\n",
            "    \"275\": \"African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus\",\n",
            "    \"276\": \"hyena, hyaena\",\n",
            "    \"277\": \"red fox, Vulpes vulpes\",\n",
            "    \"278\": \"kit fox, Vulpes macrotis\",\n",
            "    \"279\": \"Arctic fox, white fox, Alopex lagopus\",\n",
            "    \"280\": \"grey fox, gray fox, Urocyon cinereoargenteus\",\n",
            "    \"281\": \"tabby, tabby cat\",\n",
            "    \"282\": \"tiger cat\",\n",
            "    \"283\": \"Persian cat\",\n",
            "    \"284\": \"Siamese cat, Siamese\",\n",
            "    \"285\": \"Egyptian cat\",\n",
            "    \"286\": \"cougar, puma, catamount, mountain lion, painter, panther, Felis concolor\",\n",
            "    \"287\": \"lynx, catamount\",\n",
            "    \"288\": \"leopard, Panthera pardus\",\n",
            "    \"289\": \"snow leopard, ounce, Panthera uncia\",\n",
            "    \"290\": \"jaguar, panther, Panthera onca, Felis onca\",\n",
            "    \"291\": \"lion, king of beasts, Panthera leo\",\n",
            "    \"292\": \"tiger, Panthera tigris\",\n",
            "    \"293\": \"cheetah, chetah, Acinonyx jubatus\",\n",
            "    \"294\": \"brown bear, bruin, Ursus arctos\",\n",
            "    \"295\": \"American black bear, black bear, Ursus americanus, Euarctos americanus\",\n",
            "    \"296\": \"ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus\",\n",
            "    \"297\": \"sloth bear, Melursus ursinus, Ursus ursinus\",\n",
            "    \"298\": \"mongoose\",\n",
            "    \"299\": \"meerkat, mierkat\",\n",
            "    \"300\": \"tiger beetle\",\n",
            "    \"301\": \"ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle\",\n",
            "    \"302\": \"ground beetle, carabid beetle\",\n",
            "    \"303\": \"long-horned beetle, longicorn, longicorn beetle\",\n",
            "    \"304\": \"leaf beetle, chrysomelid\",\n",
            "    \"305\": \"dung beetle\",\n",
            "    \"306\": \"rhinoceros beetle\",\n",
            "    \"307\": \"weevil\",\n",
            "    \"308\": \"fly\",\n",
            "    \"309\": \"bee\",\n",
            "    \"310\": \"ant, emmet, pismire\",\n",
            "    \"311\": \"grasshopper, hopper\",\n",
            "    \"312\": \"cricket\",\n",
            "    \"313\": \"walking stick, walkingstick, stick insect\",\n",
            "    \"314\": \"cockroach, roach\",\n",
            "    \"315\": \"mantis, mantid\",\n",
            "    \"316\": \"cicada, cicala\",\n",
            "    \"317\": \"leafhopper\",\n",
            "    \"318\": \"lacewing, lacewing fly\",\n",
            "    \"319\": \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\",\n",
            "    \"320\": \"damselfly\",\n",
            "    \"321\": \"admiral\",\n",
            "    \"322\": \"ringlet, ringlet butterfly\",\n",
            "    \"323\": \"monarch, monarch butterfly, milkweed butterfly, Danaus plexippus\",\n",
            "    \"324\": \"cabbage butterfly\",\n",
            "    \"325\": \"sulphur butterfly, sulfur butterfly\",\n",
            "    \"326\": \"lycaenid, lycaenid butterfly\",\n",
            "    \"327\": \"starfish, sea star\",\n",
            "    \"328\": \"sea urchin\",\n",
            "    \"329\": \"sea cucumber, holothurian\",\n",
            "    \"330\": \"wood rabbit, cottontail, cottontail rabbit\",\n",
            "    \"331\": \"hare\",\n",
            "    \"332\": \"Angora, Angora rabbit\",\n",
            "    \"333\": \"hamster\",\n",
            "    \"334\": \"porcupine, hedgehog\",\n",
            "    \"335\": \"fox squirrel, eastern fox squirrel, Sciurus niger\",\n",
            "    \"336\": \"marmot\",\n",
            "    \"337\": \"beaver\",\n",
            "    \"338\": \"guinea pig, Cavia cobaya\",\n",
            "    \"339\": \"sorrel\",\n",
            "    \"340\": \"zebra\",\n",
            "    \"341\": \"hog, pig, grunter, squealer, Sus scrofa\",\n",
            "    \"342\": \"wild boar, boar, Sus scrofa\",\n",
            "    \"343\": \"warthog\",\n",
            "    \"344\": \"hippopotamus, hippo, river horse, Hippopotamus amphibius\",\n",
            "    \"345\": \"ox\",\n",
            "    \"346\": \"water buffalo, water ox, Asiatic buffalo, Bubalus bubalis\",\n",
            "    \"347\": \"bison\",\n",
            "    \"348\": \"ram, tup\",\n",
            "    \"349\": \"bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis\",\n",
            "    \"350\": \"ibex, Capra ibex\",\n",
            "    \"351\": \"hartebeest\",\n",
            "    \"352\": \"impala, Aepyceros melampus\",\n",
            "    \"353\": \"gazelle\",\n",
            "    \"354\": \"Arabian camel, dromedary, Camelus dromedarius\",\n",
            "    \"355\": \"llama\",\n",
            "    \"356\": \"weasel\",\n",
            "    \"357\": \"mink\",\n",
            "    \"358\": \"polecat, fitch, foulmart, foumart, Mustela putorius\",\n",
            "    \"359\": \"black-footed ferret, ferret, Mustela nigripes\",\n",
            "    \"360\": \"otter\",\n",
            "    \"361\": \"skunk, polecat, wood pussy\",\n",
            "    \"362\": \"badger\",\n",
            "    \"363\": \"armadillo\",\n",
            "    \"364\": \"three-toed sloth, ai, Bradypus tridactylus\",\n",
            "    \"365\": \"orangutan, orang, orangutang, Pongo pygmaeus\",\n",
            "    \"366\": \"gorilla, Gorilla gorilla\",\n",
            "    \"367\": \"chimpanzee, chimp, Pan troglodytes\",\n",
            "    \"368\": \"gibbon, Hylobates lar\",\n",
            "    \"369\": \"siamang, Hylobates syndactylus, Symphalangus syndactylus\",\n",
            "    \"370\": \"guenon, guenon monkey\",\n",
            "    \"371\": \"patas, hussar monkey, Erythrocebus patas\",\n",
            "    \"372\": \"baboon\",\n",
            "    \"373\": \"macaque\",\n",
            "    \"374\": \"langur\",\n",
            "    \"375\": \"colobus, colobus monkey\",\n",
            "    \"376\": \"proboscis monkey, Nasalis larvatus\",\n",
            "    \"377\": \"marmoset\",\n",
            "    \"378\": \"capuchin, ringtail, Cebus capucinus\",\n",
            "    \"379\": \"howler monkey, howler\",\n",
            "    \"380\": \"titi, titi monkey\",\n",
            "    \"381\": \"spider monkey, Ateles geoffroyi\",\n",
            "    \"382\": \"squirrel monkey, Saimiri sciureus\",\n",
            "    \"383\": \"Madagascar cat, ring-tailed lemur, Lemur catta\",\n",
            "    \"384\": \"indri, indris, Indri indri, Indri brevicaudatus\",\n",
            "    \"385\": \"Indian elephant, Elephas maximus\",\n",
            "    \"386\": \"African elephant, Loxodonta africana\",\n",
            "    \"387\": \"lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens\",\n",
            "    \"388\": \"giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca\",\n",
            "    \"389\": \"barracouta, snoek\",\n",
            "    \"390\": \"eel\",\n",
            "    \"391\": \"coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch\",\n",
            "    \"392\": \"rock beauty, Holocanthus tricolor\",\n",
            "    \"393\": \"anemone fish\",\n",
            "    \"394\": \"sturgeon\",\n",
            "    \"395\": \"gar, garfish, garpike, billfish, Lepisosteus osseus\",\n",
            "    \"396\": \"lionfish\",\n",
            "    \"397\": \"puffer, pufferfish, blowfish, globefish\",\n",
            "    \"398\": \"abacus\",\n",
            "    \"399\": \"abaya\",\n",
            "    \"400\": \"academic gown, academic robe, judge's robe\",\n",
            "    \"401\": \"accordion, piano accordion, squeeze box\",\n",
            "    \"402\": \"acoustic guitar\",\n",
            "    \"403\": \"aircraft carrier, carrier, flattop, attack aircraft carrier\",\n",
            "    \"404\": \"airliner\",\n",
            "    \"405\": \"airship, dirigible\",\n",
            "    \"406\": \"altar\",\n",
            "    \"407\": \"ambulance\",\n",
            "    \"408\": \"amphibian, amphibious vehicle\",\n",
            "    \"409\": \"analog clock\",\n",
            "    \"410\": \"apiary, bee house\",\n",
            "    \"411\": \"apron\",\n",
            "    \"412\": \"ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin\",\n",
            "    \"413\": \"assault rifle, assault gun\",\n",
            "    \"414\": \"backpack, back pack, knapsack, packsack, rucksack, haversack\",\n",
            "    \"415\": \"bakery, bakeshop, bakehouse\",\n",
            "    \"416\": \"balance beam, beam\",\n",
            "    \"417\": \"balloon\",\n",
            "    \"418\": \"ballpoint, ballpoint pen, ballpen, Biro\",\n",
            "    \"419\": \"Band Aid\",\n",
            "    \"420\": \"banjo\",\n",
            "    \"421\": \"bannister, banister, balustrade, balusters, handrail\",\n",
            "    \"422\": \"barbell\",\n",
            "    \"423\": \"barber chair\",\n",
            "    \"424\": \"barbershop\",\n",
            "    \"425\": \"barn\",\n",
            "    \"426\": \"barometer\",\n",
            "    \"427\": \"barrel, cask\",\n",
            "    \"428\": \"barrow, garden cart, lawn cart, wheelbarrow\",\n",
            "    \"429\": \"baseball\",\n",
            "    \"430\": \"basketball\",\n",
            "    \"431\": \"bassinet\",\n",
            "    \"432\": \"bassoon\",\n",
            "    \"433\": \"bathing cap, swimming cap\",\n",
            "    \"434\": \"bath towel\",\n",
            "    \"435\": \"bathtub, bathing tub, bath, tub\",\n",
            "    \"436\": \"beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon\",\n",
            "    \"437\": \"beacon, lighthouse, beacon light, pharos\",\n",
            "    \"438\": \"beaker\",\n",
            "    \"439\": \"bearskin, busby, shako\",\n",
            "    \"440\": \"beer bottle\",\n",
            "    \"441\": \"beer glass\",\n",
            "    \"442\": \"bell cote, bell cot\",\n",
            "    \"443\": \"bib\",\n",
            "    \"444\": \"bicycle-built-for-two, tandem bicycle, tandem\",\n",
            "    \"445\": \"bikini, two-piece\",\n",
            "    \"446\": \"binder, ring-binder\",\n",
            "    \"447\": \"binoculars, field glasses, opera glasses\",\n",
            "    \"448\": \"birdhouse\",\n",
            "    \"449\": \"boathouse\",\n",
            "    \"450\": \"bobsled, bobsleigh, bob\",\n",
            "    \"451\": \"bolo tie, bolo, bola tie, bola\",\n",
            "    \"452\": \"bonnet, poke bonnet\",\n",
            "    \"453\": \"bookcase\",\n",
            "    \"454\": \"bookshop, bookstore, bookstall\",\n",
            "    \"455\": \"bottlecap\",\n",
            "    \"456\": \"bow\",\n",
            "    \"457\": \"bow tie, bow-tie, bowtie\",\n",
            "    \"458\": \"brass, memorial tablet, plaque\",\n",
            "    \"459\": \"brassiere, bra, bandeau\",\n",
            "    \"460\": \"breakwater, groin, groyne, mole, bulwark, seawall, jetty\",\n",
            "    \"461\": \"breastplate, aegis, egis\",\n",
            "    \"462\": \"broom\",\n",
            "    \"463\": \"bucket, pail\",\n",
            "    \"464\": \"buckle\",\n",
            "    \"465\": \"bulletproof vest\",\n",
            "    \"466\": \"bullet train, bullet\",\n",
            "    \"467\": \"butcher shop, meat market\",\n",
            "    \"468\": \"cab, hack, taxi, taxicab\",\n",
            "    \"469\": \"caldron, cauldron\",\n",
            "    \"470\": \"candle, taper, wax light\",\n",
            "    \"471\": \"cannon\",\n",
            "    \"472\": \"canoe\",\n",
            "    \"473\": \"can opener, tin opener\",\n",
            "    \"474\": \"cardigan\",\n",
            "    \"475\": \"car mirror\",\n",
            "    \"476\": \"carousel, carrousel, merry-go-round, roundabout, whirligig\",\n",
            "    \"477\": \"carpenter's kit, tool kit\",\n",
            "    \"478\": \"carton\",\n",
            "    \"479\": \"car wheel\",\n",
            "    \"480\": \"cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM\",\n",
            "    \"481\": \"cassette\",\n",
            "    \"482\": \"cassette player\",\n",
            "    \"483\": \"castle\",\n",
            "    \"484\": \"catamaran\",\n",
            "    \"485\": \"CD player\",\n",
            "    \"486\": \"cello, violoncello\",\n",
            "    \"487\": \"cellular telephone, cellular phone, cellphone, cell, mobile phone\",\n",
            "    \"488\": \"chain\",\n",
            "    \"489\": \"chainlink fence\",\n",
            "    \"490\": \"chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour\",\n",
            "    \"491\": \"chain saw, chainsaw\",\n",
            "    \"492\": \"chest\",\n",
            "    \"493\": \"chiffonier, commode\",\n",
            "    \"494\": \"chime, bell, gong\",\n",
            "    \"495\": \"china cabinet, china closet\",\n",
            "    \"496\": \"Christmas stocking\",\n",
            "    \"497\": \"church, church building\",\n",
            "    \"498\": \"cinema, movie theater, movie theatre, movie house, picture palace\",\n",
            "    \"499\": \"cleaver, meat cleaver, chopper\",\n",
            "    \"500\": \"cliff dwelling\",\n",
            "    \"501\": \"cloak\",\n",
            "    \"502\": \"clog, geta, patten, sabot\",\n",
            "    \"503\": \"cocktail shaker\",\n",
            "    \"504\": \"coffee mug\",\n",
            "    \"505\": \"coffeepot\",\n",
            "    \"506\": \"coil, spiral, volute, whorl, helix\",\n",
            "    \"507\": \"combination lock\",\n",
            "    \"508\": \"computer keyboard, keypad\",\n",
            "    \"509\": \"confectionery, confectionary, candy store\",\n",
            "    \"510\": \"container ship, containership, container vessel\",\n",
            "    \"511\": \"convertible\",\n",
            "    \"512\": \"corkscrew, bottle screw\",\n",
            "    \"513\": \"cornet, horn, trumpet, trump\",\n",
            "    \"514\": \"cowboy boot\",\n",
            "    \"515\": \"cowboy hat, ten-gallon hat\",\n",
            "    \"516\": \"cradle\",\n",
            "    \"517\": \"crane\",\n",
            "    \"518\": \"crash helmet\",\n",
            "    \"519\": \"crate\",\n",
            "    \"520\": \"crib, cot\",\n",
            "    \"521\": \"Crock Pot\",\n",
            "    \"522\": \"croquet ball\",\n",
            "    \"523\": \"crutch\",\n",
            "    \"524\": \"cuirass\",\n",
            "    \"525\": \"dam, dike, dyke\",\n",
            "    \"526\": \"desk\",\n",
            "    \"527\": \"desktop computer\",\n",
            "    \"528\": \"dial telephone, dial phone\",\n",
            "    \"529\": \"diaper, nappy, napkin\",\n",
            "    \"530\": \"digital clock\",\n",
            "    \"531\": \"digital watch\",\n",
            "    \"532\": \"dining table, board\",\n",
            "    \"533\": \"dishrag, dishcloth\",\n",
            "    \"534\": \"dishwasher, dish washer, dishwashing machine\",\n",
            "    \"535\": \"disk brake, disc brake\",\n",
            "    \"536\": \"dock, dockage, docking facility\",\n",
            "    \"537\": \"dogsled, dog sled, dog sleigh\",\n",
            "    \"538\": \"dome\",\n",
            "    \"539\": \"doormat, welcome mat\",\n",
            "    \"540\": \"drilling platform, offshore rig\",\n",
            "    \"541\": \"drum, membranophone, tympan\",\n",
            "    \"542\": \"drumstick\",\n",
            "    \"543\": \"dumbbell\",\n",
            "    \"544\": \"Dutch oven\",\n",
            "    \"545\": \"electric fan, blower\",\n",
            "    \"546\": \"electric guitar\",\n",
            "    \"547\": \"electric locomotive\",\n",
            "    \"548\": \"entertainment center\",\n",
            "    \"549\": \"envelope\",\n",
            "    \"550\": \"espresso maker\",\n",
            "    \"551\": \"face powder\",\n",
            "    \"552\": \"feather boa, boa\",\n",
            "    \"553\": \"file, file cabinet, filing cabinet\",\n",
            "    \"554\": \"fireboat\",\n",
            "    \"555\": \"fire engine, fire truck\",\n",
            "    \"556\": \"fire screen, fireguard\",\n",
            "    \"557\": \"flagpole, flagstaff\",\n",
            "    \"558\": \"flute, transverse flute\",\n",
            "    \"559\": \"folding chair\",\n",
            "    \"560\": \"football helmet\",\n",
            "    \"561\": \"forklift\",\n",
            "    \"562\": \"fountain\",\n",
            "    \"563\": \"fountain pen\",\n",
            "    \"564\": \"four-poster\",\n",
            "    \"565\": \"freight car\",\n",
            "    \"566\": \"French horn, horn\",\n",
            "    \"567\": \"frying pan, frypan, skillet\",\n",
            "    \"568\": \"fur coat\",\n",
            "    \"569\": \"garbage truck, dustcart\",\n",
            "    \"570\": \"gasmask, respirator, gas helmet\",\n",
            "    \"571\": \"gas pump, gasoline pump, petrol pump, island dispenser\",\n",
            "    \"572\": \"goblet\",\n",
            "    \"573\": \"go-kart\",\n",
            "    \"574\": \"golf ball\",\n",
            "    \"575\": \"golfcart, golf cart\",\n",
            "    \"576\": \"gondola\",\n",
            "    \"577\": \"gong, tam-tam\",\n",
            "    \"578\": \"gown\",\n",
            "    \"579\": \"grand piano, grand\",\n",
            "    \"580\": \"greenhouse, nursery, glasshouse\",\n",
            "    \"581\": \"grille, radiator grille\",\n",
            "    \"582\": \"grocery store, grocery, food market, market\",\n",
            "    \"583\": \"guillotine\",\n",
            "    \"584\": \"hair slide\",\n",
            "    \"585\": \"hair spray\",\n",
            "    \"586\": \"half track\",\n",
            "    \"587\": \"hammer\",\n",
            "    \"588\": \"hamper\",\n",
            "    \"589\": \"hand blower, blow dryer, blow drier, hair dryer, hair drier\",\n",
            "    \"590\": \"hand-held computer, hand-held microcomputer\",\n",
            "    \"591\": \"handkerchief, hankie, hanky, hankey\",\n",
            "    \"592\": \"hard disc, hard disk, fixed disk\",\n",
            "    \"593\": \"harmonica, mouth organ, harp, mouth harp\",\n",
            "    \"594\": \"harp\",\n",
            "    \"595\": \"harvester, reaper\",\n",
            "    \"596\": \"hatchet\",\n",
            "    \"597\": \"holster\",\n",
            "    \"598\": \"home theater, home theatre\",\n",
            "    \"599\": \"honeycomb\",\n",
            "    \"600\": \"hook, claw\",\n",
            "    \"601\": \"hoopskirt, crinoline\",\n",
            "    \"602\": \"horizontal bar, high bar\",\n",
            "    \"603\": \"horse cart, horse-cart\",\n",
            "    \"604\": \"hourglass\",\n",
            "    \"605\": \"iPod\",\n",
            "    \"606\": \"iron, smoothing iron\",\n",
            "    \"607\": \"jack-o'-lantern\",\n",
            "    \"608\": \"jean, blue jean, denim\",\n",
            "    \"609\": \"jeep, landrover\",\n",
            "    \"610\": \"jersey, T-shirt, tee shirt\",\n",
            "    \"611\": \"jigsaw puzzle\",\n",
            "    \"612\": \"jinrikisha, ricksha, rickshaw\",\n",
            "    \"613\": \"joystick\",\n",
            "    \"614\": \"kimono\",\n",
            "    \"615\": \"knee pad\",\n",
            "    \"616\": \"knot\",\n",
            "    \"617\": \"lab coat, laboratory coat\",\n",
            "    \"618\": \"ladle\",\n",
            "    \"619\": \"lampshade, lamp shade\",\n",
            "    \"620\": \"laptop, laptop computer\",\n",
            "    \"621\": \"lawn mower, mower\",\n",
            "    \"622\": \"lens cap, lens cover\",\n",
            "    \"623\": \"letter opener, paper knife, paperknife\",\n",
            "    \"624\": \"library\",\n",
            "    \"625\": \"lifeboat\",\n",
            "    \"626\": \"lighter, light, igniter, ignitor\",\n",
            "    \"627\": \"limousine, limo\",\n",
            "    \"628\": \"liner, ocean liner\",\n",
            "    \"629\": \"lipstick, lip rouge\",\n",
            "    \"630\": \"Loafer\",\n",
            "    \"631\": \"lotion\",\n",
            "    \"632\": \"loudspeaker, speaker, speaker unit, loudspeaker system, speaker system\",\n",
            "    \"633\": \"loupe, jeweler's loupe\",\n",
            "    \"634\": \"lumbermill, sawmill\",\n",
            "    \"635\": \"magnetic compass\",\n",
            "    \"636\": \"mailbag, postbag\",\n",
            "    \"637\": \"mailbox, letter box\",\n",
            "    \"638\": \"maillot\",\n",
            "    \"639\": \"maillot, tank suit\",\n",
            "    \"640\": \"manhole cover\",\n",
            "    \"641\": \"maraca\",\n",
            "    \"642\": \"marimba, xylophone\",\n",
            "    \"643\": \"mask\",\n",
            "    \"644\": \"matchstick\",\n",
            "    \"645\": \"maypole\",\n",
            "    \"646\": \"maze, labyrinth\",\n",
            "    \"647\": \"measuring cup\",\n",
            "    \"648\": \"medicine chest, medicine cabinet\",\n",
            "    \"649\": \"megalith, megalithic structure\",\n",
            "    \"650\": \"microphone, mike\",\n",
            "    \"651\": \"microwave, microwave oven\",\n",
            "    \"652\": \"military uniform\",\n",
            "    \"653\": \"milk can\",\n",
            "    \"654\": \"minibus\",\n",
            "    \"655\": \"miniskirt, mini\",\n",
            "    \"656\": \"minivan\",\n",
            "    \"657\": \"missile\",\n",
            "    \"658\": \"mitten\",\n",
            "    \"659\": \"mixing bowl\",\n",
            "    \"660\": \"mobile home, manufactured home\",\n",
            "    \"661\": \"Model T\",\n",
            "    \"662\": \"modem\",\n",
            "    \"663\": \"monastery\",\n",
            "    \"664\": \"monitor\",\n",
            "    \"665\": \"moped\",\n",
            "    \"666\": \"mortar\",\n",
            "    \"667\": \"mortarboard\",\n",
            "    \"668\": \"mosque\",\n",
            "    \"669\": \"mosquito net\",\n",
            "    \"670\": \"motor scooter, scooter\",\n",
            "    \"671\": \"mountain bike, all-terrain bike, off-roader\",\n",
            "    \"672\": \"mountain tent\",\n",
            "    \"673\": \"mouse, computer mouse\",\n",
            "    \"674\": \"mousetrap\",\n",
            "    \"675\": \"moving van\",\n",
            "    \"676\": \"muzzle\",\n",
            "    \"677\": \"nail\",\n",
            "    \"678\": \"neck brace\",\n",
            "    \"679\": \"necklace\",\n",
            "    \"680\": \"nipple\",\n",
            "    \"681\": \"notebook, notebook computer\",\n",
            "    \"682\": \"obelisk\",\n",
            "    \"683\": \"oboe, hautboy, hautbois\",\n",
            "    \"684\": \"ocarina, sweet potato\",\n",
            "    \"685\": \"odometer, hodometer, mileometer, milometer\",\n",
            "    \"686\": \"oil filter\",\n",
            "    \"687\": \"organ, pipe organ\",\n",
            "    \"688\": \"oscilloscope, scope, cathode-ray oscilloscope, CRO\",\n",
            "    \"689\": \"overskirt\",\n",
            "    \"690\": \"oxcart\",\n",
            "    \"691\": \"oxygen mask\",\n",
            "    \"692\": \"packet\",\n",
            "    \"693\": \"paddle, boat paddle\",\n",
            "    \"694\": \"paddlewheel, paddle wheel\",\n",
            "    \"695\": \"padlock\",\n",
            "    \"696\": \"paintbrush\",\n",
            "    \"697\": \"pajama, pyjama, pj's, jammies\",\n",
            "    \"698\": \"palace\",\n",
            "    \"699\": \"panpipe, pandean pipe, syrinx\",\n",
            "    \"700\": \"paper towel\",\n",
            "    \"701\": \"parachute, chute\",\n",
            "    \"702\": \"parallel bars, bars\",\n",
            "    \"703\": \"park bench\",\n",
            "    \"704\": \"parking meter\",\n",
            "    \"705\": \"passenger car, coach, carriage\",\n",
            "    \"706\": \"patio, terrace\",\n",
            "    \"707\": \"pay-phone, pay-station\",\n",
            "    \"708\": \"pedestal, plinth, footstall\",\n",
            "    \"709\": \"pencil box, pencil case\",\n",
            "    \"710\": \"pencil sharpener\",\n",
            "    \"711\": \"perfume, essence\",\n",
            "    \"712\": \"Petri dish\",\n",
            "    \"713\": \"photocopier\",\n",
            "    \"714\": \"pick, plectrum, plectron\",\n",
            "    \"715\": \"pickelhaube\",\n",
            "    \"716\": \"picket fence, paling\",\n",
            "    \"717\": \"pickup, pickup truck\",\n",
            "    \"718\": \"pier\",\n",
            "    \"719\": \"piggy bank, penny bank\",\n",
            "    \"720\": \"pill bottle\",\n",
            "    \"721\": \"pillow\",\n",
            "    \"722\": \"ping-pong ball\",\n",
            "    \"723\": \"pinwheel\",\n",
            "    \"724\": \"pirate, pirate ship\",\n",
            "    \"725\": \"pitcher, ewer\",\n",
            "    \"726\": \"plane, carpenter's plane, woodworking plane\",\n",
            "    \"727\": \"planetarium\",\n",
            "    \"728\": \"plastic bag\",\n",
            "    \"729\": \"plate rack\",\n",
            "    \"730\": \"plow, plough\",\n",
            "    \"731\": \"plunger, plumber's helper\",\n",
            "    \"732\": \"Polaroid camera, Polaroid Land camera\",\n",
            "    \"733\": \"pole\",\n",
            "    \"734\": \"police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria\",\n",
            "    \"735\": \"poncho\",\n",
            "    \"736\": \"pool table, billiard table, snooker table\",\n",
            "    \"737\": \"pop bottle, soda bottle\",\n",
            "    \"738\": \"pot, flowerpot\",\n",
            "    \"739\": \"potter's wheel\",\n",
            "    \"740\": \"power drill\",\n",
            "    \"741\": \"prayer rug, prayer mat\",\n",
            "    \"742\": \"printer\",\n",
            "    \"743\": \"prison, prison house\",\n",
            "    \"744\": \"projectile, missile\",\n",
            "    \"745\": \"projector\",\n",
            "    \"746\": \"puck, hockey puck\",\n",
            "    \"747\": \"punching bag, punch bag, punching ball, punchball\",\n",
            "    \"748\": \"purse\",\n",
            "    \"749\": \"quill, quill pen\",\n",
            "    \"750\": \"quilt, comforter, comfort, puff\",\n",
            "    \"751\": \"racer, race car, racing car\",\n",
            "    \"752\": \"racket, racquet\",\n",
            "    \"753\": \"radiator\",\n",
            "    \"754\": \"radio, wireless\",\n",
            "    \"755\": \"radio telescope, radio reflector\",\n",
            "    \"756\": \"rain barrel\",\n",
            "    \"757\": \"recreational vehicle, RV, R.V.\",\n",
            "    \"758\": \"reel\",\n",
            "    \"759\": \"reflex camera\",\n",
            "    \"760\": \"refrigerator, icebox\",\n",
            "    \"761\": \"remote control, remote\",\n",
            "    \"762\": \"restaurant, eating house, eating place, eatery\",\n",
            "    \"763\": \"revolver, six-gun, six-shooter\",\n",
            "    \"764\": \"rifle\",\n",
            "    \"765\": \"rocking chair, rocker\",\n",
            "    \"766\": \"rotisserie\",\n",
            "    \"767\": \"rubber eraser, rubber, pencil eraser\",\n",
            "    \"768\": \"rugby ball\",\n",
            "    \"769\": \"rule, ruler\",\n",
            "    \"770\": \"running shoe\",\n",
            "    \"771\": \"safe\",\n",
            "    \"772\": \"safety pin\",\n",
            "    \"773\": \"saltshaker, salt shaker\",\n",
            "    \"774\": \"sandal\",\n",
            "    \"775\": \"sarong\",\n",
            "    \"776\": \"sax, saxophone\",\n",
            "    \"777\": \"scabbard\",\n",
            "    \"778\": \"scale, weighing machine\",\n",
            "    \"779\": \"school bus\",\n",
            "    \"780\": \"schooner\",\n",
            "    \"781\": \"scoreboard\",\n",
            "    \"782\": \"screen, CRT screen\",\n",
            "    \"783\": \"screw\",\n",
            "    \"784\": \"screwdriver\",\n",
            "    \"785\": \"seat belt, seatbelt\",\n",
            "    \"786\": \"sewing machine\",\n",
            "    \"787\": \"shield, buckler\",\n",
            "    \"788\": \"shoe shop, shoe-shop, shoe store\",\n",
            "    \"789\": \"shoji\",\n",
            "    \"790\": \"shopping basket\",\n",
            "    \"791\": \"shopping cart\",\n",
            "    \"792\": \"shovel\",\n",
            "    \"793\": \"shower cap\",\n",
            "    \"794\": \"shower curtain\",\n",
            "    \"795\": \"ski\",\n",
            "    \"796\": \"ski mask\",\n",
            "    \"797\": \"sleeping bag\",\n",
            "    \"798\": \"slide rule, slipstick\",\n",
            "    \"799\": \"sliding door\",\n",
            "    \"800\": \"slot, one-armed bandit\",\n",
            "    \"801\": \"snorkel\",\n",
            "    \"802\": \"snowmobile\",\n",
            "    \"803\": \"snowplow, snowplough\",\n",
            "    \"804\": \"soap dispenser\",\n",
            "    \"805\": \"soccer ball\",\n",
            "    \"806\": \"sock\",\n",
            "    \"807\": \"solar dish, solar collector, solar furnace\",\n",
            "    \"808\": \"sombrero\",\n",
            "    \"809\": \"soup bowl\",\n",
            "    \"810\": \"space bar\",\n",
            "    \"811\": \"space heater\",\n",
            "    \"812\": \"space shuttle\",\n",
            "    \"813\": \"spatula\",\n",
            "    \"814\": \"speedboat\",\n",
            "    \"815\": \"spider web, spider's web\",\n",
            "    \"816\": \"spindle\",\n",
            "    \"817\": \"sports car, sport car\",\n",
            "    \"818\": \"spotlight, spot\",\n",
            "    \"819\": \"stage\",\n",
            "    \"820\": \"steam locomotive\",\n",
            "    \"821\": \"steel arch bridge\",\n",
            "    \"822\": \"steel drum\",\n",
            "    \"823\": \"stethoscope\",\n",
            "    \"824\": \"stole\",\n",
            "    \"825\": \"stone wall\",\n",
            "    \"826\": \"stopwatch, stop watch\",\n",
            "    \"827\": \"stove\",\n",
            "    \"828\": \"strainer\",\n",
            "    \"829\": \"streetcar, tram, tramcar, trolley, trolley car\",\n",
            "    \"830\": \"stretcher\",\n",
            "    \"831\": \"studio couch, day bed\",\n",
            "    \"832\": \"stupa, tope\",\n",
            "    \"833\": \"submarine, pigboat, sub, U-boat\",\n",
            "    \"834\": \"suit, suit of clothes\",\n",
            "    \"835\": \"sundial\",\n",
            "    \"836\": \"sunglass\",\n",
            "    \"837\": \"sunglasses, dark glasses, shades\",\n",
            "    \"838\": \"sunscreen, sunblock, sun blocker\",\n",
            "    \"839\": \"suspension bridge\",\n",
            "    \"840\": \"swab, swob, mop\",\n",
            "    \"841\": \"sweatshirt\",\n",
            "    \"842\": \"swimming trunks, bathing trunks\",\n",
            "    \"843\": \"swing\",\n",
            "    \"844\": \"switch, electric switch, electrical switch\",\n",
            "    \"845\": \"syringe\",\n",
            "    \"846\": \"table lamp\",\n",
            "    \"847\": \"tank, army tank, armored combat vehicle, armoured combat vehicle\",\n",
            "    \"848\": \"tape player\",\n",
            "    \"849\": \"teapot\",\n",
            "    \"850\": \"teddy, teddy bear\",\n",
            "    \"851\": \"television, television system\",\n",
            "    \"852\": \"tennis ball\",\n",
            "    \"853\": \"thatch, thatched roof\",\n",
            "    \"854\": \"theater curtain, theatre curtain\",\n",
            "    \"855\": \"thimble\",\n",
            "    \"856\": \"thresher, thrasher, threshing machine\",\n",
            "    \"857\": \"throne\",\n",
            "    \"858\": \"tile roof\",\n",
            "    \"859\": \"toaster\",\n",
            "    \"860\": \"tobacco shop, tobacconist shop, tobacconist\",\n",
            "    \"861\": \"toilet seat\",\n",
            "    \"862\": \"torch\",\n",
            "    \"863\": \"totem pole\",\n",
            "    \"864\": \"tow truck, tow car, wrecker\",\n",
            "    \"865\": \"toyshop\",\n",
            "    \"866\": \"tractor\",\n",
            "    \"867\": \"trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi\",\n",
            "    \"868\": \"tray\",\n",
            "    \"869\": \"trench coat\",\n",
            "    \"870\": \"tricycle, trike, velocipede\",\n",
            "    \"871\": \"trimaran\",\n",
            "    \"872\": \"tripod\",\n",
            "    \"873\": \"triumphal arch\",\n",
            "    \"874\": \"trolleybus, trolley coach, trackless trolley\",\n",
            "    \"875\": \"trombone\",\n",
            "    \"876\": \"tub, vat\",\n",
            "    \"877\": \"turnstile\",\n",
            "    \"878\": \"typewriter keyboard\",\n",
            "    \"879\": \"umbrella\",\n",
            "    \"880\": \"unicycle, monocycle\",\n",
            "    \"881\": \"upright, upright piano\",\n",
            "    \"882\": \"vacuum, vacuum cleaner\",\n",
            "    \"883\": \"vase\",\n",
            "    \"884\": \"vault\",\n",
            "    \"885\": \"velvet\",\n",
            "    \"886\": \"vending machine\",\n",
            "    \"887\": \"vestment\",\n",
            "    \"888\": \"viaduct\",\n",
            "    \"889\": \"violin, fiddle\",\n",
            "    \"890\": \"volleyball\",\n",
            "    \"891\": \"waffle iron\",\n",
            "    \"892\": \"wall clock\",\n",
            "    \"893\": \"wallet, billfold, notecase, pocketbook\",\n",
            "    \"894\": \"wardrobe, closet, press\",\n",
            "    \"895\": \"warplane, military plane\",\n",
            "    \"896\": \"washbasin, handbasin, washbowl, lavabo, wash-hand basin\",\n",
            "    \"897\": \"washer, automatic washer, washing machine\",\n",
            "    \"898\": \"water bottle\",\n",
            "    \"899\": \"water jug\",\n",
            "    \"900\": \"water tower\",\n",
            "    \"901\": \"whiskey jug\",\n",
            "    \"902\": \"whistle\",\n",
            "    \"903\": \"wig\",\n",
            "    \"904\": \"window screen\",\n",
            "    \"905\": \"window shade\",\n",
            "    \"906\": \"Windsor tie\",\n",
            "    \"907\": \"wine bottle\",\n",
            "    \"908\": \"wing\",\n",
            "    \"909\": \"wok\",\n",
            "    \"910\": \"wooden spoon\",\n",
            "    \"911\": \"wool, woolen, woollen\",\n",
            "    \"912\": \"worm fence, snake fence, snake-rail fence, Virginia fence\",\n",
            "    \"913\": \"wreck\",\n",
            "    \"914\": \"yawl\",\n",
            "    \"915\": \"yurt\",\n",
            "    \"916\": \"web site, website, internet site, site\",\n",
            "    \"917\": \"comic book\",\n",
            "    \"918\": \"crossword puzzle, crossword\",\n",
            "    \"919\": \"street sign\",\n",
            "    \"920\": \"traffic light, traffic signal, stoplight\",\n",
            "    \"921\": \"book jacket, dust cover, dust jacket, dust wrapper\",\n",
            "    \"922\": \"menu\",\n",
            "    \"923\": \"plate\",\n",
            "    \"924\": \"guacamole\",\n",
            "    \"925\": \"consomme\",\n",
            "    \"926\": \"hot pot, hotpot\",\n",
            "    \"927\": \"trifle\",\n",
            "    \"928\": \"ice cream, icecream\",\n",
            "    \"929\": \"ice lolly, lolly, lollipop, popsicle\",\n",
            "    \"930\": \"French loaf\",\n",
            "    \"931\": \"bagel, beigel\",\n",
            "    \"932\": \"pretzel\",\n",
            "    \"933\": \"cheeseburger\",\n",
            "    \"934\": \"hotdog, hot dog, red hot\",\n",
            "    \"935\": \"mashed potato\",\n",
            "    \"936\": \"head cabbage\",\n",
            "    \"937\": \"broccoli\",\n",
            "    \"938\": \"cauliflower\",\n",
            "    \"939\": \"zucchini, courgette\",\n",
            "    \"940\": \"spaghetti squash\",\n",
            "    \"941\": \"acorn squash\",\n",
            "    \"942\": \"butternut squash\",\n",
            "    \"943\": \"cucumber, cuke\",\n",
            "    \"944\": \"artichoke, globe artichoke\",\n",
            "    \"945\": \"bell pepper\",\n",
            "    \"946\": \"cardoon\",\n",
            "    \"947\": \"mushroom\",\n",
            "    \"948\": \"Granny Smith\",\n",
            "    \"949\": \"strawberry\",\n",
            "    \"950\": \"orange\",\n",
            "    \"951\": \"lemon\",\n",
            "    \"952\": \"fig\",\n",
            "    \"953\": \"pineapple, ananas\",\n",
            "    \"954\": \"banana\",\n",
            "    \"955\": \"jackfruit, jak, jack\",\n",
            "    \"956\": \"custard apple\",\n",
            "    \"957\": \"pomegranate\",\n",
            "    \"958\": \"hay\",\n",
            "    \"959\": \"carbonara\",\n",
            "    \"960\": \"chocolate sauce, chocolate syrup\",\n",
            "    \"961\": \"dough\",\n",
            "    \"962\": \"meat loaf, meatloaf\",\n",
            "    \"963\": \"pizza, pizza pie\",\n",
            "    \"964\": \"potpie\",\n",
            "    \"965\": \"burrito\",\n",
            "    \"966\": \"red wine\",\n",
            "    \"967\": \"espresso\",\n",
            "    \"968\": \"cup\",\n",
            "    \"969\": \"eggnog\",\n",
            "    \"970\": \"alp\",\n",
            "    \"971\": \"bubble\",\n",
            "    \"972\": \"cliff, drop, drop-off\",\n",
            "    \"973\": \"coral reef\",\n",
            "    \"974\": \"geyser\",\n",
            "    \"975\": \"lakeside, lakeshore\",\n",
            "    \"976\": \"promontory, headland, head, foreland\",\n",
            "    \"977\": \"sandbar, sand bar\",\n",
            "    \"978\": \"seashore, coast, seacoast, sea-coast\",\n",
            "    \"979\": \"valley, vale\",\n",
            "    \"980\": \"volcano\",\n",
            "    \"981\": \"ballplayer, baseball player\",\n",
            "    \"982\": \"groom, bridegroom\",\n",
            "    \"983\": \"scuba diver\",\n",
            "    \"984\": \"rapeseed\",\n",
            "    \"985\": \"daisy\",\n",
            "    \"986\": \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\",\n",
            "    \"987\": \"corn\",\n",
            "    \"988\": \"acorn\",\n",
            "    \"989\": \"hip, rose hip, rosehip\",\n",
            "    \"990\": \"buckeye, horse chestnut, conker\",\n",
            "    \"991\": \"coral fungus\",\n",
            "    \"992\": \"agaric\",\n",
            "    \"993\": \"gyromitra\",\n",
            "    \"994\": \"stinkhorn, carrion fungus\",\n",
            "    \"995\": \"earthstar\",\n",
            "    \"996\": \"hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa\",\n",
            "    \"997\": \"bolete\",\n",
            "    \"998\": \"ear, spike, capitulum\",\n",
            "    \"999\": \"toilet tissue, toilet paper, bathroom tissue\"\n",
            "  },\n",
            "  \"image_size\": 224,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"Afghan hound, Afghan\": 160,\n",
            "    \"African chameleon, Chamaeleo chamaeleon\": 47,\n",
            "    \"African crocodile, Nile crocodile, Crocodylus niloticus\": 49,\n",
            "    \"African elephant, Loxodonta africana\": 386,\n",
            "    \"African grey, African gray, Psittacus erithacus\": 87,\n",
            "    \"African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus\": 275,\n",
            "    \"Airedale, Airedale terrier\": 191,\n",
            "    \"American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier\": 180,\n",
            "    \"American alligator, Alligator mississipiensis\": 50,\n",
            "    \"American black bear, black bear, Ursus americanus, Euarctos americanus\": 295,\n",
            "    \"American chameleon, anole, Anolis carolinensis\": 40,\n",
            "    \"American coot, marsh hen, mud hen, water hen, Fulica americana\": 137,\n",
            "    \"American egret, great white heron, Egretta albus\": 132,\n",
            "    \"American lobster, Northern lobster, Maine lobster, Homarus americanus\": 122,\n",
            "    \"Angora, Angora rabbit\": 332,\n",
            "    \"Appenzeller\": 240,\n",
            "    \"Arabian camel, dromedary, Camelus dromedarius\": 354,\n",
            "    \"Arctic fox, white fox, Alopex lagopus\": 279,\n",
            "    \"Australian terrier\": 193,\n",
            "    \"Band Aid\": 419,\n",
            "    \"Bedlington terrier\": 181,\n",
            "    \"Bernese mountain dog\": 239,\n",
            "    \"Blenheim spaniel\": 156,\n",
            "    \"Border collie\": 232,\n",
            "    \"Border terrier\": 182,\n",
            "    \"Boston bull, Boston terrier\": 195,\n",
            "    \"Bouvier des Flandres, Bouviers des Flandres\": 233,\n",
            "    \"Brabancon griffon\": 262,\n",
            "    \"Brittany spaniel\": 215,\n",
            "    \"CD player\": 485,\n",
            "    \"Cardigan, Cardigan Welsh corgi\": 264,\n",
            "    \"Chesapeake Bay retriever\": 209,\n",
            "    \"Chihuahua\": 151,\n",
            "    \"Christmas stocking\": 496,\n",
            "    \"Crock Pot\": 521,\n",
            "    \"Dandie Dinmont, Dandie Dinmont terrier\": 194,\n",
            "    \"Doberman, Doberman pinscher\": 236,\n",
            "    \"Dungeness crab, Cancer magister\": 118,\n",
            "    \"Dutch oven\": 544,\n",
            "    \"Egyptian cat\": 285,\n",
            "    \"English foxhound\": 167,\n",
            "    \"English setter\": 212,\n",
            "    \"English springer, English springer spaniel\": 217,\n",
            "    \"EntleBucher\": 241,\n",
            "    \"Eskimo dog, husky\": 248,\n",
            "    \"European fire salamander, Salamandra salamandra\": 25,\n",
            "    \"European gallinule, Porphyrio porphyrio\": 136,\n",
            "    \"French bulldog\": 245,\n",
            "    \"French horn, horn\": 566,\n",
            "    \"French loaf\": 930,\n",
            "    \"German shepherd, German shepherd dog, German police dog, alsatian\": 235,\n",
            "    \"German short-haired pointer\": 210,\n",
            "    \"Gila monster, Heloderma suspectum\": 45,\n",
            "    \"Gordon setter\": 214,\n",
            "    \"Granny Smith\": 948,\n",
            "    \"Great Dane\": 246,\n",
            "    \"Great Pyrenees\": 257,\n",
            "    \"Greater Swiss Mountain dog\": 238,\n",
            "    \"Ibizan hound, Ibizan Podenco\": 173,\n",
            "    \"Indian cobra, Naja naja\": 63,\n",
            "    \"Indian elephant, Elephas maximus\": 385,\n",
            "    \"Irish setter, red setter\": 213,\n",
            "    \"Irish terrier\": 184,\n",
            "    \"Irish water spaniel\": 221,\n",
            "    \"Irish wolfhound\": 170,\n",
            "    \"Italian greyhound\": 171,\n",
            "    \"Japanese spaniel\": 152,\n",
            "    \"Kerry blue terrier\": 183,\n",
            "    \"Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis\": 48,\n",
            "    \"Labrador retriever\": 208,\n",
            "    \"Lakeland terrier\": 189,\n",
            "    \"Leonberg\": 255,\n",
            "    \"Lhasa, Lhasa apso\": 204,\n",
            "    \"Loafer\": 630,\n",
            "    \"Madagascar cat, ring-tailed lemur, Lemur catta\": 383,\n",
            "    \"Maltese dog, Maltese terrier, Maltese\": 153,\n",
            "    \"Mexican hairless\": 268,\n",
            "    \"Model T\": 661,\n",
            "    \"Newfoundland, Newfoundland dog\": 256,\n",
            "    \"Norfolk terrier\": 185,\n",
            "    \"Norwegian elkhound, elkhound\": 174,\n",
            "    \"Norwich terrier\": 186,\n",
            "    \"Old English sheepdog, bobtail\": 229,\n",
            "    \"Pekinese, Pekingese, Peke\": 154,\n",
            "    \"Pembroke, Pembroke Welsh corgi\": 263,\n",
            "    \"Persian cat\": 283,\n",
            "    \"Petri dish\": 712,\n",
            "    \"Polaroid camera, Polaroid Land camera\": 732,\n",
            "    \"Pomeranian\": 259,\n",
            "    \"Rhodesian ridgeback\": 159,\n",
            "    \"Rottweiler\": 234,\n",
            "    \"Saint Bernard, St Bernard\": 247,\n",
            "    \"Saluki, gazelle hound\": 176,\n",
            "    \"Samoyed, Samoyede\": 258,\n",
            "    \"Scotch terrier, Scottish terrier, Scottie\": 199,\n",
            "    \"Scottish deerhound, deerhound\": 177,\n",
            "    \"Sealyham terrier, Sealyham\": 190,\n",
            "    \"Shetland sheepdog, Shetland sheep dog, Shetland\": 230,\n",
            "    \"Shih-Tzu\": 155,\n",
            "    \"Siamese cat, Siamese\": 284,\n",
            "    \"Siberian husky\": 250,\n",
            "    \"Staffordshire bullterrier, Staffordshire bull terrier\": 179,\n",
            "    \"Sussex spaniel\": 220,\n",
            "    \"Tibetan mastiff\": 244,\n",
            "    \"Tibetan terrier, chrysanthemum dog\": 200,\n",
            "    \"Walker hound, Walker foxhound\": 166,\n",
            "    \"Weimaraner\": 178,\n",
            "    \"Welsh springer spaniel\": 218,\n",
            "    \"West Highland white terrier\": 203,\n",
            "    \"Windsor tie\": 906,\n",
            "    \"Yorkshire terrier\": 187,\n",
            "    \"abacus\": 398,\n",
            "    \"abaya\": 399,\n",
            "    \"academic gown, academic robe, judge's robe\": 400,\n",
            "    \"accordion, piano accordion, squeeze box\": 401,\n",
            "    \"acorn\": 988,\n",
            "    \"acorn squash\": 941,\n",
            "    \"acoustic guitar\": 402,\n",
            "    \"admiral\": 321,\n",
            "    \"affenpinscher, monkey pinscher, monkey dog\": 252,\n",
            "    \"agama\": 42,\n",
            "    \"agaric\": 992,\n",
            "    \"aircraft carrier, carrier, flattop, attack aircraft carrier\": 403,\n",
            "    \"airliner\": 404,\n",
            "    \"airship, dirigible\": 405,\n",
            "    \"albatross, mollymawk\": 146,\n",
            "    \"alligator lizard\": 44,\n",
            "    \"alp\": 970,\n",
            "    \"altar\": 406,\n",
            "    \"ambulance\": 407,\n",
            "    \"amphibian, amphibious vehicle\": 408,\n",
            "    \"analog clock\": 409,\n",
            "    \"anemone fish\": 393,\n",
            "    \"ant, emmet, pismire\": 310,\n",
            "    \"apiary, bee house\": 410,\n",
            "    \"apron\": 411,\n",
            "    \"armadillo\": 363,\n",
            "    \"artichoke, globe artichoke\": 944,\n",
            "    \"ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin\": 412,\n",
            "    \"assault rifle, assault gun\": 413,\n",
            "    \"axolotl, mud puppy, Ambystoma mexicanum\": 29,\n",
            "    \"baboon\": 372,\n",
            "    \"backpack, back pack, knapsack, packsack, rucksack, haversack\": 414,\n",
            "    \"badger\": 362,\n",
            "    \"bagel, beigel\": 931,\n",
            "    \"bakery, bakeshop, bakehouse\": 415,\n",
            "    \"balance beam, beam\": 416,\n",
            "    \"bald eagle, American eagle, Haliaeetus leucocephalus\": 22,\n",
            "    \"balloon\": 417,\n",
            "    \"ballplayer, baseball player\": 981,\n",
            "    \"ballpoint, ballpoint pen, ballpen, Biro\": 418,\n",
            "    \"banana\": 954,\n",
            "    \"banded gecko\": 38,\n",
            "    \"banjo\": 420,\n",
            "    \"bannister, banister, balustrade, balusters, handrail\": 421,\n",
            "    \"barbell\": 422,\n",
            "    \"barber chair\": 423,\n",
            "    \"barbershop\": 424,\n",
            "    \"barn\": 425,\n",
            "    \"barn spider, Araneus cavaticus\": 73,\n",
            "    \"barometer\": 426,\n",
            "    \"barracouta, snoek\": 389,\n",
            "    \"barrel, cask\": 427,\n",
            "    \"barrow, garden cart, lawn cart, wheelbarrow\": 428,\n",
            "    \"baseball\": 429,\n",
            "    \"basenji\": 253,\n",
            "    \"basketball\": 430,\n",
            "    \"basset, basset hound\": 161,\n",
            "    \"bassinet\": 431,\n",
            "    \"bassoon\": 432,\n",
            "    \"bath towel\": 434,\n",
            "    \"bathing cap, swimming cap\": 433,\n",
            "    \"bathtub, bathing tub, bath, tub\": 435,\n",
            "    \"beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon\": 436,\n",
            "    \"beacon, lighthouse, beacon light, pharos\": 437,\n",
            "    \"beagle\": 162,\n",
            "    \"beaker\": 438,\n",
            "    \"bearskin, busby, shako\": 439,\n",
            "    \"beaver\": 337,\n",
            "    \"bee\": 309,\n",
            "    \"bee eater\": 92,\n",
            "    \"beer bottle\": 440,\n",
            "    \"beer glass\": 441,\n",
            "    \"bell cote, bell cot\": 442,\n",
            "    \"bell pepper\": 945,\n",
            "    \"bib\": 443,\n",
            "    \"bicycle-built-for-two, tandem bicycle, tandem\": 444,\n",
            "    \"bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis\": 349,\n",
            "    \"bikini, two-piece\": 445,\n",
            "    \"binder, ring-binder\": 446,\n",
            "    \"binoculars, field glasses, opera glasses\": 447,\n",
            "    \"birdhouse\": 448,\n",
            "    \"bison\": 347,\n",
            "    \"bittern\": 133,\n",
            "    \"black and gold garden spider, Argiope aurantia\": 72,\n",
            "    \"black grouse\": 80,\n",
            "    \"black stork, Ciconia nigra\": 128,\n",
            "    \"black swan, Cygnus atratus\": 100,\n",
            "    \"black widow, Latrodectus mactans\": 75,\n",
            "    \"black-and-tan coonhound\": 165,\n",
            "    \"black-footed ferret, ferret, Mustela nigripes\": 359,\n",
            "    \"bloodhound, sleuthhound\": 163,\n",
            "    \"bluetick\": 164,\n",
            "    \"boa constrictor, Constrictor constrictor\": 61,\n",
            "    \"boathouse\": 449,\n",
            "    \"bobsled, bobsleigh, bob\": 450,\n",
            "    \"bolete\": 997,\n",
            "    \"bolo tie, bolo, bola tie, bola\": 451,\n",
            "    \"bonnet, poke bonnet\": 452,\n",
            "    \"book jacket, dust cover, dust jacket, dust wrapper\": 921,\n",
            "    \"bookcase\": 453,\n",
            "    \"bookshop, bookstore, bookstall\": 454,\n",
            "    \"borzoi, Russian wolfhound\": 169,\n",
            "    \"bottlecap\": 455,\n",
            "    \"bow\": 456,\n",
            "    \"bow tie, bow-tie, bowtie\": 457,\n",
            "    \"box turtle, box tortoise\": 37,\n",
            "    \"boxer\": 242,\n",
            "    \"brain coral\": 109,\n",
            "    \"brambling, Fringilla montifringilla\": 10,\n",
            "    \"brass, memorial tablet, plaque\": 458,\n",
            "    \"brassiere, bra, bandeau\": 459,\n",
            "    \"breakwater, groin, groyne, mole, bulwark, seawall, jetty\": 460,\n",
            "    \"breastplate, aegis, egis\": 461,\n",
            "    \"briard\": 226,\n",
            "    \"broccoli\": 937,\n",
            "    \"broom\": 462,\n",
            "    \"brown bear, bruin, Ursus arctos\": 294,\n",
            "    \"bubble\": 971,\n",
            "    \"bucket, pail\": 463,\n",
            "    \"buckeye, horse chestnut, conker\": 990,\n",
            "    \"buckle\": 464,\n",
            "    \"bulbul\": 16,\n",
            "    \"bull mastiff\": 243,\n",
            "    \"bullet train, bullet\": 466,\n",
            "    \"bulletproof vest\": 465,\n",
            "    \"bullfrog, Rana catesbeiana\": 30,\n",
            "    \"burrito\": 965,\n",
            "    \"bustard\": 138,\n",
            "    \"butcher shop, meat market\": 467,\n",
            "    \"butternut squash\": 942,\n",
            "    \"cab, hack, taxi, taxicab\": 468,\n",
            "    \"cabbage butterfly\": 324,\n",
            "    \"cairn, cairn terrier\": 192,\n",
            "    \"caldron, cauldron\": 469,\n",
            "    \"can opener, tin opener\": 473,\n",
            "    \"candle, taper, wax light\": 470,\n",
            "    \"cannon\": 471,\n",
            "    \"canoe\": 472,\n",
            "    \"capuchin, ringtail, Cebus capucinus\": 378,\n",
            "    \"car mirror\": 475,\n",
            "    \"car wheel\": 479,\n",
            "    \"carbonara\": 959,\n",
            "    \"cardigan\": 474,\n",
            "    \"cardoon\": 946,\n",
            "    \"carousel, carrousel, merry-go-round, roundabout, whirligig\": 476,\n",
            "    \"carpenter's kit, tool kit\": 477,\n",
            "    \"carton\": 478,\n",
            "    \"cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM\": 480,\n",
            "    \"cassette\": 481,\n",
            "    \"cassette player\": 482,\n",
            "    \"castle\": 483,\n",
            "    \"catamaran\": 484,\n",
            "    \"cauliflower\": 938,\n",
            "    \"cello, violoncello\": 486,\n",
            "    \"cellular telephone, cellular phone, cellphone, cell, mobile phone\": 487,\n",
            "    \"centipede\": 79,\n",
            "    \"chain\": 488,\n",
            "    \"chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour\": 490,\n",
            "    \"chain saw, chainsaw\": 491,\n",
            "    \"chainlink fence\": 489,\n",
            "    \"chambered nautilus, pearly nautilus, nautilus\": 117,\n",
            "    \"cheeseburger\": 933,\n",
            "    \"cheetah, chetah, Acinonyx jubatus\": 293,\n",
            "    \"chest\": 492,\n",
            "    \"chickadee\": 19,\n",
            "    \"chiffonier, commode\": 493,\n",
            "    \"chime, bell, gong\": 494,\n",
            "    \"chimpanzee, chimp, Pan troglodytes\": 367,\n",
            "    \"china cabinet, china closet\": 495,\n",
            "    \"chiton, coat-of-mail shell, sea cradle, polyplacophore\": 116,\n",
            "    \"chocolate sauce, chocolate syrup\": 960,\n",
            "    \"chow, chow chow\": 260,\n",
            "    \"church, church building\": 497,\n",
            "    \"cicada, cicala\": 316,\n",
            "    \"cinema, movie theater, movie theatre, movie house, picture palace\": 498,\n",
            "    \"cleaver, meat cleaver, chopper\": 499,\n",
            "    \"cliff dwelling\": 500,\n",
            "    \"cliff, drop, drop-off\": 972,\n",
            "    \"cloak\": 501,\n",
            "    \"clog, geta, patten, sabot\": 502,\n",
            "    \"clumber, clumber spaniel\": 216,\n",
            "    \"cock\": 7,\n",
            "    \"cocker spaniel, English cocker spaniel, cocker\": 219,\n",
            "    \"cockroach, roach\": 314,\n",
            "    \"cocktail shaker\": 503,\n",
            "    \"coffee mug\": 504,\n",
            "    \"coffeepot\": 505,\n",
            "    \"coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch\": 391,\n",
            "    \"coil, spiral, volute, whorl, helix\": 506,\n",
            "    \"collie\": 231,\n",
            "    \"colobus, colobus monkey\": 375,\n",
            "    \"combination lock\": 507,\n",
            "    \"comic book\": 917,\n",
            "    \"common iguana, iguana, Iguana iguana\": 39,\n",
            "    \"common newt, Triturus vulgaris\": 26,\n",
            "    \"computer keyboard, keypad\": 508,\n",
            "    \"conch\": 112,\n",
            "    \"confectionery, confectionary, candy store\": 509,\n",
            "    \"consomme\": 925,\n",
            "    \"container ship, containership, container vessel\": 510,\n",
            "    \"convertible\": 511,\n",
            "    \"coral fungus\": 991,\n",
            "    \"coral reef\": 973,\n",
            "    \"corkscrew, bottle screw\": 512,\n",
            "    \"corn\": 987,\n",
            "    \"cornet, horn, trumpet, trump\": 513,\n",
            "    \"coucal\": 91,\n",
            "    \"cougar, puma, catamount, mountain lion, painter, panther, Felis concolor\": 286,\n",
            "    \"cowboy boot\": 514,\n",
            "    \"cowboy hat, ten-gallon hat\": 515,\n",
            "    \"coyote, prairie wolf, brush wolf, Canis latrans\": 272,\n",
            "    \"cradle\": 516,\n",
            "    \"crane\": 517,\n",
            "    \"crash helmet\": 518,\n",
            "    \"crate\": 519,\n",
            "    \"crayfish, crawfish, crawdad, crawdaddy\": 124,\n",
            "    \"crib, cot\": 520,\n",
            "    \"cricket\": 312,\n",
            "    \"croquet ball\": 522,\n",
            "    \"crossword puzzle, crossword\": 918,\n",
            "    \"crutch\": 523,\n",
            "    \"cucumber, cuke\": 943,\n",
            "    \"cuirass\": 524,\n",
            "    \"cup\": 968,\n",
            "    \"curly-coated retriever\": 206,\n",
            "    \"custard apple\": 956,\n",
            "    \"daisy\": 985,\n",
            "    \"dalmatian, coach dog, carriage dog\": 251,\n",
            "    \"dam, dike, dyke\": 525,\n",
            "    \"damselfly\": 320,\n",
            "    \"desk\": 526,\n",
            "    \"desktop computer\": 527,\n",
            "    \"dhole, Cuon alpinus\": 274,\n",
            "    \"dial telephone, dial phone\": 528,\n",
            "    \"diamondback, diamondback rattlesnake, Crotalus adamanteus\": 67,\n",
            "    \"diaper, nappy, napkin\": 529,\n",
            "    \"digital clock\": 530,\n",
            "    \"digital watch\": 531,\n",
            "    \"dingo, warrigal, warragal, Canis dingo\": 273,\n",
            "    \"dining table, board\": 532,\n",
            "    \"dishrag, dishcloth\": 533,\n",
            "    \"dishwasher, dish washer, dishwashing machine\": 534,\n",
            "    \"disk brake, disc brake\": 535,\n",
            "    \"dock, dockage, docking facility\": 536,\n",
            "    \"dogsled, dog sled, dog sleigh\": 537,\n",
            "    \"dome\": 538,\n",
            "    \"doormat, welcome mat\": 539,\n",
            "    \"dough\": 961,\n",
            "    \"dowitcher\": 142,\n",
            "    \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\": 319,\n",
            "    \"drake\": 97,\n",
            "    \"drilling platform, offshore rig\": 540,\n",
            "    \"drum, membranophone, tympan\": 541,\n",
            "    \"drumstick\": 542,\n",
            "    \"dugong, Dugong dugon\": 149,\n",
            "    \"dumbbell\": 543,\n",
            "    \"dung beetle\": 305,\n",
            "    \"ear, spike, capitulum\": 998,\n",
            "    \"earthstar\": 995,\n",
            "    \"echidna, spiny anteater, anteater\": 102,\n",
            "    \"eel\": 390,\n",
            "    \"eft\": 27,\n",
            "    \"eggnog\": 969,\n",
            "    \"electric fan, blower\": 545,\n",
            "    \"electric guitar\": 546,\n",
            "    \"electric locomotive\": 547,\n",
            "    \"electric ray, crampfish, numbfish, torpedo\": 5,\n",
            "    \"entertainment center\": 548,\n",
            "    \"envelope\": 549,\n",
            "    \"espresso\": 967,\n",
            "    \"espresso maker\": 550,\n",
            "    \"face powder\": 551,\n",
            "    \"feather boa, boa\": 552,\n",
            "    \"fiddler crab\": 120,\n",
            "    \"fig\": 952,\n",
            "    \"file, file cabinet, filing cabinet\": 553,\n",
            "    \"fire engine, fire truck\": 555,\n",
            "    \"fire screen, fireguard\": 556,\n",
            "    \"fireboat\": 554,\n",
            "    \"flagpole, flagstaff\": 557,\n",
            "    \"flamingo\": 130,\n",
            "    \"flat-coated retriever\": 205,\n",
            "    \"flatworm, platyhelminth\": 110,\n",
            "    \"flute, transverse flute\": 558,\n",
            "    \"fly\": 308,\n",
            "    \"folding chair\": 559,\n",
            "    \"football helmet\": 560,\n",
            "    \"forklift\": 561,\n",
            "    \"fountain\": 562,\n",
            "    \"fountain pen\": 563,\n",
            "    \"four-poster\": 564,\n",
            "    \"fox squirrel, eastern fox squirrel, Sciurus niger\": 335,\n",
            "    \"freight car\": 565,\n",
            "    \"frilled lizard, Chlamydosaurus kingi\": 43,\n",
            "    \"frying pan, frypan, skillet\": 567,\n",
            "    \"fur coat\": 568,\n",
            "    \"gar, garfish, garpike, billfish, Lepisosteus osseus\": 395,\n",
            "    \"garbage truck, dustcart\": 569,\n",
            "    \"garden spider, Aranea diademata\": 74,\n",
            "    \"garter snake, grass snake\": 57,\n",
            "    \"gas pump, gasoline pump, petrol pump, island dispenser\": 571,\n",
            "    \"gasmask, respirator, gas helmet\": 570,\n",
            "    \"gazelle\": 353,\n",
            "    \"geyser\": 974,\n",
            "    \"giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca\": 388,\n",
            "    \"giant schnauzer\": 197,\n",
            "    \"gibbon, Hylobates lar\": 368,\n",
            "    \"go-kart\": 573,\n",
            "    \"goblet\": 572,\n",
            "    \"golden retriever\": 207,\n",
            "    \"goldfinch, Carduelis carduelis\": 11,\n",
            "    \"goldfish, Carassius auratus\": 1,\n",
            "    \"golf ball\": 574,\n",
            "    \"golfcart, golf cart\": 575,\n",
            "    \"gondola\": 576,\n",
            "    \"gong, tam-tam\": 577,\n",
            "    \"goose\": 99,\n",
            "    \"gorilla, Gorilla gorilla\": 366,\n",
            "    \"gown\": 578,\n",
            "    \"grand piano, grand\": 579,\n",
            "    \"grasshopper, hopper\": 311,\n",
            "    \"great grey owl, great gray owl, Strix nebulosa\": 24,\n",
            "    \"great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias\": 2,\n",
            "    \"green lizard, Lacerta viridis\": 46,\n",
            "    \"green mamba\": 64,\n",
            "    \"green snake, grass snake\": 55,\n",
            "    \"greenhouse, nursery, glasshouse\": 580,\n",
            "    \"grey fox, gray fox, Urocyon cinereoargenteus\": 280,\n",
            "    \"grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus\": 147,\n",
            "    \"grille, radiator grille\": 581,\n",
            "    \"grocery store, grocery, food market, market\": 582,\n",
            "    \"groenendael\": 224,\n",
            "    \"groom, bridegroom\": 982,\n",
            "    \"ground beetle, carabid beetle\": 302,\n",
            "    \"guacamole\": 924,\n",
            "    \"guenon, guenon monkey\": 370,\n",
            "    \"guillotine\": 583,\n",
            "    \"guinea pig, Cavia cobaya\": 338,\n",
            "    \"gyromitra\": 993,\n",
            "    \"hair slide\": 584,\n",
            "    \"hair spray\": 585,\n",
            "    \"half track\": 586,\n",
            "    \"hammer\": 587,\n",
            "    \"hammerhead, hammerhead shark\": 4,\n",
            "    \"hamper\": 588,\n",
            "    \"hamster\": 333,\n",
            "    \"hand blower, blow dryer, blow drier, hair dryer, hair drier\": 589,\n",
            "    \"hand-held computer, hand-held microcomputer\": 590,\n",
            "    \"handkerchief, hankie, hanky, hankey\": 591,\n",
            "    \"hard disc, hard disk, fixed disk\": 592,\n",
            "    \"hare\": 331,\n",
            "    \"harmonica, mouth organ, harp, mouth harp\": 593,\n",
            "    \"harp\": 594,\n",
            "    \"hartebeest\": 351,\n",
            "    \"harvester, reaper\": 595,\n",
            "    \"harvestman, daddy longlegs, Phalangium opilio\": 70,\n",
            "    \"hatchet\": 596,\n",
            "    \"hay\": 958,\n",
            "    \"head cabbage\": 936,\n",
            "    \"hen\": 8,\n",
            "    \"hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa\": 996,\n",
            "    \"hermit crab\": 125,\n",
            "    \"hip, rose hip, rosehip\": 989,\n",
            "    \"hippopotamus, hippo, river horse, Hippopotamus amphibius\": 344,\n",
            "    \"hog, pig, grunter, squealer, Sus scrofa\": 341,\n",
            "    \"hognose snake, puff adder, sand viper\": 54,\n",
            "    \"holster\": 597,\n",
            "    \"home theater, home theatre\": 598,\n",
            "    \"honeycomb\": 599,\n",
            "    \"hook, claw\": 600,\n",
            "    \"hoopskirt, crinoline\": 601,\n",
            "    \"horizontal bar, high bar\": 602,\n",
            "    \"hornbill\": 93,\n",
            "    \"horned viper, cerastes, sand viper, horned asp, Cerastes cornutus\": 66,\n",
            "    \"horse cart, horse-cart\": 603,\n",
            "    \"hot pot, hotpot\": 926,\n",
            "    \"hotdog, hot dog, red hot\": 934,\n",
            "    \"hourglass\": 604,\n",
            "    \"house finch, linnet, Carpodacus mexicanus\": 12,\n",
            "    \"howler monkey, howler\": 379,\n",
            "    \"hummingbird\": 94,\n",
            "    \"hyena, hyaena\": 276,\n",
            "    \"iPod\": 605,\n",
            "    \"ibex, Capra ibex\": 350,\n",
            "    \"ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus\": 296,\n",
            "    \"ice cream, icecream\": 928,\n",
            "    \"ice lolly, lolly, lollipop, popsicle\": 929,\n",
            "    \"impala, Aepyceros melampus\": 352,\n",
            "    \"indigo bunting, indigo finch, indigo bird, Passerina cyanea\": 14,\n",
            "    \"indri, indris, Indri indri, Indri brevicaudatus\": 384,\n",
            "    \"iron, smoothing iron\": 606,\n",
            "    \"isopod\": 126,\n",
            "    \"jacamar\": 95,\n",
            "    \"jack-o'-lantern\": 607,\n",
            "    \"jackfruit, jak, jack\": 955,\n",
            "    \"jaguar, panther, Panthera onca, Felis onca\": 290,\n",
            "    \"jay\": 17,\n",
            "    \"jean, blue jean, denim\": 608,\n",
            "    \"jeep, landrover\": 609,\n",
            "    \"jellyfish\": 107,\n",
            "    \"jersey, T-shirt, tee shirt\": 610,\n",
            "    \"jigsaw puzzle\": 611,\n",
            "    \"jinrikisha, ricksha, rickshaw\": 612,\n",
            "    \"joystick\": 613,\n",
            "    \"junco, snowbird\": 13,\n",
            "    \"keeshond\": 261,\n",
            "    \"kelpie\": 227,\n",
            "    \"killer whale, killer, orca, grampus, sea wolf, Orcinus orca\": 148,\n",
            "    \"kimono\": 614,\n",
            "    \"king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica\": 121,\n",
            "    \"king penguin, Aptenodytes patagonica\": 145,\n",
            "    \"king snake, kingsnake\": 56,\n",
            "    \"kit fox, Vulpes macrotis\": 278,\n",
            "    \"kite\": 21,\n",
            "    \"knee pad\": 615,\n",
            "    \"knot\": 616,\n",
            "    \"koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus\": 105,\n",
            "    \"komondor\": 228,\n",
            "    \"kuvasz\": 222,\n",
            "    \"lab coat, laboratory coat\": 617,\n",
            "    \"lacewing, lacewing fly\": 318,\n",
            "    \"ladle\": 618,\n",
            "    \"ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle\": 301,\n",
            "    \"lakeside, lakeshore\": 975,\n",
            "    \"lampshade, lamp shade\": 619,\n",
            "    \"langur\": 374,\n",
            "    \"laptop, laptop computer\": 620,\n",
            "    \"lawn mower, mower\": 621,\n",
            "    \"leaf beetle, chrysomelid\": 304,\n",
            "    \"leafhopper\": 317,\n",
            "    \"leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea\": 34,\n",
            "    \"lemon\": 951,\n",
            "    \"lens cap, lens cover\": 622,\n",
            "    \"leopard, Panthera pardus\": 288,\n",
            "    \"lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens\": 387,\n",
            "    \"letter opener, paper knife, paperknife\": 623,\n",
            "    \"library\": 624,\n",
            "    \"lifeboat\": 625,\n",
            "    \"lighter, light, igniter, ignitor\": 626,\n",
            "    \"limousine, limo\": 627,\n",
            "    \"limpkin, Aramus pictus\": 135,\n",
            "    \"liner, ocean liner\": 628,\n",
            "    \"lion, king of beasts, Panthera leo\": 291,\n",
            "    \"lionfish\": 396,\n",
            "    \"lipstick, lip rouge\": 629,\n",
            "    \"little blue heron, Egretta caerulea\": 131,\n",
            "    \"llama\": 355,\n",
            "    \"loggerhead, loggerhead turtle, Caretta caretta\": 33,\n",
            "    \"long-horned beetle, longicorn, longicorn beetle\": 303,\n",
            "    \"lorikeet\": 90,\n",
            "    \"lotion\": 631,\n",
            "    \"loudspeaker, speaker, speaker unit, loudspeaker system, speaker system\": 632,\n",
            "    \"loupe, jeweler's loupe\": 633,\n",
            "    \"lumbermill, sawmill\": 634,\n",
            "    \"lycaenid, lycaenid butterfly\": 326,\n",
            "    \"lynx, catamount\": 287,\n",
            "    \"macaque\": 373,\n",
            "    \"macaw\": 88,\n",
            "    \"magnetic compass\": 635,\n",
            "    \"magpie\": 18,\n",
            "    \"mailbag, postbag\": 636,\n",
            "    \"mailbox, letter box\": 637,\n",
            "    \"maillot\": 638,\n",
            "    \"maillot, tank suit\": 639,\n",
            "    \"malamute, malemute, Alaskan malamute\": 249,\n",
            "    \"malinois\": 225,\n",
            "    \"manhole cover\": 640,\n",
            "    \"mantis, mantid\": 315,\n",
            "    \"maraca\": 641,\n",
            "    \"marimba, xylophone\": 642,\n",
            "    \"marmoset\": 377,\n",
            "    \"marmot\": 336,\n",
            "    \"mashed potato\": 935,\n",
            "    \"mask\": 643,\n",
            "    \"matchstick\": 644,\n",
            "    \"maypole\": 645,\n",
            "    \"maze, labyrinth\": 646,\n",
            "    \"measuring cup\": 647,\n",
            "    \"meat loaf, meatloaf\": 962,\n",
            "    \"medicine chest, medicine cabinet\": 648,\n",
            "    \"meerkat, mierkat\": 299,\n",
            "    \"megalith, megalithic structure\": 649,\n",
            "    \"menu\": 922,\n",
            "    \"microphone, mike\": 650,\n",
            "    \"microwave, microwave oven\": 651,\n",
            "    \"military uniform\": 652,\n",
            "    \"milk can\": 653,\n",
            "    \"miniature pinscher\": 237,\n",
            "    \"miniature poodle\": 266,\n",
            "    \"miniature schnauzer\": 196,\n",
            "    \"minibus\": 654,\n",
            "    \"miniskirt, mini\": 655,\n",
            "    \"minivan\": 656,\n",
            "    \"mink\": 357,\n",
            "    \"missile\": 657,\n",
            "    \"mitten\": 658,\n",
            "    \"mixing bowl\": 659,\n",
            "    \"mobile home, manufactured home\": 660,\n",
            "    \"modem\": 662,\n",
            "    \"monarch, monarch butterfly, milkweed butterfly, Danaus plexippus\": 323,\n",
            "    \"monastery\": 663,\n",
            "    \"mongoose\": 298,\n",
            "    \"monitor\": 664,\n",
            "    \"moped\": 665,\n",
            "    \"mortar\": 666,\n",
            "    \"mortarboard\": 667,\n",
            "    \"mosque\": 668,\n",
            "    \"mosquito net\": 669,\n",
            "    \"motor scooter, scooter\": 670,\n",
            "    \"mountain bike, all-terrain bike, off-roader\": 671,\n",
            "    \"mountain tent\": 672,\n",
            "    \"mouse, computer mouse\": 673,\n",
            "    \"mousetrap\": 674,\n",
            "    \"moving van\": 675,\n",
            "    \"mud turtle\": 35,\n",
            "    \"mushroom\": 947,\n",
            "    \"muzzle\": 676,\n",
            "    \"nail\": 677,\n",
            "    \"neck brace\": 678,\n",
            "    \"necklace\": 679,\n",
            "    \"nematode, nematode worm, roundworm\": 111,\n",
            "    \"night snake, Hypsiglena torquata\": 60,\n",
            "    \"nipple\": 680,\n",
            "    \"notebook, notebook computer\": 681,\n",
            "    \"obelisk\": 682,\n",
            "    \"oboe, hautboy, hautbois\": 683,\n",
            "    \"ocarina, sweet potato\": 684,\n",
            "    \"odometer, hodometer, mileometer, milometer\": 685,\n",
            "    \"oil filter\": 686,\n",
            "    \"orange\": 950,\n",
            "    \"orangutan, orang, orangutang, Pongo pygmaeus\": 365,\n",
            "    \"organ, pipe organ\": 687,\n",
            "    \"oscilloscope, scope, cathode-ray oscilloscope, CRO\": 688,\n",
            "    \"ostrich, Struthio camelus\": 9,\n",
            "    \"otter\": 360,\n",
            "    \"otterhound, otter hound\": 175,\n",
            "    \"overskirt\": 689,\n",
            "    \"ox\": 345,\n",
            "    \"oxcart\": 690,\n",
            "    \"oxygen mask\": 691,\n",
            "    \"oystercatcher, oyster catcher\": 143,\n",
            "    \"packet\": 692,\n",
            "    \"paddle, boat paddle\": 693,\n",
            "    \"paddlewheel, paddle wheel\": 694,\n",
            "    \"padlock\": 695,\n",
            "    \"paintbrush\": 696,\n",
            "    \"pajama, pyjama, pj's, jammies\": 697,\n",
            "    \"palace\": 698,\n",
            "    \"panpipe, pandean pipe, syrinx\": 699,\n",
            "    \"paper towel\": 700,\n",
            "    \"papillon\": 157,\n",
            "    \"parachute, chute\": 701,\n",
            "    \"parallel bars, bars\": 702,\n",
            "    \"park bench\": 703,\n",
            "    \"parking meter\": 704,\n",
            "    \"partridge\": 86,\n",
            "    \"passenger car, coach, carriage\": 705,\n",
            "    \"patas, hussar monkey, Erythrocebus patas\": 371,\n",
            "    \"patio, terrace\": 706,\n",
            "    \"pay-phone, pay-station\": 707,\n",
            "    \"peacock\": 84,\n",
            "    \"pedestal, plinth, footstall\": 708,\n",
            "    \"pelican\": 144,\n",
            "    \"pencil box, pencil case\": 709,\n",
            "    \"pencil sharpener\": 710,\n",
            "    \"perfume, essence\": 711,\n",
            "    \"photocopier\": 713,\n",
            "    \"pick, plectrum, plectron\": 714,\n",
            "    \"pickelhaube\": 715,\n",
            "    \"picket fence, paling\": 716,\n",
            "    \"pickup, pickup truck\": 717,\n",
            "    \"pier\": 718,\n",
            "    \"piggy bank, penny bank\": 719,\n",
            "    \"pill bottle\": 720,\n",
            "    \"pillow\": 721,\n",
            "    \"pineapple, ananas\": 953,\n",
            "    \"ping-pong ball\": 722,\n",
            "    \"pinwheel\": 723,\n",
            "    \"pirate, pirate ship\": 724,\n",
            "    \"pitcher, ewer\": 725,\n",
            "    \"pizza, pizza pie\": 963,\n",
            "    \"plane, carpenter's plane, woodworking plane\": 726,\n",
            "    \"planetarium\": 727,\n",
            "    \"plastic bag\": 728,\n",
            "    \"plate\": 923,\n",
            "    \"plate rack\": 729,\n",
            "    \"platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus\": 103,\n",
            "    \"plow, plough\": 730,\n",
            "    \"plunger, plumber's helper\": 731,\n",
            "    \"pole\": 733,\n",
            "    \"polecat, fitch, foulmart, foumart, Mustela putorius\": 358,\n",
            "    \"police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria\": 734,\n",
            "    \"pomegranate\": 957,\n",
            "    \"poncho\": 735,\n",
            "    \"pool table, billiard table, snooker table\": 736,\n",
            "    \"pop bottle, soda bottle\": 737,\n",
            "    \"porcupine, hedgehog\": 334,\n",
            "    \"pot, flowerpot\": 738,\n",
            "    \"potpie\": 964,\n",
            "    \"potter's wheel\": 739,\n",
            "    \"power drill\": 740,\n",
            "    \"prairie chicken, prairie grouse, prairie fowl\": 83,\n",
            "    \"prayer rug, prayer mat\": 741,\n",
            "    \"pretzel\": 932,\n",
            "    \"printer\": 742,\n",
            "    \"prison, prison house\": 743,\n",
            "    \"proboscis monkey, Nasalis larvatus\": 376,\n",
            "    \"projectile, missile\": 744,\n",
            "    \"projector\": 745,\n",
            "    \"promontory, headland, head, foreland\": 976,\n",
            "    \"ptarmigan\": 81,\n",
            "    \"puck, hockey puck\": 746,\n",
            "    \"puffer, pufferfish, blowfish, globefish\": 397,\n",
            "    \"pug, pug-dog\": 254,\n",
            "    \"punching bag, punch bag, punching ball, punchball\": 747,\n",
            "    \"purse\": 748,\n",
            "    \"quail\": 85,\n",
            "    \"quill, quill pen\": 749,\n",
            "    \"quilt, comforter, comfort, puff\": 750,\n",
            "    \"racer, race car, racing car\": 751,\n",
            "    \"racket, racquet\": 752,\n",
            "    \"radiator\": 753,\n",
            "    \"radio telescope, radio reflector\": 755,\n",
            "    \"radio, wireless\": 754,\n",
            "    \"rain barrel\": 756,\n",
            "    \"ram, tup\": 348,\n",
            "    \"rapeseed\": 984,\n",
            "    \"recreational vehicle, RV, R.V.\": 757,\n",
            "    \"red fox, Vulpes vulpes\": 277,\n",
            "    \"red wine\": 966,\n",
            "    \"red wolf, maned wolf, Canis rufus, Canis niger\": 271,\n",
            "    \"red-backed sandpiper, dunlin, Erolia alpina\": 140,\n",
            "    \"red-breasted merganser, Mergus serrator\": 98,\n",
            "    \"redbone\": 168,\n",
            "    \"redshank, Tringa totanus\": 141,\n",
            "    \"reel\": 758,\n",
            "    \"reflex camera\": 759,\n",
            "    \"refrigerator, icebox\": 760,\n",
            "    \"remote control, remote\": 761,\n",
            "    \"restaurant, eating house, eating place, eatery\": 762,\n",
            "    \"revolver, six-gun, six-shooter\": 763,\n",
            "    \"rhinoceros beetle\": 306,\n",
            "    \"rifle\": 764,\n",
            "    \"ringlet, ringlet butterfly\": 322,\n",
            "    \"ringneck snake, ring-necked snake, ring snake\": 53,\n",
            "    \"robin, American robin, Turdus migratorius\": 15,\n",
            "    \"rock beauty, Holocanthus tricolor\": 392,\n",
            "    \"rock crab, Cancer irroratus\": 119,\n",
            "    \"rock python, rock snake, Python sebae\": 62,\n",
            "    \"rocking chair, rocker\": 765,\n",
            "    \"rotisserie\": 766,\n",
            "    \"rubber eraser, rubber, pencil eraser\": 767,\n",
            "    \"ruddy turnstone, Arenaria interpres\": 139,\n",
            "    \"ruffed grouse, partridge, Bonasa umbellus\": 82,\n",
            "    \"rugby ball\": 768,\n",
            "    \"rule, ruler\": 769,\n",
            "    \"running shoe\": 770,\n",
            "    \"safe\": 771,\n",
            "    \"safety pin\": 772,\n",
            "    \"saltshaker, salt shaker\": 773,\n",
            "    \"sandal\": 774,\n",
            "    \"sandbar, sand bar\": 977,\n",
            "    \"sarong\": 775,\n",
            "    \"sax, saxophone\": 776,\n",
            "    \"scabbard\": 777,\n",
            "    \"scale, weighing machine\": 778,\n",
            "    \"schipperke\": 223,\n",
            "    \"school bus\": 779,\n",
            "    \"schooner\": 780,\n",
            "    \"scoreboard\": 781,\n",
            "    \"scorpion\": 71,\n",
            "    \"screen, CRT screen\": 782,\n",
            "    \"screw\": 783,\n",
            "    \"screwdriver\": 784,\n",
            "    \"scuba diver\": 983,\n",
            "    \"sea anemone, anemone\": 108,\n",
            "    \"sea cucumber, holothurian\": 329,\n",
            "    \"sea lion\": 150,\n",
            "    \"sea slug, nudibranch\": 115,\n",
            "    \"sea snake\": 65,\n",
            "    \"sea urchin\": 328,\n",
            "    \"seashore, coast, seacoast, sea-coast\": 978,\n",
            "    \"seat belt, seatbelt\": 785,\n",
            "    \"sewing machine\": 786,\n",
            "    \"shield, buckler\": 787,\n",
            "    \"shoe shop, shoe-shop, shoe store\": 788,\n",
            "    \"shoji\": 789,\n",
            "    \"shopping basket\": 790,\n",
            "    \"shopping cart\": 791,\n",
            "    \"shovel\": 792,\n",
            "    \"shower cap\": 793,\n",
            "    \"shower curtain\": 794,\n",
            "    \"siamang, Hylobates syndactylus, Symphalangus syndactylus\": 369,\n",
            "    \"sidewinder, horned rattlesnake, Crotalus cerastes\": 68,\n",
            "    \"silky terrier, Sydney silky\": 201,\n",
            "    \"ski\": 795,\n",
            "    \"ski mask\": 796,\n",
            "    \"skunk, polecat, wood pussy\": 361,\n",
            "    \"sleeping bag\": 797,\n",
            "    \"slide rule, slipstick\": 798,\n",
            "    \"sliding door\": 799,\n",
            "    \"slot, one-armed bandit\": 800,\n",
            "    \"sloth bear, Melursus ursinus, Ursus ursinus\": 297,\n",
            "    \"slug\": 114,\n",
            "    \"snail\": 113,\n",
            "    \"snorkel\": 801,\n",
            "    \"snow leopard, ounce, Panthera uncia\": 289,\n",
            "    \"snowmobile\": 802,\n",
            "    \"snowplow, snowplough\": 803,\n",
            "    \"soap dispenser\": 804,\n",
            "    \"soccer ball\": 805,\n",
            "    \"sock\": 806,\n",
            "    \"soft-coated wheaten terrier\": 202,\n",
            "    \"solar dish, solar collector, solar furnace\": 807,\n",
            "    \"sombrero\": 808,\n",
            "    \"sorrel\": 339,\n",
            "    \"soup bowl\": 809,\n",
            "    \"space bar\": 810,\n",
            "    \"space heater\": 811,\n",
            "    \"space shuttle\": 812,\n",
            "    \"spaghetti squash\": 940,\n",
            "    \"spatula\": 813,\n",
            "    \"speedboat\": 814,\n",
            "    \"spider monkey, Ateles geoffroyi\": 381,\n",
            "    \"spider web, spider's web\": 815,\n",
            "    \"spindle\": 816,\n",
            "    \"spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish\": 123,\n",
            "    \"spoonbill\": 129,\n",
            "    \"sports car, sport car\": 817,\n",
            "    \"spotlight, spot\": 818,\n",
            "    \"spotted salamander, Ambystoma maculatum\": 28,\n",
            "    \"squirrel monkey, Saimiri sciureus\": 382,\n",
            "    \"stage\": 819,\n",
            "    \"standard poodle\": 267,\n",
            "    \"standard schnauzer\": 198,\n",
            "    \"starfish, sea star\": 327,\n",
            "    \"steam locomotive\": 820,\n",
            "    \"steel arch bridge\": 821,\n",
            "    \"steel drum\": 822,\n",
            "    \"stethoscope\": 823,\n",
            "    \"stingray\": 6,\n",
            "    \"stinkhorn, carrion fungus\": 994,\n",
            "    \"stole\": 824,\n",
            "    \"stone wall\": 825,\n",
            "    \"stopwatch, stop watch\": 826,\n",
            "    \"stove\": 827,\n",
            "    \"strainer\": 828,\n",
            "    \"strawberry\": 949,\n",
            "    \"street sign\": 919,\n",
            "    \"streetcar, tram, tramcar, trolley, trolley car\": 829,\n",
            "    \"stretcher\": 830,\n",
            "    \"studio couch, day bed\": 831,\n",
            "    \"stupa, tope\": 832,\n",
            "    \"sturgeon\": 394,\n",
            "    \"submarine, pigboat, sub, U-boat\": 833,\n",
            "    \"suit, suit of clothes\": 834,\n",
            "    \"sulphur butterfly, sulfur butterfly\": 325,\n",
            "    \"sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita\": 89,\n",
            "    \"sundial\": 835,\n",
            "    \"sunglass\": 836,\n",
            "    \"sunglasses, dark glasses, shades\": 837,\n",
            "    \"sunscreen, sunblock, sun blocker\": 838,\n",
            "    \"suspension bridge\": 839,\n",
            "    \"swab, swob, mop\": 840,\n",
            "    \"sweatshirt\": 841,\n",
            "    \"swimming trunks, bathing trunks\": 842,\n",
            "    \"swing\": 843,\n",
            "    \"switch, electric switch, electrical switch\": 844,\n",
            "    \"syringe\": 845,\n",
            "    \"tabby, tabby cat\": 281,\n",
            "    \"table lamp\": 846,\n",
            "    \"tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui\": 32,\n",
            "    \"tank, army tank, armored combat vehicle, armoured combat vehicle\": 847,\n",
            "    \"tape player\": 848,\n",
            "    \"tarantula\": 76,\n",
            "    \"teapot\": 849,\n",
            "    \"teddy, teddy bear\": 850,\n",
            "    \"television, television system\": 851,\n",
            "    \"tench, Tinca tinca\": 0,\n",
            "    \"tennis ball\": 852,\n",
            "    \"terrapin\": 36,\n",
            "    \"thatch, thatched roof\": 853,\n",
            "    \"theater curtain, theatre curtain\": 854,\n",
            "    \"thimble\": 855,\n",
            "    \"three-toed sloth, ai, Bradypus tridactylus\": 364,\n",
            "    \"thresher, thrasher, threshing machine\": 856,\n",
            "    \"throne\": 857,\n",
            "    \"thunder snake, worm snake, Carphophis amoenus\": 52,\n",
            "    \"tick\": 78,\n",
            "    \"tiger beetle\": 300,\n",
            "    \"tiger cat\": 282,\n",
            "    \"tiger shark, Galeocerdo cuvieri\": 3,\n",
            "    \"tiger, Panthera tigris\": 292,\n",
            "    \"tile roof\": 858,\n",
            "    \"timber wolf, grey wolf, gray wolf, Canis lupus\": 269,\n",
            "    \"titi, titi monkey\": 380,\n",
            "    \"toaster\": 859,\n",
            "    \"tobacco shop, tobacconist shop, tobacconist\": 860,\n",
            "    \"toilet seat\": 861,\n",
            "    \"toilet tissue, toilet paper, bathroom tissue\": 999,\n",
            "    \"torch\": 862,\n",
            "    \"totem pole\": 863,\n",
            "    \"toucan\": 96,\n",
            "    \"tow truck, tow car, wrecker\": 864,\n",
            "    \"toy poodle\": 265,\n",
            "    \"toy terrier\": 158,\n",
            "    \"toyshop\": 865,\n",
            "    \"tractor\": 866,\n",
            "    \"traffic light, traffic signal, stoplight\": 920,\n",
            "    \"trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi\": 867,\n",
            "    \"tray\": 868,\n",
            "    \"tree frog, tree-frog\": 31,\n",
            "    \"trench coat\": 869,\n",
            "    \"triceratops\": 51,\n",
            "    \"tricycle, trike, velocipede\": 870,\n",
            "    \"trifle\": 927,\n",
            "    \"trilobite\": 69,\n",
            "    \"trimaran\": 871,\n",
            "    \"tripod\": 872,\n",
            "    \"triumphal arch\": 873,\n",
            "    \"trolleybus, trolley coach, trackless trolley\": 874,\n",
            "    \"trombone\": 875,\n",
            "    \"tub, vat\": 876,\n",
            "    \"turnstile\": 877,\n",
            "    \"tusker\": 101,\n",
            "    \"typewriter keyboard\": 878,\n",
            "    \"umbrella\": 879,\n",
            "    \"unicycle, monocycle\": 880,\n",
            "    \"upright, upright piano\": 881,\n",
            "    \"vacuum, vacuum cleaner\": 882,\n",
            "    \"valley, vale\": 979,\n",
            "    \"vase\": 883,\n",
            "    \"vault\": 884,\n",
            "    \"velvet\": 885,\n",
            "    \"vending machine\": 886,\n",
            "    \"vestment\": 887,\n",
            "    \"viaduct\": 888,\n",
            "    \"vine snake\": 59,\n",
            "    \"violin, fiddle\": 889,\n",
            "    \"vizsla, Hungarian pointer\": 211,\n",
            "    \"volcano\": 980,\n",
            "    \"volleyball\": 890,\n",
            "    \"vulture\": 23,\n",
            "    \"waffle iron\": 891,\n",
            "    \"walking stick, walkingstick, stick insect\": 313,\n",
            "    \"wall clock\": 892,\n",
            "    \"wallaby, brush kangaroo\": 104,\n",
            "    \"wallet, billfold, notecase, pocketbook\": 893,\n",
            "    \"wardrobe, closet, press\": 894,\n",
            "    \"warplane, military plane\": 895,\n",
            "    \"warthog\": 343,\n",
            "    \"washbasin, handbasin, washbowl, lavabo, wash-hand basin\": 896,\n",
            "    \"washer, automatic washer, washing machine\": 897,\n",
            "    \"water bottle\": 898,\n",
            "    \"water buffalo, water ox, Asiatic buffalo, Bubalus bubalis\": 346,\n",
            "    \"water jug\": 899,\n",
            "    \"water ouzel, dipper\": 20,\n",
            "    \"water snake\": 58,\n",
            "    \"water tower\": 900,\n",
            "    \"weasel\": 356,\n",
            "    \"web site, website, internet site, site\": 916,\n",
            "    \"weevil\": 307,\n",
            "    \"whippet\": 172,\n",
            "    \"whiptail, whiptail lizard\": 41,\n",
            "    \"whiskey jug\": 901,\n",
            "    \"whistle\": 902,\n",
            "    \"white stork, Ciconia ciconia\": 127,\n",
            "    \"white wolf, Arctic wolf, Canis lupus tundrarum\": 270,\n",
            "    \"wig\": 903,\n",
            "    \"wild boar, boar, Sus scrofa\": 342,\n",
            "    \"window screen\": 904,\n",
            "    \"window shade\": 905,\n",
            "    \"wine bottle\": 907,\n",
            "    \"wing\": 908,\n",
            "    \"wire-haired fox terrier\": 188,\n",
            "    \"wok\": 909,\n",
            "    \"wolf spider, hunting spider\": 77,\n",
            "    \"wombat\": 106,\n",
            "    \"wood rabbit, cottontail, cottontail rabbit\": 330,\n",
            "    \"wooden spoon\": 910,\n",
            "    \"wool, woolen, woollen\": 911,\n",
            "    \"worm fence, snake fence, snake-rail fence, Virginia fence\": 912,\n",
            "    \"wreck\": 913,\n",
            "    \"yawl\": 914,\n",
            "    \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\": 986,\n",
            "    \"yurt\": 915,\n",
            "    \"zebra\": 340,\n",
            "    \"zucchini, courgette\": 939\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"model_type\": \"vit\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_channels\": 3,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"patch_size\": 16,\n",
            "  \"pooler_act\": \"tanh\",\n",
            "  \"pooler_output_size\": 768,\n",
            "  \"qkv_bias\": true,\n",
            "  \"transformers_version\": \"4.55.0\"\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1308] 2025-08-06 00:55:43,058 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--vit-base-patch16-224/snapshots/3f49326eb077187dfe1c2a2bb15fbd74e6ab91e3/model.safetensors\n",
            "[INFO|modeling_utils.py:5596] 2025-08-06 00:55:43,143 >> Some weights of the model checkpoint at google/vit-base-patch16-224 were not used when initializing ViTModel: ['classifier.bias', 'classifier.weight']\n",
            "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:5608] 2025-08-06 00:55:43,144 >> Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2469e29021454e4fad31abddfabdef8c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO|image_processing_base.py:378] 2025-08-06 00:55:43,278 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--google--vit-base-patch16-224/snapshots/3f49326eb077187dfe1c2a2bb15fbd74e6ab91e3/preprocessor_config.json\n",
            "[INFO|configuration_utils.py:752] 2025-08-06 00:55:43,387 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--vit-base-patch16-224/snapshots/3f49326eb077187dfe1c2a2bb15fbd74e6ab91e3/config.json\n",
            "[INFO|configuration_utils.py:817] 2025-08-06 00:55:43,392 >> Model config ViTConfig {\n",
            "  \"architectures\": [\n",
            "    \"ViTForImageClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"encoder_stride\": 16,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"tench, Tinca tinca\",\n",
            "    \"1\": \"goldfish, Carassius auratus\",\n",
            "    \"2\": \"great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias\",\n",
            "    \"3\": \"tiger shark, Galeocerdo cuvieri\",\n",
            "    \"4\": \"hammerhead, hammerhead shark\",\n",
            "    \"5\": \"electric ray, crampfish, numbfish, torpedo\",\n",
            "    \"6\": \"stingray\",\n",
            "    \"7\": \"cock\",\n",
            "    \"8\": \"hen\",\n",
            "    \"9\": \"ostrich, Struthio camelus\",\n",
            "    \"10\": \"brambling, Fringilla montifringilla\",\n",
            "    \"11\": \"goldfinch, Carduelis carduelis\",\n",
            "    \"12\": \"house finch, linnet, Carpodacus mexicanus\",\n",
            "    \"13\": \"junco, snowbird\",\n",
            "    \"14\": \"indigo bunting, indigo finch, indigo bird, Passerina cyanea\",\n",
            "    \"15\": \"robin, American robin, Turdus migratorius\",\n",
            "    \"16\": \"bulbul\",\n",
            "    \"17\": \"jay\",\n",
            "    \"18\": \"magpie\",\n",
            "    \"19\": \"chickadee\",\n",
            "    \"20\": \"water ouzel, dipper\",\n",
            "    \"21\": \"kite\",\n",
            "    \"22\": \"bald eagle, American eagle, Haliaeetus leucocephalus\",\n",
            "    \"23\": \"vulture\",\n",
            "    \"24\": \"great grey owl, great gray owl, Strix nebulosa\",\n",
            "    \"25\": \"European fire salamander, Salamandra salamandra\",\n",
            "    \"26\": \"common newt, Triturus vulgaris\",\n",
            "    \"27\": \"eft\",\n",
            "    \"28\": \"spotted salamander, Ambystoma maculatum\",\n",
            "    \"29\": \"axolotl, mud puppy, Ambystoma mexicanum\",\n",
            "    \"30\": \"bullfrog, Rana catesbeiana\",\n",
            "    \"31\": \"tree frog, tree-frog\",\n",
            "    \"32\": \"tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui\",\n",
            "    \"33\": \"loggerhead, loggerhead turtle, Caretta caretta\",\n",
            "    \"34\": \"leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea\",\n",
            "    \"35\": \"mud turtle\",\n",
            "    \"36\": \"terrapin\",\n",
            "    \"37\": \"box turtle, box tortoise\",\n",
            "    \"38\": \"banded gecko\",\n",
            "    \"39\": \"common iguana, iguana, Iguana iguana\",\n",
            "    \"40\": \"American chameleon, anole, Anolis carolinensis\",\n",
            "    \"41\": \"whiptail, whiptail lizard\",\n",
            "    \"42\": \"agama\",\n",
            "    \"43\": \"frilled lizard, Chlamydosaurus kingi\",\n",
            "    \"44\": \"alligator lizard\",\n",
            "    \"45\": \"Gila monster, Heloderma suspectum\",\n",
            "    \"46\": \"green lizard, Lacerta viridis\",\n",
            "    \"47\": \"African chameleon, Chamaeleo chamaeleon\",\n",
            "    \"48\": \"Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis\",\n",
            "    \"49\": \"African crocodile, Nile crocodile, Crocodylus niloticus\",\n",
            "    \"50\": \"American alligator, Alligator mississipiensis\",\n",
            "    \"51\": \"triceratops\",\n",
            "    \"52\": \"thunder snake, worm snake, Carphophis amoenus\",\n",
            "    \"53\": \"ringneck snake, ring-necked snake, ring snake\",\n",
            "    \"54\": \"hognose snake, puff adder, sand viper\",\n",
            "    \"55\": \"green snake, grass snake\",\n",
            "    \"56\": \"king snake, kingsnake\",\n",
            "    \"57\": \"garter snake, grass snake\",\n",
            "    \"58\": \"water snake\",\n",
            "    \"59\": \"vine snake\",\n",
            "    \"60\": \"night snake, Hypsiglena torquata\",\n",
            "    \"61\": \"boa constrictor, Constrictor constrictor\",\n",
            "    \"62\": \"rock python, rock snake, Python sebae\",\n",
            "    \"63\": \"Indian cobra, Naja naja\",\n",
            "    \"64\": \"green mamba\",\n",
            "    \"65\": \"sea snake\",\n",
            "    \"66\": \"horned viper, cerastes, sand viper, horned asp, Cerastes cornutus\",\n",
            "    \"67\": \"diamondback, diamondback rattlesnake, Crotalus adamanteus\",\n",
            "    \"68\": \"sidewinder, horned rattlesnake, Crotalus cerastes\",\n",
            "    \"69\": \"trilobite\",\n",
            "    \"70\": \"harvestman, daddy longlegs, Phalangium opilio\",\n",
            "    \"71\": \"scorpion\",\n",
            "    \"72\": \"black and gold garden spider, Argiope aurantia\",\n",
            "    \"73\": \"barn spider, Araneus cavaticus\",\n",
            "    \"74\": \"garden spider, Aranea diademata\",\n",
            "    \"75\": \"black widow, Latrodectus mactans\",\n",
            "    \"76\": \"tarantula\",\n",
            "    \"77\": \"wolf spider, hunting spider\",\n",
            "    \"78\": \"tick\",\n",
            "    \"79\": \"centipede\",\n",
            "    \"80\": \"black grouse\",\n",
            "    \"81\": \"ptarmigan\",\n",
            "    \"82\": \"ruffed grouse, partridge, Bonasa umbellus\",\n",
            "    \"83\": \"prairie chicken, prairie grouse, prairie fowl\",\n",
            "    \"84\": \"peacock\",\n",
            "    \"85\": \"quail\",\n",
            "    \"86\": \"partridge\",\n",
            "    \"87\": \"African grey, African gray, Psittacus erithacus\",\n",
            "    \"88\": \"macaw\",\n",
            "    \"89\": \"sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita\",\n",
            "    \"90\": \"lorikeet\",\n",
            "    \"91\": \"coucal\",\n",
            "    \"92\": \"bee eater\",\n",
            "    \"93\": \"hornbill\",\n",
            "    \"94\": \"hummingbird\",\n",
            "    \"95\": \"jacamar\",\n",
            "    \"96\": \"toucan\",\n",
            "    \"97\": \"drake\",\n",
            "    \"98\": \"red-breasted merganser, Mergus serrator\",\n",
            "    \"99\": \"goose\",\n",
            "    \"100\": \"black swan, Cygnus atratus\",\n",
            "    \"101\": \"tusker\",\n",
            "    \"102\": \"echidna, spiny anteater, anteater\",\n",
            "    \"103\": \"platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus\",\n",
            "    \"104\": \"wallaby, brush kangaroo\",\n",
            "    \"105\": \"koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus\",\n",
            "    \"106\": \"wombat\",\n",
            "    \"107\": \"jellyfish\",\n",
            "    \"108\": \"sea anemone, anemone\",\n",
            "    \"109\": \"brain coral\",\n",
            "    \"110\": \"flatworm, platyhelminth\",\n",
            "    \"111\": \"nematode, nematode worm, roundworm\",\n",
            "    \"112\": \"conch\",\n",
            "    \"113\": \"snail\",\n",
            "    \"114\": \"slug\",\n",
            "    \"115\": \"sea slug, nudibranch\",\n",
            "    \"116\": \"chiton, coat-of-mail shell, sea cradle, polyplacophore\",\n",
            "    \"117\": \"chambered nautilus, pearly nautilus, nautilus\",\n",
            "    \"118\": \"Dungeness crab, Cancer magister\",\n",
            "    \"119\": \"rock crab, Cancer irroratus\",\n",
            "    \"120\": \"fiddler crab\",\n",
            "    \"121\": \"king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica\",\n",
            "    \"122\": \"American lobster, Northern lobster, Maine lobster, Homarus americanus\",\n",
            "    \"123\": \"spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish\",\n",
            "    \"124\": \"crayfish, crawfish, crawdad, crawdaddy\",\n",
            "    \"125\": \"hermit crab\",\n",
            "    \"126\": \"isopod\",\n",
            "    \"127\": \"white stork, Ciconia ciconia\",\n",
            "    \"128\": \"black stork, Ciconia nigra\",\n",
            "    \"129\": \"spoonbill\",\n",
            "    \"130\": \"flamingo\",\n",
            "    \"131\": \"little blue heron, Egretta caerulea\",\n",
            "    \"132\": \"American egret, great white heron, Egretta albus\",\n",
            "    \"133\": \"bittern\",\n",
            "    \"134\": \"crane\",\n",
            "    \"135\": \"limpkin, Aramus pictus\",\n",
            "    \"136\": \"European gallinule, Porphyrio porphyrio\",\n",
            "    \"137\": \"American coot, marsh hen, mud hen, water hen, Fulica americana\",\n",
            "    \"138\": \"bustard\",\n",
            "    \"139\": \"ruddy turnstone, Arenaria interpres\",\n",
            "    \"140\": \"red-backed sandpiper, dunlin, Erolia alpina\",\n",
            "    \"141\": \"redshank, Tringa totanus\",\n",
            "    \"142\": \"dowitcher\",\n",
            "    \"143\": \"oystercatcher, oyster catcher\",\n",
            "    \"144\": \"pelican\",\n",
            "    \"145\": \"king penguin, Aptenodytes patagonica\",\n",
            "    \"146\": \"albatross, mollymawk\",\n",
            "    \"147\": \"grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus\",\n",
            "    \"148\": \"killer whale, killer, orca, grampus, sea wolf, Orcinus orca\",\n",
            "    \"149\": \"dugong, Dugong dugon\",\n",
            "    \"150\": \"sea lion\",\n",
            "    \"151\": \"Chihuahua\",\n",
            "    \"152\": \"Japanese spaniel\",\n",
            "    \"153\": \"Maltese dog, Maltese terrier, Maltese\",\n",
            "    \"154\": \"Pekinese, Pekingese, Peke\",\n",
            "    \"155\": \"Shih-Tzu\",\n",
            "    \"156\": \"Blenheim spaniel\",\n",
            "    \"157\": \"papillon\",\n",
            "    \"158\": \"toy terrier\",\n",
            "    \"159\": \"Rhodesian ridgeback\",\n",
            "    \"160\": \"Afghan hound, Afghan\",\n",
            "    \"161\": \"basset, basset hound\",\n",
            "    \"162\": \"beagle\",\n",
            "    \"163\": \"bloodhound, sleuthhound\",\n",
            "    \"164\": \"bluetick\",\n",
            "    \"165\": \"black-and-tan coonhound\",\n",
            "    \"166\": \"Walker hound, Walker foxhound\",\n",
            "    \"167\": \"English foxhound\",\n",
            "    \"168\": \"redbone\",\n",
            "    \"169\": \"borzoi, Russian wolfhound\",\n",
            "    \"170\": \"Irish wolfhound\",\n",
            "    \"171\": \"Italian greyhound\",\n",
            "    \"172\": \"whippet\",\n",
            "    \"173\": \"Ibizan hound, Ibizan Podenco\",\n",
            "    \"174\": \"Norwegian elkhound, elkhound\",\n",
            "    \"175\": \"otterhound, otter hound\",\n",
            "    \"176\": \"Saluki, gazelle hound\",\n",
            "    \"177\": \"Scottish deerhound, deerhound\",\n",
            "    \"178\": \"Weimaraner\",\n",
            "    \"179\": \"Staffordshire bullterrier, Staffordshire bull terrier\",\n",
            "    \"180\": \"American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier\",\n",
            "    \"181\": \"Bedlington terrier\",\n",
            "    \"182\": \"Border terrier\",\n",
            "    \"183\": \"Kerry blue terrier\",\n",
            "    \"184\": \"Irish terrier\",\n",
            "    \"185\": \"Norfolk terrier\",\n",
            "    \"186\": \"Norwich terrier\",\n",
            "    \"187\": \"Yorkshire terrier\",\n",
            "    \"188\": \"wire-haired fox terrier\",\n",
            "    \"189\": \"Lakeland terrier\",\n",
            "    \"190\": \"Sealyham terrier, Sealyham\",\n",
            "    \"191\": \"Airedale, Airedale terrier\",\n",
            "    \"192\": \"cairn, cairn terrier\",\n",
            "    \"193\": \"Australian terrier\",\n",
            "    \"194\": \"Dandie Dinmont, Dandie Dinmont terrier\",\n",
            "    \"195\": \"Boston bull, Boston terrier\",\n",
            "    \"196\": \"miniature schnauzer\",\n",
            "    \"197\": \"giant schnauzer\",\n",
            "    \"198\": \"standard schnauzer\",\n",
            "    \"199\": \"Scotch terrier, Scottish terrier, Scottie\",\n",
            "    \"200\": \"Tibetan terrier, chrysanthemum dog\",\n",
            "    \"201\": \"silky terrier, Sydney silky\",\n",
            "    \"202\": \"soft-coated wheaten terrier\",\n",
            "    \"203\": \"West Highland white terrier\",\n",
            "    \"204\": \"Lhasa, Lhasa apso\",\n",
            "    \"205\": \"flat-coated retriever\",\n",
            "    \"206\": \"curly-coated retriever\",\n",
            "    \"207\": \"golden retriever\",\n",
            "    \"208\": \"Labrador retriever\",\n",
            "    \"209\": \"Chesapeake Bay retriever\",\n",
            "    \"210\": \"German short-haired pointer\",\n",
            "    \"211\": \"vizsla, Hungarian pointer\",\n",
            "    \"212\": \"English setter\",\n",
            "    \"213\": \"Irish setter, red setter\",\n",
            "    \"214\": \"Gordon setter\",\n",
            "    \"215\": \"Brittany spaniel\",\n",
            "    \"216\": \"clumber, clumber spaniel\",\n",
            "    \"217\": \"English springer, English springer spaniel\",\n",
            "    \"218\": \"Welsh springer spaniel\",\n",
            "    \"219\": \"cocker spaniel, English cocker spaniel, cocker\",\n",
            "    \"220\": \"Sussex spaniel\",\n",
            "    \"221\": \"Irish water spaniel\",\n",
            "    \"222\": \"kuvasz\",\n",
            "    \"223\": \"schipperke\",\n",
            "    \"224\": \"groenendael\",\n",
            "    \"225\": \"malinois\",\n",
            "    \"226\": \"briard\",\n",
            "    \"227\": \"kelpie\",\n",
            "    \"228\": \"komondor\",\n",
            "    \"229\": \"Old English sheepdog, bobtail\",\n",
            "    \"230\": \"Shetland sheepdog, Shetland sheep dog, Shetland\",\n",
            "    \"231\": \"collie\",\n",
            "    \"232\": \"Border collie\",\n",
            "    \"233\": \"Bouvier des Flandres, Bouviers des Flandres\",\n",
            "    \"234\": \"Rottweiler\",\n",
            "    \"235\": \"German shepherd, German shepherd dog, German police dog, alsatian\",\n",
            "    \"236\": \"Doberman, Doberman pinscher\",\n",
            "    \"237\": \"miniature pinscher\",\n",
            "    \"238\": \"Greater Swiss Mountain dog\",\n",
            "    \"239\": \"Bernese mountain dog\",\n",
            "    \"240\": \"Appenzeller\",\n",
            "    \"241\": \"EntleBucher\",\n",
            "    \"242\": \"boxer\",\n",
            "    \"243\": \"bull mastiff\",\n",
            "    \"244\": \"Tibetan mastiff\",\n",
            "    \"245\": \"French bulldog\",\n",
            "    \"246\": \"Great Dane\",\n",
            "    \"247\": \"Saint Bernard, St Bernard\",\n",
            "    \"248\": \"Eskimo dog, husky\",\n",
            "    \"249\": \"malamute, malemute, Alaskan malamute\",\n",
            "    \"250\": \"Siberian husky\",\n",
            "    \"251\": \"dalmatian, coach dog, carriage dog\",\n",
            "    \"252\": \"affenpinscher, monkey pinscher, monkey dog\",\n",
            "    \"253\": \"basenji\",\n",
            "    \"254\": \"pug, pug-dog\",\n",
            "    \"255\": \"Leonberg\",\n",
            "    \"256\": \"Newfoundland, Newfoundland dog\",\n",
            "    \"257\": \"Great Pyrenees\",\n",
            "    \"258\": \"Samoyed, Samoyede\",\n",
            "    \"259\": \"Pomeranian\",\n",
            "    \"260\": \"chow, chow chow\",\n",
            "    \"261\": \"keeshond\",\n",
            "    \"262\": \"Brabancon griffon\",\n",
            "    \"263\": \"Pembroke, Pembroke Welsh corgi\",\n",
            "    \"264\": \"Cardigan, Cardigan Welsh corgi\",\n",
            "    \"265\": \"toy poodle\",\n",
            "    \"266\": \"miniature poodle\",\n",
            "    \"267\": \"standard poodle\",\n",
            "    \"268\": \"Mexican hairless\",\n",
            "    \"269\": \"timber wolf, grey wolf, gray wolf, Canis lupus\",\n",
            "    \"270\": \"white wolf, Arctic wolf, Canis lupus tundrarum\",\n",
            "    \"271\": \"red wolf, maned wolf, Canis rufus, Canis niger\",\n",
            "    \"272\": \"coyote, prairie wolf, brush wolf, Canis latrans\",\n",
            "    \"273\": \"dingo, warrigal, warragal, Canis dingo\",\n",
            "    \"274\": \"dhole, Cuon alpinus\",\n",
            "    \"275\": \"African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus\",\n",
            "    \"276\": \"hyena, hyaena\",\n",
            "    \"277\": \"red fox, Vulpes vulpes\",\n",
            "    \"278\": \"kit fox, Vulpes macrotis\",\n",
            "    \"279\": \"Arctic fox, white fox, Alopex lagopus\",\n",
            "    \"280\": \"grey fox, gray fox, Urocyon cinereoargenteus\",\n",
            "    \"281\": \"tabby, tabby cat\",\n",
            "    \"282\": \"tiger cat\",\n",
            "    \"283\": \"Persian cat\",\n",
            "    \"284\": \"Siamese cat, Siamese\",\n",
            "    \"285\": \"Egyptian cat\",\n",
            "    \"286\": \"cougar, puma, catamount, mountain lion, painter, panther, Felis concolor\",\n",
            "    \"287\": \"lynx, catamount\",\n",
            "    \"288\": \"leopard, Panthera pardus\",\n",
            "    \"289\": \"snow leopard, ounce, Panthera uncia\",\n",
            "    \"290\": \"jaguar, panther, Panthera onca, Felis onca\",\n",
            "    \"291\": \"lion, king of beasts, Panthera leo\",\n",
            "    \"292\": \"tiger, Panthera tigris\",\n",
            "    \"293\": \"cheetah, chetah, Acinonyx jubatus\",\n",
            "    \"294\": \"brown bear, bruin, Ursus arctos\",\n",
            "    \"295\": \"American black bear, black bear, Ursus americanus, Euarctos americanus\",\n",
            "    \"296\": \"ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus\",\n",
            "    \"297\": \"sloth bear, Melursus ursinus, Ursus ursinus\",\n",
            "    \"298\": \"mongoose\",\n",
            "    \"299\": \"meerkat, mierkat\",\n",
            "    \"300\": \"tiger beetle\",\n",
            "    \"301\": \"ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle\",\n",
            "    \"302\": \"ground beetle, carabid beetle\",\n",
            "    \"303\": \"long-horned beetle, longicorn, longicorn beetle\",\n",
            "    \"304\": \"leaf beetle, chrysomelid\",\n",
            "    \"305\": \"dung beetle\",\n",
            "    \"306\": \"rhinoceros beetle\",\n",
            "    \"307\": \"weevil\",\n",
            "    \"308\": \"fly\",\n",
            "    \"309\": \"bee\",\n",
            "    \"310\": \"ant, emmet, pismire\",\n",
            "    \"311\": \"grasshopper, hopper\",\n",
            "    \"312\": \"cricket\",\n",
            "    \"313\": \"walking stick, walkingstick, stick insect\",\n",
            "    \"314\": \"cockroach, roach\",\n",
            "    \"315\": \"mantis, mantid\",\n",
            "    \"316\": \"cicada, cicala\",\n",
            "    \"317\": \"leafhopper\",\n",
            "    \"318\": \"lacewing, lacewing fly\",\n",
            "    \"319\": \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\",\n",
            "    \"320\": \"damselfly\",\n",
            "    \"321\": \"admiral\",\n",
            "    \"322\": \"ringlet, ringlet butterfly\",\n",
            "    \"323\": \"monarch, monarch butterfly, milkweed butterfly, Danaus plexippus\",\n",
            "    \"324\": \"cabbage butterfly\",\n",
            "    \"325\": \"sulphur butterfly, sulfur butterfly\",\n",
            "    \"326\": \"lycaenid, lycaenid butterfly\",\n",
            "    \"327\": \"starfish, sea star\",\n",
            "    \"328\": \"sea urchin\",\n",
            "    \"329\": \"sea cucumber, holothurian\",\n",
            "    \"330\": \"wood rabbit, cottontail, cottontail rabbit\",\n",
            "    \"331\": \"hare\",\n",
            "    \"332\": \"Angora, Angora rabbit\",\n",
            "    \"333\": \"hamster\",\n",
            "    \"334\": \"porcupine, hedgehog\",\n",
            "    \"335\": \"fox squirrel, eastern fox squirrel, Sciurus niger\",\n",
            "    \"336\": \"marmot\",\n",
            "    \"337\": \"beaver\",\n",
            "    \"338\": \"guinea pig, Cavia cobaya\",\n",
            "    \"339\": \"sorrel\",\n",
            "    \"340\": \"zebra\",\n",
            "    \"341\": \"hog, pig, grunter, squealer, Sus scrofa\",\n",
            "    \"342\": \"wild boar, boar, Sus scrofa\",\n",
            "    \"343\": \"warthog\",\n",
            "    \"344\": \"hippopotamus, hippo, river horse, Hippopotamus amphibius\",\n",
            "    \"345\": \"ox\",\n",
            "    \"346\": \"water buffalo, water ox, Asiatic buffalo, Bubalus bubalis\",\n",
            "    \"347\": \"bison\",\n",
            "    \"348\": \"ram, tup\",\n",
            "    \"349\": \"bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis\",\n",
            "    \"350\": \"ibex, Capra ibex\",\n",
            "    \"351\": \"hartebeest\",\n",
            "    \"352\": \"impala, Aepyceros melampus\",\n",
            "    \"353\": \"gazelle\",\n",
            "    \"354\": \"Arabian camel, dromedary, Camelus dromedarius\",\n",
            "    \"355\": \"llama\",\n",
            "    \"356\": \"weasel\",\n",
            "    \"357\": \"mink\",\n",
            "    \"358\": \"polecat, fitch, foulmart, foumart, Mustela putorius\",\n",
            "    \"359\": \"black-footed ferret, ferret, Mustela nigripes\",\n",
            "    \"360\": \"otter\",\n",
            "    \"361\": \"skunk, polecat, wood pussy\",\n",
            "    \"362\": \"badger\",\n",
            "    \"363\": \"armadillo\",\n",
            "    \"364\": \"three-toed sloth, ai, Bradypus tridactylus\",\n",
            "    \"365\": \"orangutan, orang, orangutang, Pongo pygmaeus\",\n",
            "    \"366\": \"gorilla, Gorilla gorilla\",\n",
            "    \"367\": \"chimpanzee, chimp, Pan troglodytes\",\n",
            "    \"368\": \"gibbon, Hylobates lar\",\n",
            "    \"369\": \"siamang, Hylobates syndactylus, Symphalangus syndactylus\",\n",
            "    \"370\": \"guenon, guenon monkey\",\n",
            "    \"371\": \"patas, hussar monkey, Erythrocebus patas\",\n",
            "    \"372\": \"baboon\",\n",
            "    \"373\": \"macaque\",\n",
            "    \"374\": \"langur\",\n",
            "    \"375\": \"colobus, colobus monkey\",\n",
            "    \"376\": \"proboscis monkey, Nasalis larvatus\",\n",
            "    \"377\": \"marmoset\",\n",
            "    \"378\": \"capuchin, ringtail, Cebus capucinus\",\n",
            "    \"379\": \"howler monkey, howler\",\n",
            "    \"380\": \"titi, titi monkey\",\n",
            "    \"381\": \"spider monkey, Ateles geoffroyi\",\n",
            "    \"382\": \"squirrel monkey, Saimiri sciureus\",\n",
            "    \"383\": \"Madagascar cat, ring-tailed lemur, Lemur catta\",\n",
            "    \"384\": \"indri, indris, Indri indri, Indri brevicaudatus\",\n",
            "    \"385\": \"Indian elephant, Elephas maximus\",\n",
            "    \"386\": \"African elephant, Loxodonta africana\",\n",
            "    \"387\": \"lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens\",\n",
            "    \"388\": \"giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca\",\n",
            "    \"389\": \"barracouta, snoek\",\n",
            "    \"390\": \"eel\",\n",
            "    \"391\": \"coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch\",\n",
            "    \"392\": \"rock beauty, Holocanthus tricolor\",\n",
            "    \"393\": \"anemone fish\",\n",
            "    \"394\": \"sturgeon\",\n",
            "    \"395\": \"gar, garfish, garpike, billfish, Lepisosteus osseus\",\n",
            "    \"396\": \"lionfish\",\n",
            "    \"397\": \"puffer, pufferfish, blowfish, globefish\",\n",
            "    \"398\": \"abacus\",\n",
            "    \"399\": \"abaya\",\n",
            "    \"400\": \"academic gown, academic robe, judge's robe\",\n",
            "    \"401\": \"accordion, piano accordion, squeeze box\",\n",
            "    \"402\": \"acoustic guitar\",\n",
            "    \"403\": \"aircraft carrier, carrier, flattop, attack aircraft carrier\",\n",
            "    \"404\": \"airliner\",\n",
            "    \"405\": \"airship, dirigible\",\n",
            "    \"406\": \"altar\",\n",
            "    \"407\": \"ambulance\",\n",
            "    \"408\": \"amphibian, amphibious vehicle\",\n",
            "    \"409\": \"analog clock\",\n",
            "    \"410\": \"apiary, bee house\",\n",
            "    \"411\": \"apron\",\n",
            "    \"412\": \"ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin\",\n",
            "    \"413\": \"assault rifle, assault gun\",\n",
            "    \"414\": \"backpack, back pack, knapsack, packsack, rucksack, haversack\",\n",
            "    \"415\": \"bakery, bakeshop, bakehouse\",\n",
            "    \"416\": \"balance beam, beam\",\n",
            "    \"417\": \"balloon\",\n",
            "    \"418\": \"ballpoint, ballpoint pen, ballpen, Biro\",\n",
            "    \"419\": \"Band Aid\",\n",
            "    \"420\": \"banjo\",\n",
            "    \"421\": \"bannister, banister, balustrade, balusters, handrail\",\n",
            "    \"422\": \"barbell\",\n",
            "    \"423\": \"barber chair\",\n",
            "    \"424\": \"barbershop\",\n",
            "    \"425\": \"barn\",\n",
            "    \"426\": \"barometer\",\n",
            "    \"427\": \"barrel, cask\",\n",
            "    \"428\": \"barrow, garden cart, lawn cart, wheelbarrow\",\n",
            "    \"429\": \"baseball\",\n",
            "    \"430\": \"basketball\",\n",
            "    \"431\": \"bassinet\",\n",
            "    \"432\": \"bassoon\",\n",
            "    \"433\": \"bathing cap, swimming cap\",\n",
            "    \"434\": \"bath towel\",\n",
            "    \"435\": \"bathtub, bathing tub, bath, tub\",\n",
            "    \"436\": \"beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon\",\n",
            "    \"437\": \"beacon, lighthouse, beacon light, pharos\",\n",
            "    \"438\": \"beaker\",\n",
            "    \"439\": \"bearskin, busby, shako\",\n",
            "    \"440\": \"beer bottle\",\n",
            "    \"441\": \"beer glass\",\n",
            "    \"442\": \"bell cote, bell cot\",\n",
            "    \"443\": \"bib\",\n",
            "    \"444\": \"bicycle-built-for-two, tandem bicycle, tandem\",\n",
            "    \"445\": \"bikini, two-piece\",\n",
            "    \"446\": \"binder, ring-binder\",\n",
            "    \"447\": \"binoculars, field glasses, opera glasses\",\n",
            "    \"448\": \"birdhouse\",\n",
            "    \"449\": \"boathouse\",\n",
            "    \"450\": \"bobsled, bobsleigh, bob\",\n",
            "    \"451\": \"bolo tie, bolo, bola tie, bola\",\n",
            "    \"452\": \"bonnet, poke bonnet\",\n",
            "    \"453\": \"bookcase\",\n",
            "    \"454\": \"bookshop, bookstore, bookstall\",\n",
            "    \"455\": \"bottlecap\",\n",
            "    \"456\": \"bow\",\n",
            "    \"457\": \"bow tie, bow-tie, bowtie\",\n",
            "    \"458\": \"brass, memorial tablet, plaque\",\n",
            "    \"459\": \"brassiere, bra, bandeau\",\n",
            "    \"460\": \"breakwater, groin, groyne, mole, bulwark, seawall, jetty\",\n",
            "    \"461\": \"breastplate, aegis, egis\",\n",
            "    \"462\": \"broom\",\n",
            "    \"463\": \"bucket, pail\",\n",
            "    \"464\": \"buckle\",\n",
            "    \"465\": \"bulletproof vest\",\n",
            "    \"466\": \"bullet train, bullet\",\n",
            "    \"467\": \"butcher shop, meat market\",\n",
            "    \"468\": \"cab, hack, taxi, taxicab\",\n",
            "    \"469\": \"caldron, cauldron\",\n",
            "    \"470\": \"candle, taper, wax light\",\n",
            "    \"471\": \"cannon\",\n",
            "    \"472\": \"canoe\",\n",
            "    \"473\": \"can opener, tin opener\",\n",
            "    \"474\": \"cardigan\",\n",
            "    \"475\": \"car mirror\",\n",
            "    \"476\": \"carousel, carrousel, merry-go-round, roundabout, whirligig\",\n",
            "    \"477\": \"carpenter's kit, tool kit\",\n",
            "    \"478\": \"carton\",\n",
            "    \"479\": \"car wheel\",\n",
            "    \"480\": \"cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM\",\n",
            "    \"481\": \"cassette\",\n",
            "    \"482\": \"cassette player\",\n",
            "    \"483\": \"castle\",\n",
            "    \"484\": \"catamaran\",\n",
            "    \"485\": \"CD player\",\n",
            "    \"486\": \"cello, violoncello\",\n",
            "    \"487\": \"cellular telephone, cellular phone, cellphone, cell, mobile phone\",\n",
            "    \"488\": \"chain\",\n",
            "    \"489\": \"chainlink fence\",\n",
            "    \"490\": \"chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour\",\n",
            "    \"491\": \"chain saw, chainsaw\",\n",
            "    \"492\": \"chest\",\n",
            "    \"493\": \"chiffonier, commode\",\n",
            "    \"494\": \"chime, bell, gong\",\n",
            "    \"495\": \"china cabinet, china closet\",\n",
            "    \"496\": \"Christmas stocking\",\n",
            "    \"497\": \"church, church building\",\n",
            "    \"498\": \"cinema, movie theater, movie theatre, movie house, picture palace\",\n",
            "    \"499\": \"cleaver, meat cleaver, chopper\",\n",
            "    \"500\": \"cliff dwelling\",\n",
            "    \"501\": \"cloak\",\n",
            "    \"502\": \"clog, geta, patten, sabot\",\n",
            "    \"503\": \"cocktail shaker\",\n",
            "    \"504\": \"coffee mug\",\n",
            "    \"505\": \"coffeepot\",\n",
            "    \"506\": \"coil, spiral, volute, whorl, helix\",\n",
            "    \"507\": \"combination lock\",\n",
            "    \"508\": \"computer keyboard, keypad\",\n",
            "    \"509\": \"confectionery, confectionary, candy store\",\n",
            "    \"510\": \"container ship, containership, container vessel\",\n",
            "    \"511\": \"convertible\",\n",
            "    \"512\": \"corkscrew, bottle screw\",\n",
            "    \"513\": \"cornet, horn, trumpet, trump\",\n",
            "    \"514\": \"cowboy boot\",\n",
            "    \"515\": \"cowboy hat, ten-gallon hat\",\n",
            "    \"516\": \"cradle\",\n",
            "    \"517\": \"crane\",\n",
            "    \"518\": \"crash helmet\",\n",
            "    \"519\": \"crate\",\n",
            "    \"520\": \"crib, cot\",\n",
            "    \"521\": \"Crock Pot\",\n",
            "    \"522\": \"croquet ball\",\n",
            "    \"523\": \"crutch\",\n",
            "    \"524\": \"cuirass\",\n",
            "    \"525\": \"dam, dike, dyke\",\n",
            "    \"526\": \"desk\",\n",
            "    \"527\": \"desktop computer\",\n",
            "    \"528\": \"dial telephone, dial phone\",\n",
            "    \"529\": \"diaper, nappy, napkin\",\n",
            "    \"530\": \"digital clock\",\n",
            "    \"531\": \"digital watch\",\n",
            "    \"532\": \"dining table, board\",\n",
            "    \"533\": \"dishrag, dishcloth\",\n",
            "    \"534\": \"dishwasher, dish washer, dishwashing machine\",\n",
            "    \"535\": \"disk brake, disc brake\",\n",
            "    \"536\": \"dock, dockage, docking facility\",\n",
            "    \"537\": \"dogsled, dog sled, dog sleigh\",\n",
            "    \"538\": \"dome\",\n",
            "    \"539\": \"doormat, welcome mat\",\n",
            "    \"540\": \"drilling platform, offshore rig\",\n",
            "    \"541\": \"drum, membranophone, tympan\",\n",
            "    \"542\": \"drumstick\",\n",
            "    \"543\": \"dumbbell\",\n",
            "    \"544\": \"Dutch oven\",\n",
            "    \"545\": \"electric fan, blower\",\n",
            "    \"546\": \"electric guitar\",\n",
            "    \"547\": \"electric locomotive\",\n",
            "    \"548\": \"entertainment center\",\n",
            "    \"549\": \"envelope\",\n",
            "    \"550\": \"espresso maker\",\n",
            "    \"551\": \"face powder\",\n",
            "    \"552\": \"feather boa, boa\",\n",
            "    \"553\": \"file, file cabinet, filing cabinet\",\n",
            "    \"554\": \"fireboat\",\n",
            "    \"555\": \"fire engine, fire truck\",\n",
            "    \"556\": \"fire screen, fireguard\",\n",
            "    \"557\": \"flagpole, flagstaff\",\n",
            "    \"558\": \"flute, transverse flute\",\n",
            "    \"559\": \"folding chair\",\n",
            "    \"560\": \"football helmet\",\n",
            "    \"561\": \"forklift\",\n",
            "    \"562\": \"fountain\",\n",
            "    \"563\": \"fountain pen\",\n",
            "    \"564\": \"four-poster\",\n",
            "    \"565\": \"freight car\",\n",
            "    \"566\": \"French horn, horn\",\n",
            "    \"567\": \"frying pan, frypan, skillet\",\n",
            "    \"568\": \"fur coat\",\n",
            "    \"569\": \"garbage truck, dustcart\",\n",
            "    \"570\": \"gasmask, respirator, gas helmet\",\n",
            "    \"571\": \"gas pump, gasoline pump, petrol pump, island dispenser\",\n",
            "    \"572\": \"goblet\",\n",
            "    \"573\": \"go-kart\",\n",
            "    \"574\": \"golf ball\",\n",
            "    \"575\": \"golfcart, golf cart\",\n",
            "    \"576\": \"gondola\",\n",
            "    \"577\": \"gong, tam-tam\",\n",
            "    \"578\": \"gown\",\n",
            "    \"579\": \"grand piano, grand\",\n",
            "    \"580\": \"greenhouse, nursery, glasshouse\",\n",
            "    \"581\": \"grille, radiator grille\",\n",
            "    \"582\": \"grocery store, grocery, food market, market\",\n",
            "    \"583\": \"guillotine\",\n",
            "    \"584\": \"hair slide\",\n",
            "    \"585\": \"hair spray\",\n",
            "    \"586\": \"half track\",\n",
            "    \"587\": \"hammer\",\n",
            "    \"588\": \"hamper\",\n",
            "    \"589\": \"hand blower, blow dryer, blow drier, hair dryer, hair drier\",\n",
            "    \"590\": \"hand-held computer, hand-held microcomputer\",\n",
            "    \"591\": \"handkerchief, hankie, hanky, hankey\",\n",
            "    \"592\": \"hard disc, hard disk, fixed disk\",\n",
            "    \"593\": \"harmonica, mouth organ, harp, mouth harp\",\n",
            "    \"594\": \"harp\",\n",
            "    \"595\": \"harvester, reaper\",\n",
            "    \"596\": \"hatchet\",\n",
            "    \"597\": \"holster\",\n",
            "    \"598\": \"home theater, home theatre\",\n",
            "    \"599\": \"honeycomb\",\n",
            "    \"600\": \"hook, claw\",\n",
            "    \"601\": \"hoopskirt, crinoline\",\n",
            "    \"602\": \"horizontal bar, high bar\",\n",
            "    \"603\": \"horse cart, horse-cart\",\n",
            "    \"604\": \"hourglass\",\n",
            "    \"605\": \"iPod\",\n",
            "    \"606\": \"iron, smoothing iron\",\n",
            "    \"607\": \"jack-o'-lantern\",\n",
            "    \"608\": \"jean, blue jean, denim\",\n",
            "    \"609\": \"jeep, landrover\",\n",
            "    \"610\": \"jersey, T-shirt, tee shirt\",\n",
            "    \"611\": \"jigsaw puzzle\",\n",
            "    \"612\": \"jinrikisha, ricksha, rickshaw\",\n",
            "    \"613\": \"joystick\",\n",
            "    \"614\": \"kimono\",\n",
            "    \"615\": \"knee pad\",\n",
            "    \"616\": \"knot\",\n",
            "    \"617\": \"lab coat, laboratory coat\",\n",
            "    \"618\": \"ladle\",\n",
            "    \"619\": \"lampshade, lamp shade\",\n",
            "    \"620\": \"laptop, laptop computer\",\n",
            "    \"621\": \"lawn mower, mower\",\n",
            "    \"622\": \"lens cap, lens cover\",\n",
            "    \"623\": \"letter opener, paper knife, paperknife\",\n",
            "    \"624\": \"library\",\n",
            "    \"625\": \"lifeboat\",\n",
            "    \"626\": \"lighter, light, igniter, ignitor\",\n",
            "    \"627\": \"limousine, limo\",\n",
            "    \"628\": \"liner, ocean liner\",\n",
            "    \"629\": \"lipstick, lip rouge\",\n",
            "    \"630\": \"Loafer\",\n",
            "    \"631\": \"lotion\",\n",
            "    \"632\": \"loudspeaker, speaker, speaker unit, loudspeaker system, speaker system\",\n",
            "    \"633\": \"loupe, jeweler's loupe\",\n",
            "    \"634\": \"lumbermill, sawmill\",\n",
            "    \"635\": \"magnetic compass\",\n",
            "    \"636\": \"mailbag, postbag\",\n",
            "    \"637\": \"mailbox, letter box\",\n",
            "    \"638\": \"maillot\",\n",
            "    \"639\": \"maillot, tank suit\",\n",
            "    \"640\": \"manhole cover\",\n",
            "    \"641\": \"maraca\",\n",
            "    \"642\": \"marimba, xylophone\",\n",
            "    \"643\": \"mask\",\n",
            "    \"644\": \"matchstick\",\n",
            "    \"645\": \"maypole\",\n",
            "    \"646\": \"maze, labyrinth\",\n",
            "    \"647\": \"measuring cup\",\n",
            "    \"648\": \"medicine chest, medicine cabinet\",\n",
            "    \"649\": \"megalith, megalithic structure\",\n",
            "    \"650\": \"microphone, mike\",\n",
            "    \"651\": \"microwave, microwave oven\",\n",
            "    \"652\": \"military uniform\",\n",
            "    \"653\": \"milk can\",\n",
            "    \"654\": \"minibus\",\n",
            "    \"655\": \"miniskirt, mini\",\n",
            "    \"656\": \"minivan\",\n",
            "    \"657\": \"missile\",\n",
            "    \"658\": \"mitten\",\n",
            "    \"659\": \"mixing bowl\",\n",
            "    \"660\": \"mobile home, manufactured home\",\n",
            "    \"661\": \"Model T\",\n",
            "    \"662\": \"modem\",\n",
            "    \"663\": \"monastery\",\n",
            "    \"664\": \"monitor\",\n",
            "    \"665\": \"moped\",\n",
            "    \"666\": \"mortar\",\n",
            "    \"667\": \"mortarboard\",\n",
            "    \"668\": \"mosque\",\n",
            "    \"669\": \"mosquito net\",\n",
            "    \"670\": \"motor scooter, scooter\",\n",
            "    \"671\": \"mountain bike, all-terrain bike, off-roader\",\n",
            "    \"672\": \"mountain tent\",\n",
            "    \"673\": \"mouse, computer mouse\",\n",
            "    \"674\": \"mousetrap\",\n",
            "    \"675\": \"moving van\",\n",
            "    \"676\": \"muzzle\",\n",
            "    \"677\": \"nail\",\n",
            "    \"678\": \"neck brace\",\n",
            "    \"679\": \"necklace\",\n",
            "    \"680\": \"nipple\",\n",
            "    \"681\": \"notebook, notebook computer\",\n",
            "    \"682\": \"obelisk\",\n",
            "    \"683\": \"oboe, hautboy, hautbois\",\n",
            "    \"684\": \"ocarina, sweet potato\",\n",
            "    \"685\": \"odometer, hodometer, mileometer, milometer\",\n",
            "    \"686\": \"oil filter\",\n",
            "    \"687\": \"organ, pipe organ\",\n",
            "    \"688\": \"oscilloscope, scope, cathode-ray oscilloscope, CRO\",\n",
            "    \"689\": \"overskirt\",\n",
            "    \"690\": \"oxcart\",\n",
            "    \"691\": \"oxygen mask\",\n",
            "    \"692\": \"packet\",\n",
            "    \"693\": \"paddle, boat paddle\",\n",
            "    \"694\": \"paddlewheel, paddle wheel\",\n",
            "    \"695\": \"padlock\",\n",
            "    \"696\": \"paintbrush\",\n",
            "    \"697\": \"pajama, pyjama, pj's, jammies\",\n",
            "    \"698\": \"palace\",\n",
            "    \"699\": \"panpipe, pandean pipe, syrinx\",\n",
            "    \"700\": \"paper towel\",\n",
            "    \"701\": \"parachute, chute\",\n",
            "    \"702\": \"parallel bars, bars\",\n",
            "    \"703\": \"park bench\",\n",
            "    \"704\": \"parking meter\",\n",
            "    \"705\": \"passenger car, coach, carriage\",\n",
            "    \"706\": \"patio, terrace\",\n",
            "    \"707\": \"pay-phone, pay-station\",\n",
            "    \"708\": \"pedestal, plinth, footstall\",\n",
            "    \"709\": \"pencil box, pencil case\",\n",
            "    \"710\": \"pencil sharpener\",\n",
            "    \"711\": \"perfume, essence\",\n",
            "    \"712\": \"Petri dish\",\n",
            "    \"713\": \"photocopier\",\n",
            "    \"714\": \"pick, plectrum, plectron\",\n",
            "    \"715\": \"pickelhaube\",\n",
            "    \"716\": \"picket fence, paling\",\n",
            "    \"717\": \"pickup, pickup truck\",\n",
            "    \"718\": \"pier\",\n",
            "    \"719\": \"piggy bank, penny bank\",\n",
            "    \"720\": \"pill bottle\",\n",
            "    \"721\": \"pillow\",\n",
            "    \"722\": \"ping-pong ball\",\n",
            "    \"723\": \"pinwheel\",\n",
            "    \"724\": \"pirate, pirate ship\",\n",
            "    \"725\": \"pitcher, ewer\",\n",
            "    \"726\": \"plane, carpenter's plane, woodworking plane\",\n",
            "    \"727\": \"planetarium\",\n",
            "    \"728\": \"plastic bag\",\n",
            "    \"729\": \"plate rack\",\n",
            "    \"730\": \"plow, plough\",\n",
            "    \"731\": \"plunger, plumber's helper\",\n",
            "    \"732\": \"Polaroid camera, Polaroid Land camera\",\n",
            "    \"733\": \"pole\",\n",
            "    \"734\": \"police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria\",\n",
            "    \"735\": \"poncho\",\n",
            "    \"736\": \"pool table, billiard table, snooker table\",\n",
            "    \"737\": \"pop bottle, soda bottle\",\n",
            "    \"738\": \"pot, flowerpot\",\n",
            "    \"739\": \"potter's wheel\",\n",
            "    \"740\": \"power drill\",\n",
            "    \"741\": \"prayer rug, prayer mat\",\n",
            "    \"742\": \"printer\",\n",
            "    \"743\": \"prison, prison house\",\n",
            "    \"744\": \"projectile, missile\",\n",
            "    \"745\": \"projector\",\n",
            "    \"746\": \"puck, hockey puck\",\n",
            "    \"747\": \"punching bag, punch bag, punching ball, punchball\",\n",
            "    \"748\": \"purse\",\n",
            "    \"749\": \"quill, quill pen\",\n",
            "    \"750\": \"quilt, comforter, comfort, puff\",\n",
            "    \"751\": \"racer, race car, racing car\",\n",
            "    \"752\": \"racket, racquet\",\n",
            "    \"753\": \"radiator\",\n",
            "    \"754\": \"radio, wireless\",\n",
            "    \"755\": \"radio telescope, radio reflector\",\n",
            "    \"756\": \"rain barrel\",\n",
            "    \"757\": \"recreational vehicle, RV, R.V.\",\n",
            "    \"758\": \"reel\",\n",
            "    \"759\": \"reflex camera\",\n",
            "    \"760\": \"refrigerator, icebox\",\n",
            "    \"761\": \"remote control, remote\",\n",
            "    \"762\": \"restaurant, eating house, eating place, eatery\",\n",
            "    \"763\": \"revolver, six-gun, six-shooter\",\n",
            "    \"764\": \"rifle\",\n",
            "    \"765\": \"rocking chair, rocker\",\n",
            "    \"766\": \"rotisserie\",\n",
            "    \"767\": \"rubber eraser, rubber, pencil eraser\",\n",
            "    \"768\": \"rugby ball\",\n",
            "    \"769\": \"rule, ruler\",\n",
            "    \"770\": \"running shoe\",\n",
            "    \"771\": \"safe\",\n",
            "    \"772\": \"safety pin\",\n",
            "    \"773\": \"saltshaker, salt shaker\",\n",
            "    \"774\": \"sandal\",\n",
            "    \"775\": \"sarong\",\n",
            "    \"776\": \"sax, saxophone\",\n",
            "    \"777\": \"scabbard\",\n",
            "    \"778\": \"scale, weighing machine\",\n",
            "    \"779\": \"school bus\",\n",
            "    \"780\": \"schooner\",\n",
            "    \"781\": \"scoreboard\",\n",
            "    \"782\": \"screen, CRT screen\",\n",
            "    \"783\": \"screw\",\n",
            "    \"784\": \"screwdriver\",\n",
            "    \"785\": \"seat belt, seatbelt\",\n",
            "    \"786\": \"sewing machine\",\n",
            "    \"787\": \"shield, buckler\",\n",
            "    \"788\": \"shoe shop, shoe-shop, shoe store\",\n",
            "    \"789\": \"shoji\",\n",
            "    \"790\": \"shopping basket\",\n",
            "    \"791\": \"shopping cart\",\n",
            "    \"792\": \"shovel\",\n",
            "    \"793\": \"shower cap\",\n",
            "    \"794\": \"shower curtain\",\n",
            "    \"795\": \"ski\",\n",
            "    \"796\": \"ski mask\",\n",
            "    \"797\": \"sleeping bag\",\n",
            "    \"798\": \"slide rule, slipstick\",\n",
            "    \"799\": \"sliding door\",\n",
            "    \"800\": \"slot, one-armed bandit\",\n",
            "    \"801\": \"snorkel\",\n",
            "    \"802\": \"snowmobile\",\n",
            "    \"803\": \"snowplow, snowplough\",\n",
            "    \"804\": \"soap dispenser\",\n",
            "    \"805\": \"soccer ball\",\n",
            "    \"806\": \"sock\",\n",
            "    \"807\": \"solar dish, solar collector, solar furnace\",\n",
            "    \"808\": \"sombrero\",\n",
            "    \"809\": \"soup bowl\",\n",
            "    \"810\": \"space bar\",\n",
            "    \"811\": \"space heater\",\n",
            "    \"812\": \"space shuttle\",\n",
            "    \"813\": \"spatula\",\n",
            "    \"814\": \"speedboat\",\n",
            "    \"815\": \"spider web, spider's web\",\n",
            "    \"816\": \"spindle\",\n",
            "    \"817\": \"sports car, sport car\",\n",
            "    \"818\": \"spotlight, spot\",\n",
            "    \"819\": \"stage\",\n",
            "    \"820\": \"steam locomotive\",\n",
            "    \"821\": \"steel arch bridge\",\n",
            "    \"822\": \"steel drum\",\n",
            "    \"823\": \"stethoscope\",\n",
            "    \"824\": \"stole\",\n",
            "    \"825\": \"stone wall\",\n",
            "    \"826\": \"stopwatch, stop watch\",\n",
            "    \"827\": \"stove\",\n",
            "    \"828\": \"strainer\",\n",
            "    \"829\": \"streetcar, tram, tramcar, trolley, trolley car\",\n",
            "    \"830\": \"stretcher\",\n",
            "    \"831\": \"studio couch, day bed\",\n",
            "    \"832\": \"stupa, tope\",\n",
            "    \"833\": \"submarine, pigboat, sub, U-boat\",\n",
            "    \"834\": \"suit, suit of clothes\",\n",
            "    \"835\": \"sundial\",\n",
            "    \"836\": \"sunglass\",\n",
            "    \"837\": \"sunglasses, dark glasses, shades\",\n",
            "    \"838\": \"sunscreen, sunblock, sun blocker\",\n",
            "    \"839\": \"suspension bridge\",\n",
            "    \"840\": \"swab, swob, mop\",\n",
            "    \"841\": \"sweatshirt\",\n",
            "    \"842\": \"swimming trunks, bathing trunks\",\n",
            "    \"843\": \"swing\",\n",
            "    \"844\": \"switch, electric switch, electrical switch\",\n",
            "    \"845\": \"syringe\",\n",
            "    \"846\": \"table lamp\",\n",
            "    \"847\": \"tank, army tank, armored combat vehicle, armoured combat vehicle\",\n",
            "    \"848\": \"tape player\",\n",
            "    \"849\": \"teapot\",\n",
            "    \"850\": \"teddy, teddy bear\",\n",
            "    \"851\": \"television, television system\",\n",
            "    \"852\": \"tennis ball\",\n",
            "    \"853\": \"thatch, thatched roof\",\n",
            "    \"854\": \"theater curtain, theatre curtain\",\n",
            "    \"855\": \"thimble\",\n",
            "    \"856\": \"thresher, thrasher, threshing machine\",\n",
            "    \"857\": \"throne\",\n",
            "    \"858\": \"tile roof\",\n",
            "    \"859\": \"toaster\",\n",
            "    \"860\": \"tobacco shop, tobacconist shop, tobacconist\",\n",
            "    \"861\": \"toilet seat\",\n",
            "    \"862\": \"torch\",\n",
            "    \"863\": \"totem pole\",\n",
            "    \"864\": \"tow truck, tow car, wrecker\",\n",
            "    \"865\": \"toyshop\",\n",
            "    \"866\": \"tractor\",\n",
            "    \"867\": \"trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi\",\n",
            "    \"868\": \"tray\",\n",
            "    \"869\": \"trench coat\",\n",
            "    \"870\": \"tricycle, trike, velocipede\",\n",
            "    \"871\": \"trimaran\",\n",
            "    \"872\": \"tripod\",\n",
            "    \"873\": \"triumphal arch\",\n",
            "    \"874\": \"trolleybus, trolley coach, trackless trolley\",\n",
            "    \"875\": \"trombone\",\n",
            "    \"876\": \"tub, vat\",\n",
            "    \"877\": \"turnstile\",\n",
            "    \"878\": \"typewriter keyboard\",\n",
            "    \"879\": \"umbrella\",\n",
            "    \"880\": \"unicycle, monocycle\",\n",
            "    \"881\": \"upright, upright piano\",\n",
            "    \"882\": \"vacuum, vacuum cleaner\",\n",
            "    \"883\": \"vase\",\n",
            "    \"884\": \"vault\",\n",
            "    \"885\": \"velvet\",\n",
            "    \"886\": \"vending machine\",\n",
            "    \"887\": \"vestment\",\n",
            "    \"888\": \"viaduct\",\n",
            "    \"889\": \"violin, fiddle\",\n",
            "    \"890\": \"volleyball\",\n",
            "    \"891\": \"waffle iron\",\n",
            "    \"892\": \"wall clock\",\n",
            "    \"893\": \"wallet, billfold, notecase, pocketbook\",\n",
            "    \"894\": \"wardrobe, closet, press\",\n",
            "    \"895\": \"warplane, military plane\",\n",
            "    \"896\": \"washbasin, handbasin, washbowl, lavabo, wash-hand basin\",\n",
            "    \"897\": \"washer, automatic washer, washing machine\",\n",
            "    \"898\": \"water bottle\",\n",
            "    \"899\": \"water jug\",\n",
            "    \"900\": \"water tower\",\n",
            "    \"901\": \"whiskey jug\",\n",
            "    \"902\": \"whistle\",\n",
            "    \"903\": \"wig\",\n",
            "    \"904\": \"window screen\",\n",
            "    \"905\": \"window shade\",\n",
            "    \"906\": \"Windsor tie\",\n",
            "    \"907\": \"wine bottle\",\n",
            "    \"908\": \"wing\",\n",
            "    \"909\": \"wok\",\n",
            "    \"910\": \"wooden spoon\",\n",
            "    \"911\": \"wool, woolen, woollen\",\n",
            "    \"912\": \"worm fence, snake fence, snake-rail fence, Virginia fence\",\n",
            "    \"913\": \"wreck\",\n",
            "    \"914\": \"yawl\",\n",
            "    \"915\": \"yurt\",\n",
            "    \"916\": \"web site, website, internet site, site\",\n",
            "    \"917\": \"comic book\",\n",
            "    \"918\": \"crossword puzzle, crossword\",\n",
            "    \"919\": \"street sign\",\n",
            "    \"920\": \"traffic light, traffic signal, stoplight\",\n",
            "    \"921\": \"book jacket, dust cover, dust jacket, dust wrapper\",\n",
            "    \"922\": \"menu\",\n",
            "    \"923\": \"plate\",\n",
            "    \"924\": \"guacamole\",\n",
            "    \"925\": \"consomme\",\n",
            "    \"926\": \"hot pot, hotpot\",\n",
            "    \"927\": \"trifle\",\n",
            "    \"928\": \"ice cream, icecream\",\n",
            "    \"929\": \"ice lolly, lolly, lollipop, popsicle\",\n",
            "    \"930\": \"French loaf\",\n",
            "    \"931\": \"bagel, beigel\",\n",
            "    \"932\": \"pretzel\",\n",
            "    \"933\": \"cheeseburger\",\n",
            "    \"934\": \"hotdog, hot dog, red hot\",\n",
            "    \"935\": \"mashed potato\",\n",
            "    \"936\": \"head cabbage\",\n",
            "    \"937\": \"broccoli\",\n",
            "    \"938\": \"cauliflower\",\n",
            "    \"939\": \"zucchini, courgette\",\n",
            "    \"940\": \"spaghetti squash\",\n",
            "    \"941\": \"acorn squash\",\n",
            "    \"942\": \"butternut squash\",\n",
            "    \"943\": \"cucumber, cuke\",\n",
            "    \"944\": \"artichoke, globe artichoke\",\n",
            "    \"945\": \"bell pepper\",\n",
            "    \"946\": \"cardoon\",\n",
            "    \"947\": \"mushroom\",\n",
            "    \"948\": \"Granny Smith\",\n",
            "    \"949\": \"strawberry\",\n",
            "    \"950\": \"orange\",\n",
            "    \"951\": \"lemon\",\n",
            "    \"952\": \"fig\",\n",
            "    \"953\": \"pineapple, ananas\",\n",
            "    \"954\": \"banana\",\n",
            "    \"955\": \"jackfruit, jak, jack\",\n",
            "    \"956\": \"custard apple\",\n",
            "    \"957\": \"pomegranate\",\n",
            "    \"958\": \"hay\",\n",
            "    \"959\": \"carbonara\",\n",
            "    \"960\": \"chocolate sauce, chocolate syrup\",\n",
            "    \"961\": \"dough\",\n",
            "    \"962\": \"meat loaf, meatloaf\",\n",
            "    \"963\": \"pizza, pizza pie\",\n",
            "    \"964\": \"potpie\",\n",
            "    \"965\": \"burrito\",\n",
            "    \"966\": \"red wine\",\n",
            "    \"967\": \"espresso\",\n",
            "    \"968\": \"cup\",\n",
            "    \"969\": \"eggnog\",\n",
            "    \"970\": \"alp\",\n",
            "    \"971\": \"bubble\",\n",
            "    \"972\": \"cliff, drop, drop-off\",\n",
            "    \"973\": \"coral reef\",\n",
            "    \"974\": \"geyser\",\n",
            "    \"975\": \"lakeside, lakeshore\",\n",
            "    \"976\": \"promontory, headland, head, foreland\",\n",
            "    \"977\": \"sandbar, sand bar\",\n",
            "    \"978\": \"seashore, coast, seacoast, sea-coast\",\n",
            "    \"979\": \"valley, vale\",\n",
            "    \"980\": \"volcano\",\n",
            "    \"981\": \"ballplayer, baseball player\",\n",
            "    \"982\": \"groom, bridegroom\",\n",
            "    \"983\": \"scuba diver\",\n",
            "    \"984\": \"rapeseed\",\n",
            "    \"985\": \"daisy\",\n",
            "    \"986\": \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\",\n",
            "    \"987\": \"corn\",\n",
            "    \"988\": \"acorn\",\n",
            "    \"989\": \"hip, rose hip, rosehip\",\n",
            "    \"990\": \"buckeye, horse chestnut, conker\",\n",
            "    \"991\": \"coral fungus\",\n",
            "    \"992\": \"agaric\",\n",
            "    \"993\": \"gyromitra\",\n",
            "    \"994\": \"stinkhorn, carrion fungus\",\n",
            "    \"995\": \"earthstar\",\n",
            "    \"996\": \"hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa\",\n",
            "    \"997\": \"bolete\",\n",
            "    \"998\": \"ear, spike, capitulum\",\n",
            "    \"999\": \"toilet tissue, toilet paper, bathroom tissue\"\n",
            "  },\n",
            "  \"image_size\": 224,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"Afghan hound, Afghan\": 160,\n",
            "    \"African chameleon, Chamaeleo chamaeleon\": 47,\n",
            "    \"African crocodile, Nile crocodile, Crocodylus niloticus\": 49,\n",
            "    \"African elephant, Loxodonta africana\": 386,\n",
            "    \"African grey, African gray, Psittacus erithacus\": 87,\n",
            "    \"African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus\": 275,\n",
            "    \"Airedale, Airedale terrier\": 191,\n",
            "    \"American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier\": 180,\n",
            "    \"American alligator, Alligator mississipiensis\": 50,\n",
            "    \"American black bear, black bear, Ursus americanus, Euarctos americanus\": 295,\n",
            "    \"American chameleon, anole, Anolis carolinensis\": 40,\n",
            "    \"American coot, marsh hen, mud hen, water hen, Fulica americana\": 137,\n",
            "    \"American egret, great white heron, Egretta albus\": 132,\n",
            "    \"American lobster, Northern lobster, Maine lobster, Homarus americanus\": 122,\n",
            "    \"Angora, Angora rabbit\": 332,\n",
            "    \"Appenzeller\": 240,\n",
            "    \"Arabian camel, dromedary, Camelus dromedarius\": 354,\n",
            "    \"Arctic fox, white fox, Alopex lagopus\": 279,\n",
            "    \"Australian terrier\": 193,\n",
            "    \"Band Aid\": 419,\n",
            "    \"Bedlington terrier\": 181,\n",
            "    \"Bernese mountain dog\": 239,\n",
            "    \"Blenheim spaniel\": 156,\n",
            "    \"Border collie\": 232,\n",
            "    \"Border terrier\": 182,\n",
            "    \"Boston bull, Boston terrier\": 195,\n",
            "    \"Bouvier des Flandres, Bouviers des Flandres\": 233,\n",
            "    \"Brabancon griffon\": 262,\n",
            "    \"Brittany spaniel\": 215,\n",
            "    \"CD player\": 485,\n",
            "    \"Cardigan, Cardigan Welsh corgi\": 264,\n",
            "    \"Chesapeake Bay retriever\": 209,\n",
            "    \"Chihuahua\": 151,\n",
            "    \"Christmas stocking\": 496,\n",
            "    \"Crock Pot\": 521,\n",
            "    \"Dandie Dinmont, Dandie Dinmont terrier\": 194,\n",
            "    \"Doberman, Doberman pinscher\": 236,\n",
            "    \"Dungeness crab, Cancer magister\": 118,\n",
            "    \"Dutch oven\": 544,\n",
            "    \"Egyptian cat\": 285,\n",
            "    \"English foxhound\": 167,\n",
            "    \"English setter\": 212,\n",
            "    \"English springer, English springer spaniel\": 217,\n",
            "    \"EntleBucher\": 241,\n",
            "    \"Eskimo dog, husky\": 248,\n",
            "    \"European fire salamander, Salamandra salamandra\": 25,\n",
            "    \"European gallinule, Porphyrio porphyrio\": 136,\n",
            "    \"French bulldog\": 245,\n",
            "    \"French horn, horn\": 566,\n",
            "    \"French loaf\": 930,\n",
            "    \"German shepherd, German shepherd dog, German police dog, alsatian\": 235,\n",
            "    \"German short-haired pointer\": 210,\n",
            "    \"Gila monster, Heloderma suspectum\": 45,\n",
            "    \"Gordon setter\": 214,\n",
            "    \"Granny Smith\": 948,\n",
            "    \"Great Dane\": 246,\n",
            "    \"Great Pyrenees\": 257,\n",
            "    \"Greater Swiss Mountain dog\": 238,\n",
            "    \"Ibizan hound, Ibizan Podenco\": 173,\n",
            "    \"Indian cobra, Naja naja\": 63,\n",
            "    \"Indian elephant, Elephas maximus\": 385,\n",
            "    \"Irish setter, red setter\": 213,\n",
            "    \"Irish terrier\": 184,\n",
            "    \"Irish water spaniel\": 221,\n",
            "    \"Irish wolfhound\": 170,\n",
            "    \"Italian greyhound\": 171,\n",
            "    \"Japanese spaniel\": 152,\n",
            "    \"Kerry blue terrier\": 183,\n",
            "    \"Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis\": 48,\n",
            "    \"Labrador retriever\": 208,\n",
            "    \"Lakeland terrier\": 189,\n",
            "    \"Leonberg\": 255,\n",
            "    \"Lhasa, Lhasa apso\": 204,\n",
            "    \"Loafer\": 630,\n",
            "    \"Madagascar cat, ring-tailed lemur, Lemur catta\": 383,\n",
            "    \"Maltese dog, Maltese terrier, Maltese\": 153,\n",
            "    \"Mexican hairless\": 268,\n",
            "    \"Model T\": 661,\n",
            "    \"Newfoundland, Newfoundland dog\": 256,\n",
            "    \"Norfolk terrier\": 185,\n",
            "    \"Norwegian elkhound, elkhound\": 174,\n",
            "    \"Norwich terrier\": 186,\n",
            "    \"Old English sheepdog, bobtail\": 229,\n",
            "    \"Pekinese, Pekingese, Peke\": 154,\n",
            "    \"Pembroke, Pembroke Welsh corgi\": 263,\n",
            "    \"Persian cat\": 283,\n",
            "    \"Petri dish\": 712,\n",
            "    \"Polaroid camera, Polaroid Land camera\": 732,\n",
            "    \"Pomeranian\": 259,\n",
            "    \"Rhodesian ridgeback\": 159,\n",
            "    \"Rottweiler\": 234,\n",
            "    \"Saint Bernard, St Bernard\": 247,\n",
            "    \"Saluki, gazelle hound\": 176,\n",
            "    \"Samoyed, Samoyede\": 258,\n",
            "    \"Scotch terrier, Scottish terrier, Scottie\": 199,\n",
            "    \"Scottish deerhound, deerhound\": 177,\n",
            "    \"Sealyham terrier, Sealyham\": 190,\n",
            "    \"Shetland sheepdog, Shetland sheep dog, Shetland\": 230,\n",
            "    \"Shih-Tzu\": 155,\n",
            "    \"Siamese cat, Siamese\": 284,\n",
            "    \"Siberian husky\": 250,\n",
            "    \"Staffordshire bullterrier, Staffordshire bull terrier\": 179,\n",
            "    \"Sussex spaniel\": 220,\n",
            "    \"Tibetan mastiff\": 244,\n",
            "    \"Tibetan terrier, chrysanthemum dog\": 200,\n",
            "    \"Walker hound, Walker foxhound\": 166,\n",
            "    \"Weimaraner\": 178,\n",
            "    \"Welsh springer spaniel\": 218,\n",
            "    \"West Highland white terrier\": 203,\n",
            "    \"Windsor tie\": 906,\n",
            "    \"Yorkshire terrier\": 187,\n",
            "    \"abacus\": 398,\n",
            "    \"abaya\": 399,\n",
            "    \"academic gown, academic robe, judge's robe\": 400,\n",
            "    \"accordion, piano accordion, squeeze box\": 401,\n",
            "    \"acorn\": 988,\n",
            "    \"acorn squash\": 941,\n",
            "    \"acoustic guitar\": 402,\n",
            "    \"admiral\": 321,\n",
            "    \"affenpinscher, monkey pinscher, monkey dog\": 252,\n",
            "    \"agama\": 42,\n",
            "    \"agaric\": 992,\n",
            "    \"aircraft carrier, carrier, flattop, attack aircraft carrier\": 403,\n",
            "    \"airliner\": 404,\n",
            "    \"airship, dirigible\": 405,\n",
            "    \"albatross, mollymawk\": 146,\n",
            "    \"alligator lizard\": 44,\n",
            "    \"alp\": 970,\n",
            "    \"altar\": 406,\n",
            "    \"ambulance\": 407,\n",
            "    \"amphibian, amphibious vehicle\": 408,\n",
            "    \"analog clock\": 409,\n",
            "    \"anemone fish\": 393,\n",
            "    \"ant, emmet, pismire\": 310,\n",
            "    \"apiary, bee house\": 410,\n",
            "    \"apron\": 411,\n",
            "    \"armadillo\": 363,\n",
            "    \"artichoke, globe artichoke\": 944,\n",
            "    \"ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin\": 412,\n",
            "    \"assault rifle, assault gun\": 413,\n",
            "    \"axolotl, mud puppy, Ambystoma mexicanum\": 29,\n",
            "    \"baboon\": 372,\n",
            "    \"backpack, back pack, knapsack, packsack, rucksack, haversack\": 414,\n",
            "    \"badger\": 362,\n",
            "    \"bagel, beigel\": 931,\n",
            "    \"bakery, bakeshop, bakehouse\": 415,\n",
            "    \"balance beam, beam\": 416,\n",
            "    \"bald eagle, American eagle, Haliaeetus leucocephalus\": 22,\n",
            "    \"balloon\": 417,\n",
            "    \"ballplayer, baseball player\": 981,\n",
            "    \"ballpoint, ballpoint pen, ballpen, Biro\": 418,\n",
            "    \"banana\": 954,\n",
            "    \"banded gecko\": 38,\n",
            "    \"banjo\": 420,\n",
            "    \"bannister, banister, balustrade, balusters, handrail\": 421,\n",
            "    \"barbell\": 422,\n",
            "    \"barber chair\": 423,\n",
            "    \"barbershop\": 424,\n",
            "    \"barn\": 425,\n",
            "    \"barn spider, Araneus cavaticus\": 73,\n",
            "    \"barometer\": 426,\n",
            "    \"barracouta, snoek\": 389,\n",
            "    \"barrel, cask\": 427,\n",
            "    \"barrow, garden cart, lawn cart, wheelbarrow\": 428,\n",
            "    \"baseball\": 429,\n",
            "    \"basenji\": 253,\n",
            "    \"basketball\": 430,\n",
            "    \"basset, basset hound\": 161,\n",
            "    \"bassinet\": 431,\n",
            "    \"bassoon\": 432,\n",
            "    \"bath towel\": 434,\n",
            "    \"bathing cap, swimming cap\": 433,\n",
            "    \"bathtub, bathing tub, bath, tub\": 435,\n",
            "    \"beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon\": 436,\n",
            "    \"beacon, lighthouse, beacon light, pharos\": 437,\n",
            "    \"beagle\": 162,\n",
            "    \"beaker\": 438,\n",
            "    \"bearskin, busby, shako\": 439,\n",
            "    \"beaver\": 337,\n",
            "    \"bee\": 309,\n",
            "    \"bee eater\": 92,\n",
            "    \"beer bottle\": 440,\n",
            "    \"beer glass\": 441,\n",
            "    \"bell cote, bell cot\": 442,\n",
            "    \"bell pepper\": 945,\n",
            "    \"bib\": 443,\n",
            "    \"bicycle-built-for-two, tandem bicycle, tandem\": 444,\n",
            "    \"bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis\": 349,\n",
            "    \"bikini, two-piece\": 445,\n",
            "    \"binder, ring-binder\": 446,\n",
            "    \"binoculars, field glasses, opera glasses\": 447,\n",
            "    \"birdhouse\": 448,\n",
            "    \"bison\": 347,\n",
            "    \"bittern\": 133,\n",
            "    \"black and gold garden spider, Argiope aurantia\": 72,\n",
            "    \"black grouse\": 80,\n",
            "    \"black stork, Ciconia nigra\": 128,\n",
            "    \"black swan, Cygnus atratus\": 100,\n",
            "    \"black widow, Latrodectus mactans\": 75,\n",
            "    \"black-and-tan coonhound\": 165,\n",
            "    \"black-footed ferret, ferret, Mustela nigripes\": 359,\n",
            "    \"bloodhound, sleuthhound\": 163,\n",
            "    \"bluetick\": 164,\n",
            "    \"boa constrictor, Constrictor constrictor\": 61,\n",
            "    \"boathouse\": 449,\n",
            "    \"bobsled, bobsleigh, bob\": 450,\n",
            "    \"bolete\": 997,\n",
            "    \"bolo tie, bolo, bola tie, bola\": 451,\n",
            "    \"bonnet, poke bonnet\": 452,\n",
            "    \"book jacket, dust cover, dust jacket, dust wrapper\": 921,\n",
            "    \"bookcase\": 453,\n",
            "    \"bookshop, bookstore, bookstall\": 454,\n",
            "    \"borzoi, Russian wolfhound\": 169,\n",
            "    \"bottlecap\": 455,\n",
            "    \"bow\": 456,\n",
            "    \"bow tie, bow-tie, bowtie\": 457,\n",
            "    \"box turtle, box tortoise\": 37,\n",
            "    \"boxer\": 242,\n",
            "    \"brain coral\": 109,\n",
            "    \"brambling, Fringilla montifringilla\": 10,\n",
            "    \"brass, memorial tablet, plaque\": 458,\n",
            "    \"brassiere, bra, bandeau\": 459,\n",
            "    \"breakwater, groin, groyne, mole, bulwark, seawall, jetty\": 460,\n",
            "    \"breastplate, aegis, egis\": 461,\n",
            "    \"briard\": 226,\n",
            "    \"broccoli\": 937,\n",
            "    \"broom\": 462,\n",
            "    \"brown bear, bruin, Ursus arctos\": 294,\n",
            "    \"bubble\": 971,\n",
            "    \"bucket, pail\": 463,\n",
            "    \"buckeye, horse chestnut, conker\": 990,\n",
            "    \"buckle\": 464,\n",
            "    \"bulbul\": 16,\n",
            "    \"bull mastiff\": 243,\n",
            "    \"bullet train, bullet\": 466,\n",
            "    \"bulletproof vest\": 465,\n",
            "    \"bullfrog, Rana catesbeiana\": 30,\n",
            "    \"burrito\": 965,\n",
            "    \"bustard\": 138,\n",
            "    \"butcher shop, meat market\": 467,\n",
            "    \"butternut squash\": 942,\n",
            "    \"cab, hack, taxi, taxicab\": 468,\n",
            "    \"cabbage butterfly\": 324,\n",
            "    \"cairn, cairn terrier\": 192,\n",
            "    \"caldron, cauldron\": 469,\n",
            "    \"can opener, tin opener\": 473,\n",
            "    \"candle, taper, wax light\": 470,\n",
            "    \"cannon\": 471,\n",
            "    \"canoe\": 472,\n",
            "    \"capuchin, ringtail, Cebus capucinus\": 378,\n",
            "    \"car mirror\": 475,\n",
            "    \"car wheel\": 479,\n",
            "    \"carbonara\": 959,\n",
            "    \"cardigan\": 474,\n",
            "    \"cardoon\": 946,\n",
            "    \"carousel, carrousel, merry-go-round, roundabout, whirligig\": 476,\n",
            "    \"carpenter's kit, tool kit\": 477,\n",
            "    \"carton\": 478,\n",
            "    \"cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM\": 480,\n",
            "    \"cassette\": 481,\n",
            "    \"cassette player\": 482,\n",
            "    \"castle\": 483,\n",
            "    \"catamaran\": 484,\n",
            "    \"cauliflower\": 938,\n",
            "    \"cello, violoncello\": 486,\n",
            "    \"cellular telephone, cellular phone, cellphone, cell, mobile phone\": 487,\n",
            "    \"centipede\": 79,\n",
            "    \"chain\": 488,\n",
            "    \"chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour\": 490,\n",
            "    \"chain saw, chainsaw\": 491,\n",
            "    \"chainlink fence\": 489,\n",
            "    \"chambered nautilus, pearly nautilus, nautilus\": 117,\n",
            "    \"cheeseburger\": 933,\n",
            "    \"cheetah, chetah, Acinonyx jubatus\": 293,\n",
            "    \"chest\": 492,\n",
            "    \"chickadee\": 19,\n",
            "    \"chiffonier, commode\": 493,\n",
            "    \"chime, bell, gong\": 494,\n",
            "    \"chimpanzee, chimp, Pan troglodytes\": 367,\n",
            "    \"china cabinet, china closet\": 495,\n",
            "    \"chiton, coat-of-mail shell, sea cradle, polyplacophore\": 116,\n",
            "    \"chocolate sauce, chocolate syrup\": 960,\n",
            "    \"chow, chow chow\": 260,\n",
            "    \"church, church building\": 497,\n",
            "    \"cicada, cicala\": 316,\n",
            "    \"cinema, movie theater, movie theatre, movie house, picture palace\": 498,\n",
            "    \"cleaver, meat cleaver, chopper\": 499,\n",
            "    \"cliff dwelling\": 500,\n",
            "    \"cliff, drop, drop-off\": 972,\n",
            "    \"cloak\": 501,\n",
            "    \"clog, geta, patten, sabot\": 502,\n",
            "    \"clumber, clumber spaniel\": 216,\n",
            "    \"cock\": 7,\n",
            "    \"cocker spaniel, English cocker spaniel, cocker\": 219,\n",
            "    \"cockroach, roach\": 314,\n",
            "    \"cocktail shaker\": 503,\n",
            "    \"coffee mug\": 504,\n",
            "    \"coffeepot\": 505,\n",
            "    \"coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch\": 391,\n",
            "    \"coil, spiral, volute, whorl, helix\": 506,\n",
            "    \"collie\": 231,\n",
            "    \"colobus, colobus monkey\": 375,\n",
            "    \"combination lock\": 507,\n",
            "    \"comic book\": 917,\n",
            "    \"common iguana, iguana, Iguana iguana\": 39,\n",
            "    \"common newt, Triturus vulgaris\": 26,\n",
            "    \"computer keyboard, keypad\": 508,\n",
            "    \"conch\": 112,\n",
            "    \"confectionery, confectionary, candy store\": 509,\n",
            "    \"consomme\": 925,\n",
            "    \"container ship, containership, container vessel\": 510,\n",
            "    \"convertible\": 511,\n",
            "    \"coral fungus\": 991,\n",
            "    \"coral reef\": 973,\n",
            "    \"corkscrew, bottle screw\": 512,\n",
            "    \"corn\": 987,\n",
            "    \"cornet, horn, trumpet, trump\": 513,\n",
            "    \"coucal\": 91,\n",
            "    \"cougar, puma, catamount, mountain lion, painter, panther, Felis concolor\": 286,\n",
            "    \"cowboy boot\": 514,\n",
            "    \"cowboy hat, ten-gallon hat\": 515,\n",
            "    \"coyote, prairie wolf, brush wolf, Canis latrans\": 272,\n",
            "    \"cradle\": 516,\n",
            "    \"crane\": 517,\n",
            "    \"crash helmet\": 518,\n",
            "    \"crate\": 519,\n",
            "    \"crayfish, crawfish, crawdad, crawdaddy\": 124,\n",
            "    \"crib, cot\": 520,\n",
            "    \"cricket\": 312,\n",
            "    \"croquet ball\": 522,\n",
            "    \"crossword puzzle, crossword\": 918,\n",
            "    \"crutch\": 523,\n",
            "    \"cucumber, cuke\": 943,\n",
            "    \"cuirass\": 524,\n",
            "    \"cup\": 968,\n",
            "    \"curly-coated retriever\": 206,\n",
            "    \"custard apple\": 956,\n",
            "    \"daisy\": 985,\n",
            "    \"dalmatian, coach dog, carriage dog\": 251,\n",
            "    \"dam, dike, dyke\": 525,\n",
            "    \"damselfly\": 320,\n",
            "    \"desk\": 526,\n",
            "    \"desktop computer\": 527,\n",
            "    \"dhole, Cuon alpinus\": 274,\n",
            "    \"dial telephone, dial phone\": 528,\n",
            "    \"diamondback, diamondback rattlesnake, Crotalus adamanteus\": 67,\n",
            "    \"diaper, nappy, napkin\": 529,\n",
            "    \"digital clock\": 530,\n",
            "    \"digital watch\": 531,\n",
            "    \"dingo, warrigal, warragal, Canis dingo\": 273,\n",
            "    \"dining table, board\": 532,\n",
            "    \"dishrag, dishcloth\": 533,\n",
            "    \"dishwasher, dish washer, dishwashing machine\": 534,\n",
            "    \"disk brake, disc brake\": 535,\n",
            "    \"dock, dockage, docking facility\": 536,\n",
            "    \"dogsled, dog sled, dog sleigh\": 537,\n",
            "    \"dome\": 538,\n",
            "    \"doormat, welcome mat\": 539,\n",
            "    \"dough\": 961,\n",
            "    \"dowitcher\": 142,\n",
            "    \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\": 319,\n",
            "    \"drake\": 97,\n",
            "    \"drilling platform, offshore rig\": 540,\n",
            "    \"drum, membranophone, tympan\": 541,\n",
            "    \"drumstick\": 542,\n",
            "    \"dugong, Dugong dugon\": 149,\n",
            "    \"dumbbell\": 543,\n",
            "    \"dung beetle\": 305,\n",
            "    \"ear, spike, capitulum\": 998,\n",
            "    \"earthstar\": 995,\n",
            "    \"echidna, spiny anteater, anteater\": 102,\n",
            "    \"eel\": 390,\n",
            "    \"eft\": 27,\n",
            "    \"eggnog\": 969,\n",
            "    \"electric fan, blower\": 545,\n",
            "    \"electric guitar\": 546,\n",
            "    \"electric locomotive\": 547,\n",
            "    \"electric ray, crampfish, numbfish, torpedo\": 5,\n",
            "    \"entertainment center\": 548,\n",
            "    \"envelope\": 549,\n",
            "    \"espresso\": 967,\n",
            "    \"espresso maker\": 550,\n",
            "    \"face powder\": 551,\n",
            "    \"feather boa, boa\": 552,\n",
            "    \"fiddler crab\": 120,\n",
            "    \"fig\": 952,\n",
            "    \"file, file cabinet, filing cabinet\": 553,\n",
            "    \"fire engine, fire truck\": 555,\n",
            "    \"fire screen, fireguard\": 556,\n",
            "    \"fireboat\": 554,\n",
            "    \"flagpole, flagstaff\": 557,\n",
            "    \"flamingo\": 130,\n",
            "    \"flat-coated retriever\": 205,\n",
            "    \"flatworm, platyhelminth\": 110,\n",
            "    \"flute, transverse flute\": 558,\n",
            "    \"fly\": 308,\n",
            "    \"folding chair\": 559,\n",
            "    \"football helmet\": 560,\n",
            "    \"forklift\": 561,\n",
            "    \"fountain\": 562,\n",
            "    \"fountain pen\": 563,\n",
            "    \"four-poster\": 564,\n",
            "    \"fox squirrel, eastern fox squirrel, Sciurus niger\": 335,\n",
            "    \"freight car\": 565,\n",
            "    \"frilled lizard, Chlamydosaurus kingi\": 43,\n",
            "    \"frying pan, frypan, skillet\": 567,\n",
            "    \"fur coat\": 568,\n",
            "    \"gar, garfish, garpike, billfish, Lepisosteus osseus\": 395,\n",
            "    \"garbage truck, dustcart\": 569,\n",
            "    \"garden spider, Aranea diademata\": 74,\n",
            "    \"garter snake, grass snake\": 57,\n",
            "    \"gas pump, gasoline pump, petrol pump, island dispenser\": 571,\n",
            "    \"gasmask, respirator, gas helmet\": 570,\n",
            "    \"gazelle\": 353,\n",
            "    \"geyser\": 974,\n",
            "    \"giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca\": 388,\n",
            "    \"giant schnauzer\": 197,\n",
            "    \"gibbon, Hylobates lar\": 368,\n",
            "    \"go-kart\": 573,\n",
            "    \"goblet\": 572,\n",
            "    \"golden retriever\": 207,\n",
            "    \"goldfinch, Carduelis carduelis\": 11,\n",
            "    \"goldfish, Carassius auratus\": 1,\n",
            "    \"golf ball\": 574,\n",
            "    \"golfcart, golf cart\": 575,\n",
            "    \"gondola\": 576,\n",
            "    \"gong, tam-tam\": 577,\n",
            "    \"goose\": 99,\n",
            "    \"gorilla, Gorilla gorilla\": 366,\n",
            "    \"gown\": 578,\n",
            "    \"grand piano, grand\": 579,\n",
            "    \"grasshopper, hopper\": 311,\n",
            "    \"great grey owl, great gray owl, Strix nebulosa\": 24,\n",
            "    \"great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias\": 2,\n",
            "    \"green lizard, Lacerta viridis\": 46,\n",
            "    \"green mamba\": 64,\n",
            "    \"green snake, grass snake\": 55,\n",
            "    \"greenhouse, nursery, glasshouse\": 580,\n",
            "    \"grey fox, gray fox, Urocyon cinereoargenteus\": 280,\n",
            "    \"grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus\": 147,\n",
            "    \"grille, radiator grille\": 581,\n",
            "    \"grocery store, grocery, food market, market\": 582,\n",
            "    \"groenendael\": 224,\n",
            "    \"groom, bridegroom\": 982,\n",
            "    \"ground beetle, carabid beetle\": 302,\n",
            "    \"guacamole\": 924,\n",
            "    \"guenon, guenon monkey\": 370,\n",
            "    \"guillotine\": 583,\n",
            "    \"guinea pig, Cavia cobaya\": 338,\n",
            "    \"gyromitra\": 993,\n",
            "    \"hair slide\": 584,\n",
            "    \"hair spray\": 585,\n",
            "    \"half track\": 586,\n",
            "    \"hammer\": 587,\n",
            "    \"hammerhead, hammerhead shark\": 4,\n",
            "    \"hamper\": 588,\n",
            "    \"hamster\": 333,\n",
            "    \"hand blower, blow dryer, blow drier, hair dryer, hair drier\": 589,\n",
            "    \"hand-held computer, hand-held microcomputer\": 590,\n",
            "    \"handkerchief, hankie, hanky, hankey\": 591,\n",
            "    \"hard disc, hard disk, fixed disk\": 592,\n",
            "    \"hare\": 331,\n",
            "    \"harmonica, mouth organ, harp, mouth harp\": 593,\n",
            "    \"harp\": 594,\n",
            "    \"hartebeest\": 351,\n",
            "    \"harvester, reaper\": 595,\n",
            "    \"harvestman, daddy longlegs, Phalangium opilio\": 70,\n",
            "    \"hatchet\": 596,\n",
            "    \"hay\": 958,\n",
            "    \"head cabbage\": 936,\n",
            "    \"hen\": 8,\n",
            "    \"hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa\": 996,\n",
            "    \"hermit crab\": 125,\n",
            "    \"hip, rose hip, rosehip\": 989,\n",
            "    \"hippopotamus, hippo, river horse, Hippopotamus amphibius\": 344,\n",
            "    \"hog, pig, grunter, squealer, Sus scrofa\": 341,\n",
            "    \"hognose snake, puff adder, sand viper\": 54,\n",
            "    \"holster\": 597,\n",
            "    \"home theater, home theatre\": 598,\n",
            "    \"honeycomb\": 599,\n",
            "    \"hook, claw\": 600,\n",
            "    \"hoopskirt, crinoline\": 601,\n",
            "    \"horizontal bar, high bar\": 602,\n",
            "    \"hornbill\": 93,\n",
            "    \"horned viper, cerastes, sand viper, horned asp, Cerastes cornutus\": 66,\n",
            "    \"horse cart, horse-cart\": 603,\n",
            "    \"hot pot, hotpot\": 926,\n",
            "    \"hotdog, hot dog, red hot\": 934,\n",
            "    \"hourglass\": 604,\n",
            "    \"house finch, linnet, Carpodacus mexicanus\": 12,\n",
            "    \"howler monkey, howler\": 379,\n",
            "    \"hummingbird\": 94,\n",
            "    \"hyena, hyaena\": 276,\n",
            "    \"iPod\": 605,\n",
            "    \"ibex, Capra ibex\": 350,\n",
            "    \"ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus\": 296,\n",
            "    \"ice cream, icecream\": 928,\n",
            "    \"ice lolly, lolly, lollipop, popsicle\": 929,\n",
            "    \"impala, Aepyceros melampus\": 352,\n",
            "    \"indigo bunting, indigo finch, indigo bird, Passerina cyanea\": 14,\n",
            "    \"indri, indris, Indri indri, Indri brevicaudatus\": 384,\n",
            "    \"iron, smoothing iron\": 606,\n",
            "    \"isopod\": 126,\n",
            "    \"jacamar\": 95,\n",
            "    \"jack-o'-lantern\": 607,\n",
            "    \"jackfruit, jak, jack\": 955,\n",
            "    \"jaguar, panther, Panthera onca, Felis onca\": 290,\n",
            "    \"jay\": 17,\n",
            "    \"jean, blue jean, denim\": 608,\n",
            "    \"jeep, landrover\": 609,\n",
            "    \"jellyfish\": 107,\n",
            "    \"jersey, T-shirt, tee shirt\": 610,\n",
            "    \"jigsaw puzzle\": 611,\n",
            "    \"jinrikisha, ricksha, rickshaw\": 612,\n",
            "    \"joystick\": 613,\n",
            "    \"junco, snowbird\": 13,\n",
            "    \"keeshond\": 261,\n",
            "    \"kelpie\": 227,\n",
            "    \"killer whale, killer, orca, grampus, sea wolf, Orcinus orca\": 148,\n",
            "    \"kimono\": 614,\n",
            "    \"king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica\": 121,\n",
            "    \"king penguin, Aptenodytes patagonica\": 145,\n",
            "    \"king snake, kingsnake\": 56,\n",
            "    \"kit fox, Vulpes macrotis\": 278,\n",
            "    \"kite\": 21,\n",
            "    \"knee pad\": 615,\n",
            "    \"knot\": 616,\n",
            "    \"koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus\": 105,\n",
            "    \"komondor\": 228,\n",
            "    \"kuvasz\": 222,\n",
            "    \"lab coat, laboratory coat\": 617,\n",
            "    \"lacewing, lacewing fly\": 318,\n",
            "    \"ladle\": 618,\n",
            "    \"ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle\": 301,\n",
            "    \"lakeside, lakeshore\": 975,\n",
            "    \"lampshade, lamp shade\": 619,\n",
            "    \"langur\": 374,\n",
            "    \"laptop, laptop computer\": 620,\n",
            "    \"lawn mower, mower\": 621,\n",
            "    \"leaf beetle, chrysomelid\": 304,\n",
            "    \"leafhopper\": 317,\n",
            "    \"leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea\": 34,\n",
            "    \"lemon\": 951,\n",
            "    \"lens cap, lens cover\": 622,\n",
            "    \"leopard, Panthera pardus\": 288,\n",
            "    \"lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens\": 387,\n",
            "    \"letter opener, paper knife, paperknife\": 623,\n",
            "    \"library\": 624,\n",
            "    \"lifeboat\": 625,\n",
            "    \"lighter, light, igniter, ignitor\": 626,\n",
            "    \"limousine, limo\": 627,\n",
            "    \"limpkin, Aramus pictus\": 135,\n",
            "    \"liner, ocean liner\": 628,\n",
            "    \"lion, king of beasts, Panthera leo\": 291,\n",
            "    \"lionfish\": 396,\n",
            "    \"lipstick, lip rouge\": 629,\n",
            "    \"little blue heron, Egretta caerulea\": 131,\n",
            "    \"llama\": 355,\n",
            "    \"loggerhead, loggerhead turtle, Caretta caretta\": 33,\n",
            "    \"long-horned beetle, longicorn, longicorn beetle\": 303,\n",
            "    \"lorikeet\": 90,\n",
            "    \"lotion\": 631,\n",
            "    \"loudspeaker, speaker, speaker unit, loudspeaker system, speaker system\": 632,\n",
            "    \"loupe, jeweler's loupe\": 633,\n",
            "    \"lumbermill, sawmill\": 634,\n",
            "    \"lycaenid, lycaenid butterfly\": 326,\n",
            "    \"lynx, catamount\": 287,\n",
            "    \"macaque\": 373,\n",
            "    \"macaw\": 88,\n",
            "    \"magnetic compass\": 635,\n",
            "    \"magpie\": 18,\n",
            "    \"mailbag, postbag\": 636,\n",
            "    \"mailbox, letter box\": 637,\n",
            "    \"maillot\": 638,\n",
            "    \"maillot, tank suit\": 639,\n",
            "    \"malamute, malemute, Alaskan malamute\": 249,\n",
            "    \"malinois\": 225,\n",
            "    \"manhole cover\": 640,\n",
            "    \"mantis, mantid\": 315,\n",
            "    \"maraca\": 641,\n",
            "    \"marimba, xylophone\": 642,\n",
            "    \"marmoset\": 377,\n",
            "    \"marmot\": 336,\n",
            "    \"mashed potato\": 935,\n",
            "    \"mask\": 643,\n",
            "    \"matchstick\": 644,\n",
            "    \"maypole\": 645,\n",
            "    \"maze, labyrinth\": 646,\n",
            "    \"measuring cup\": 647,\n",
            "    \"meat loaf, meatloaf\": 962,\n",
            "    \"medicine chest, medicine cabinet\": 648,\n",
            "    \"meerkat, mierkat\": 299,\n",
            "    \"megalith, megalithic structure\": 649,\n",
            "    \"menu\": 922,\n",
            "    \"microphone, mike\": 650,\n",
            "    \"microwave, microwave oven\": 651,\n",
            "    \"military uniform\": 652,\n",
            "    \"milk can\": 653,\n",
            "    \"miniature pinscher\": 237,\n",
            "    \"miniature poodle\": 266,\n",
            "    \"miniature schnauzer\": 196,\n",
            "    \"minibus\": 654,\n",
            "    \"miniskirt, mini\": 655,\n",
            "    \"minivan\": 656,\n",
            "    \"mink\": 357,\n",
            "    \"missile\": 657,\n",
            "    \"mitten\": 658,\n",
            "    \"mixing bowl\": 659,\n",
            "    \"mobile home, manufactured home\": 660,\n",
            "    \"modem\": 662,\n",
            "    \"monarch, monarch butterfly, milkweed butterfly, Danaus plexippus\": 323,\n",
            "    \"monastery\": 663,\n",
            "    \"mongoose\": 298,\n",
            "    \"monitor\": 664,\n",
            "    \"moped\": 665,\n",
            "    \"mortar\": 666,\n",
            "    \"mortarboard\": 667,\n",
            "    \"mosque\": 668,\n",
            "    \"mosquito net\": 669,\n",
            "    \"motor scooter, scooter\": 670,\n",
            "    \"mountain bike, all-terrain bike, off-roader\": 671,\n",
            "    \"mountain tent\": 672,\n",
            "    \"mouse, computer mouse\": 673,\n",
            "    \"mousetrap\": 674,\n",
            "    \"moving van\": 675,\n",
            "    \"mud turtle\": 35,\n",
            "    \"mushroom\": 947,\n",
            "    \"muzzle\": 676,\n",
            "    \"nail\": 677,\n",
            "    \"neck brace\": 678,\n",
            "    \"necklace\": 679,\n",
            "    \"nematode, nematode worm, roundworm\": 111,\n",
            "    \"night snake, Hypsiglena torquata\": 60,\n",
            "    \"nipple\": 680,\n",
            "    \"notebook, notebook computer\": 681,\n",
            "    \"obelisk\": 682,\n",
            "    \"oboe, hautboy, hautbois\": 683,\n",
            "    \"ocarina, sweet potato\": 684,\n",
            "    \"odometer, hodometer, mileometer, milometer\": 685,\n",
            "    \"oil filter\": 686,\n",
            "    \"orange\": 950,\n",
            "    \"orangutan, orang, orangutang, Pongo pygmaeus\": 365,\n",
            "    \"organ, pipe organ\": 687,\n",
            "    \"oscilloscope, scope, cathode-ray oscilloscope, CRO\": 688,\n",
            "    \"ostrich, Struthio camelus\": 9,\n",
            "    \"otter\": 360,\n",
            "    \"otterhound, otter hound\": 175,\n",
            "    \"overskirt\": 689,\n",
            "    \"ox\": 345,\n",
            "    \"oxcart\": 690,\n",
            "    \"oxygen mask\": 691,\n",
            "    \"oystercatcher, oyster catcher\": 143,\n",
            "    \"packet\": 692,\n",
            "    \"paddle, boat paddle\": 693,\n",
            "    \"paddlewheel, paddle wheel\": 694,\n",
            "    \"padlock\": 695,\n",
            "    \"paintbrush\": 696,\n",
            "    \"pajama, pyjama, pj's, jammies\": 697,\n",
            "    \"palace\": 698,\n",
            "    \"panpipe, pandean pipe, syrinx\": 699,\n",
            "    \"paper towel\": 700,\n",
            "    \"papillon\": 157,\n",
            "    \"parachute, chute\": 701,\n",
            "    \"parallel bars, bars\": 702,\n",
            "    \"park bench\": 703,\n",
            "    \"parking meter\": 704,\n",
            "    \"partridge\": 86,\n",
            "    \"passenger car, coach, carriage\": 705,\n",
            "    \"patas, hussar monkey, Erythrocebus patas\": 371,\n",
            "    \"patio, terrace\": 706,\n",
            "    \"pay-phone, pay-station\": 707,\n",
            "    \"peacock\": 84,\n",
            "    \"pedestal, plinth, footstall\": 708,\n",
            "    \"pelican\": 144,\n",
            "    \"pencil box, pencil case\": 709,\n",
            "    \"pencil sharpener\": 710,\n",
            "    \"perfume, essence\": 711,\n",
            "    \"photocopier\": 713,\n",
            "    \"pick, plectrum, plectron\": 714,\n",
            "    \"pickelhaube\": 715,\n",
            "    \"picket fence, paling\": 716,\n",
            "    \"pickup, pickup truck\": 717,\n",
            "    \"pier\": 718,\n",
            "    \"piggy bank, penny bank\": 719,\n",
            "    \"pill bottle\": 720,\n",
            "    \"pillow\": 721,\n",
            "    \"pineapple, ananas\": 953,\n",
            "    \"ping-pong ball\": 722,\n",
            "    \"pinwheel\": 723,\n",
            "    \"pirate, pirate ship\": 724,\n",
            "    \"pitcher, ewer\": 725,\n",
            "    \"pizza, pizza pie\": 963,\n",
            "    \"plane, carpenter's plane, woodworking plane\": 726,\n",
            "    \"planetarium\": 727,\n",
            "    \"plastic bag\": 728,\n",
            "    \"plate\": 923,\n",
            "    \"plate rack\": 729,\n",
            "    \"platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus\": 103,\n",
            "    \"plow, plough\": 730,\n",
            "    \"plunger, plumber's helper\": 731,\n",
            "    \"pole\": 733,\n",
            "    \"polecat, fitch, foulmart, foumart, Mustela putorius\": 358,\n",
            "    \"police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria\": 734,\n",
            "    \"pomegranate\": 957,\n",
            "    \"poncho\": 735,\n",
            "    \"pool table, billiard table, snooker table\": 736,\n",
            "    \"pop bottle, soda bottle\": 737,\n",
            "    \"porcupine, hedgehog\": 334,\n",
            "    \"pot, flowerpot\": 738,\n",
            "    \"potpie\": 964,\n",
            "    \"potter's wheel\": 739,\n",
            "    \"power drill\": 740,\n",
            "    \"prairie chicken, prairie grouse, prairie fowl\": 83,\n",
            "    \"prayer rug, prayer mat\": 741,\n",
            "    \"pretzel\": 932,\n",
            "    \"printer\": 742,\n",
            "    \"prison, prison house\": 743,\n",
            "    \"proboscis monkey, Nasalis larvatus\": 376,\n",
            "    \"projectile, missile\": 744,\n",
            "    \"projector\": 745,\n",
            "    \"promontory, headland, head, foreland\": 976,\n",
            "    \"ptarmigan\": 81,\n",
            "    \"puck, hockey puck\": 746,\n",
            "    \"puffer, pufferfish, blowfish, globefish\": 397,\n",
            "    \"pug, pug-dog\": 254,\n",
            "    \"punching bag, punch bag, punching ball, punchball\": 747,\n",
            "    \"purse\": 748,\n",
            "    \"quail\": 85,\n",
            "    \"quill, quill pen\": 749,\n",
            "    \"quilt, comforter, comfort, puff\": 750,\n",
            "    \"racer, race car, racing car\": 751,\n",
            "    \"racket, racquet\": 752,\n",
            "    \"radiator\": 753,\n",
            "    \"radio telescope, radio reflector\": 755,\n",
            "    \"radio, wireless\": 754,\n",
            "    \"rain barrel\": 756,\n",
            "    \"ram, tup\": 348,\n",
            "    \"rapeseed\": 984,\n",
            "    \"recreational vehicle, RV, R.V.\": 757,\n",
            "    \"red fox, Vulpes vulpes\": 277,\n",
            "    \"red wine\": 966,\n",
            "    \"red wolf, maned wolf, Canis rufus, Canis niger\": 271,\n",
            "    \"red-backed sandpiper, dunlin, Erolia alpina\": 140,\n",
            "    \"red-breasted merganser, Mergus serrator\": 98,\n",
            "    \"redbone\": 168,\n",
            "    \"redshank, Tringa totanus\": 141,\n",
            "    \"reel\": 758,\n",
            "    \"reflex camera\": 759,\n",
            "    \"refrigerator, icebox\": 760,\n",
            "    \"remote control, remote\": 761,\n",
            "    \"restaurant, eating house, eating place, eatery\": 762,\n",
            "    \"revolver, six-gun, six-shooter\": 763,\n",
            "    \"rhinoceros beetle\": 306,\n",
            "    \"rifle\": 764,\n",
            "    \"ringlet, ringlet butterfly\": 322,\n",
            "    \"ringneck snake, ring-necked snake, ring snake\": 53,\n",
            "    \"robin, American robin, Turdus migratorius\": 15,\n",
            "    \"rock beauty, Holocanthus tricolor\": 392,\n",
            "    \"rock crab, Cancer irroratus\": 119,\n",
            "    \"rock python, rock snake, Python sebae\": 62,\n",
            "    \"rocking chair, rocker\": 765,\n",
            "    \"rotisserie\": 766,\n",
            "    \"rubber eraser, rubber, pencil eraser\": 767,\n",
            "    \"ruddy turnstone, Arenaria interpres\": 139,\n",
            "    \"ruffed grouse, partridge, Bonasa umbellus\": 82,\n",
            "    \"rugby ball\": 768,\n",
            "    \"rule, ruler\": 769,\n",
            "    \"running shoe\": 770,\n",
            "    \"safe\": 771,\n",
            "    \"safety pin\": 772,\n",
            "    \"saltshaker, salt shaker\": 773,\n",
            "    \"sandal\": 774,\n",
            "    \"sandbar, sand bar\": 977,\n",
            "    \"sarong\": 775,\n",
            "    \"sax, saxophone\": 776,\n",
            "    \"scabbard\": 777,\n",
            "    \"scale, weighing machine\": 778,\n",
            "    \"schipperke\": 223,\n",
            "    \"school bus\": 779,\n",
            "    \"schooner\": 780,\n",
            "    \"scoreboard\": 781,\n",
            "    \"scorpion\": 71,\n",
            "    \"screen, CRT screen\": 782,\n",
            "    \"screw\": 783,\n",
            "    \"screwdriver\": 784,\n",
            "    \"scuba diver\": 983,\n",
            "    \"sea anemone, anemone\": 108,\n",
            "    \"sea cucumber, holothurian\": 329,\n",
            "    \"sea lion\": 150,\n",
            "    \"sea slug, nudibranch\": 115,\n",
            "    \"sea snake\": 65,\n",
            "    \"sea urchin\": 328,\n",
            "    \"seashore, coast, seacoast, sea-coast\": 978,\n",
            "    \"seat belt, seatbelt\": 785,\n",
            "    \"sewing machine\": 786,\n",
            "    \"shield, buckler\": 787,\n",
            "    \"shoe shop, shoe-shop, shoe store\": 788,\n",
            "    \"shoji\": 789,\n",
            "    \"shopping basket\": 790,\n",
            "    \"shopping cart\": 791,\n",
            "    \"shovel\": 792,\n",
            "    \"shower cap\": 793,\n",
            "    \"shower curtain\": 794,\n",
            "    \"siamang, Hylobates syndactylus, Symphalangus syndactylus\": 369,\n",
            "    \"sidewinder, horned rattlesnake, Crotalus cerastes\": 68,\n",
            "    \"silky terrier, Sydney silky\": 201,\n",
            "    \"ski\": 795,\n",
            "    \"ski mask\": 796,\n",
            "    \"skunk, polecat, wood pussy\": 361,\n",
            "    \"sleeping bag\": 797,\n",
            "    \"slide rule, slipstick\": 798,\n",
            "    \"sliding door\": 799,\n",
            "    \"slot, one-armed bandit\": 800,\n",
            "    \"sloth bear, Melursus ursinus, Ursus ursinus\": 297,\n",
            "    \"slug\": 114,\n",
            "    \"snail\": 113,\n",
            "    \"snorkel\": 801,\n",
            "    \"snow leopard, ounce, Panthera uncia\": 289,\n",
            "    \"snowmobile\": 802,\n",
            "    \"snowplow, snowplough\": 803,\n",
            "    \"soap dispenser\": 804,\n",
            "    \"soccer ball\": 805,\n",
            "    \"sock\": 806,\n",
            "    \"soft-coated wheaten terrier\": 202,\n",
            "    \"solar dish, solar collector, solar furnace\": 807,\n",
            "    \"sombrero\": 808,\n",
            "    \"sorrel\": 339,\n",
            "    \"soup bowl\": 809,\n",
            "    \"space bar\": 810,\n",
            "    \"space heater\": 811,\n",
            "    \"space shuttle\": 812,\n",
            "    \"spaghetti squash\": 940,\n",
            "    \"spatula\": 813,\n",
            "    \"speedboat\": 814,\n",
            "    \"spider monkey, Ateles geoffroyi\": 381,\n",
            "    \"spider web, spider's web\": 815,\n",
            "    \"spindle\": 816,\n",
            "    \"spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish\": 123,\n",
            "    \"spoonbill\": 129,\n",
            "    \"sports car, sport car\": 817,\n",
            "    \"spotlight, spot\": 818,\n",
            "    \"spotted salamander, Ambystoma maculatum\": 28,\n",
            "    \"squirrel monkey, Saimiri sciureus\": 382,\n",
            "    \"stage\": 819,\n",
            "    \"standard poodle\": 267,\n",
            "    \"standard schnauzer\": 198,\n",
            "    \"starfish, sea star\": 327,\n",
            "    \"steam locomotive\": 820,\n",
            "    \"steel arch bridge\": 821,\n",
            "    \"steel drum\": 822,\n",
            "    \"stethoscope\": 823,\n",
            "    \"stingray\": 6,\n",
            "    \"stinkhorn, carrion fungus\": 994,\n",
            "    \"stole\": 824,\n",
            "    \"stone wall\": 825,\n",
            "    \"stopwatch, stop watch\": 826,\n",
            "    \"stove\": 827,\n",
            "    \"strainer\": 828,\n",
            "    \"strawberry\": 949,\n",
            "    \"street sign\": 919,\n",
            "    \"streetcar, tram, tramcar, trolley, trolley car\": 829,\n",
            "    \"stretcher\": 830,\n",
            "    \"studio couch, day bed\": 831,\n",
            "    \"stupa, tope\": 832,\n",
            "    \"sturgeon\": 394,\n",
            "    \"submarine, pigboat, sub, U-boat\": 833,\n",
            "    \"suit, suit of clothes\": 834,\n",
            "    \"sulphur butterfly, sulfur butterfly\": 325,\n",
            "    \"sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita\": 89,\n",
            "    \"sundial\": 835,\n",
            "    \"sunglass\": 836,\n",
            "    \"sunglasses, dark glasses, shades\": 837,\n",
            "    \"sunscreen, sunblock, sun blocker\": 838,\n",
            "    \"suspension bridge\": 839,\n",
            "    \"swab, swob, mop\": 840,\n",
            "    \"sweatshirt\": 841,\n",
            "    \"swimming trunks, bathing trunks\": 842,\n",
            "    \"swing\": 843,\n",
            "    \"switch, electric switch, electrical switch\": 844,\n",
            "    \"syringe\": 845,\n",
            "    \"tabby, tabby cat\": 281,\n",
            "    \"table lamp\": 846,\n",
            "    \"tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui\": 32,\n",
            "    \"tank, army tank, armored combat vehicle, armoured combat vehicle\": 847,\n",
            "    \"tape player\": 848,\n",
            "    \"tarantula\": 76,\n",
            "    \"teapot\": 849,\n",
            "    \"teddy, teddy bear\": 850,\n",
            "    \"television, television system\": 851,\n",
            "    \"tench, Tinca tinca\": 0,\n",
            "    \"tennis ball\": 852,\n",
            "    \"terrapin\": 36,\n",
            "    \"thatch, thatched roof\": 853,\n",
            "    \"theater curtain, theatre curtain\": 854,\n",
            "    \"thimble\": 855,\n",
            "    \"three-toed sloth, ai, Bradypus tridactylus\": 364,\n",
            "    \"thresher, thrasher, threshing machine\": 856,\n",
            "    \"throne\": 857,\n",
            "    \"thunder snake, worm snake, Carphophis amoenus\": 52,\n",
            "    \"tick\": 78,\n",
            "    \"tiger beetle\": 300,\n",
            "    \"tiger cat\": 282,\n",
            "    \"tiger shark, Galeocerdo cuvieri\": 3,\n",
            "    \"tiger, Panthera tigris\": 292,\n",
            "    \"tile roof\": 858,\n",
            "    \"timber wolf, grey wolf, gray wolf, Canis lupus\": 269,\n",
            "    \"titi, titi monkey\": 380,\n",
            "    \"toaster\": 859,\n",
            "    \"tobacco shop, tobacconist shop, tobacconist\": 860,\n",
            "    \"toilet seat\": 861,\n",
            "    \"toilet tissue, toilet paper, bathroom tissue\": 999,\n",
            "    \"torch\": 862,\n",
            "    \"totem pole\": 863,\n",
            "    \"toucan\": 96,\n",
            "    \"tow truck, tow car, wrecker\": 864,\n",
            "    \"toy poodle\": 265,\n",
            "    \"toy terrier\": 158,\n",
            "    \"toyshop\": 865,\n",
            "    \"tractor\": 866,\n",
            "    \"traffic light, traffic signal, stoplight\": 920,\n",
            "    \"trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi\": 867,\n",
            "    \"tray\": 868,\n",
            "    \"tree frog, tree-frog\": 31,\n",
            "    \"trench coat\": 869,\n",
            "    \"triceratops\": 51,\n",
            "    \"tricycle, trike, velocipede\": 870,\n",
            "    \"trifle\": 927,\n",
            "    \"trilobite\": 69,\n",
            "    \"trimaran\": 871,\n",
            "    \"tripod\": 872,\n",
            "    \"triumphal arch\": 873,\n",
            "    \"trolleybus, trolley coach, trackless trolley\": 874,\n",
            "    \"trombone\": 875,\n",
            "    \"tub, vat\": 876,\n",
            "    \"turnstile\": 877,\n",
            "    \"tusker\": 101,\n",
            "    \"typewriter keyboard\": 878,\n",
            "    \"umbrella\": 879,\n",
            "    \"unicycle, monocycle\": 880,\n",
            "    \"upright, upright piano\": 881,\n",
            "    \"vacuum, vacuum cleaner\": 882,\n",
            "    \"valley, vale\": 979,\n",
            "    \"vase\": 883,\n",
            "    \"vault\": 884,\n",
            "    \"velvet\": 885,\n",
            "    \"vending machine\": 886,\n",
            "    \"vestment\": 887,\n",
            "    \"viaduct\": 888,\n",
            "    \"vine snake\": 59,\n",
            "    \"violin, fiddle\": 889,\n",
            "    \"vizsla, Hungarian pointer\": 211,\n",
            "    \"volcano\": 980,\n",
            "    \"volleyball\": 890,\n",
            "    \"vulture\": 23,\n",
            "    \"waffle iron\": 891,\n",
            "    \"walking stick, walkingstick, stick insect\": 313,\n",
            "    \"wall clock\": 892,\n",
            "    \"wallaby, brush kangaroo\": 104,\n",
            "    \"wallet, billfold, notecase, pocketbook\": 893,\n",
            "    \"wardrobe, closet, press\": 894,\n",
            "    \"warplane, military plane\": 895,\n",
            "    \"warthog\": 343,\n",
            "    \"washbasin, handbasin, washbowl, lavabo, wash-hand basin\": 896,\n",
            "    \"washer, automatic washer, washing machine\": 897,\n",
            "    \"water bottle\": 898,\n",
            "    \"water buffalo, water ox, Asiatic buffalo, Bubalus bubalis\": 346,\n",
            "    \"water jug\": 899,\n",
            "    \"water ouzel, dipper\": 20,\n",
            "    \"water snake\": 58,\n",
            "    \"water tower\": 900,\n",
            "    \"weasel\": 356,\n",
            "    \"web site, website, internet site, site\": 916,\n",
            "    \"weevil\": 307,\n",
            "    \"whippet\": 172,\n",
            "    \"whiptail, whiptail lizard\": 41,\n",
            "    \"whiskey jug\": 901,\n",
            "    \"whistle\": 902,\n",
            "    \"white stork, Ciconia ciconia\": 127,\n",
            "    \"white wolf, Arctic wolf, Canis lupus tundrarum\": 270,\n",
            "    \"wig\": 903,\n",
            "    \"wild boar, boar, Sus scrofa\": 342,\n",
            "    \"window screen\": 904,\n",
            "    \"window shade\": 905,\n",
            "    \"wine bottle\": 907,\n",
            "    \"wing\": 908,\n",
            "    \"wire-haired fox terrier\": 188,\n",
            "    \"wok\": 909,\n",
            "    \"wolf spider, hunting spider\": 77,\n",
            "    \"wombat\": 106,\n",
            "    \"wood rabbit, cottontail, cottontail rabbit\": 330,\n",
            "    \"wooden spoon\": 910,\n",
            "    \"wool, woolen, woollen\": 911,\n",
            "    \"worm fence, snake fence, snake-rail fence, Virginia fence\": 912,\n",
            "    \"wreck\": 913,\n",
            "    \"yawl\": 914,\n",
            "    \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\": 986,\n",
            "    \"yurt\": 915,\n",
            "    \"zebra\": 340,\n",
            "    \"zucchini, courgette\": 939\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"model_type\": \"vit\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_channels\": 3,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"patch_size\": 16,\n",
            "  \"pooler_act\": \"tanh\",\n",
            "  \"pooler_output_size\": 768,\n",
            "  \"qkv_bias\": true,\n",
            "  \"transformers_version\": \"4.55.0\"\n",
            "}\n",
            "\n",
            "[WARNING|image_processing_auto.py:338] 2025-08-06 00:55:43,395 >> Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "[INFO|image_processing_base.py:378] 2025-08-06 00:55:43,502 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--google--vit-base-patch16-224/snapshots/3f49326eb077187dfe1c2a2bb15fbd74e6ab91e3/preprocessor_config.json\n",
            "[INFO|image_processing_utils.py:241] 2025-08-06 00:55:43,503 >> size should be a dictionary on of the following set of keys: ({'width', 'height'}, {'shortest_edge'}, {'shortest_edge', 'longest_edge'}, {'longest_edge'}, {'max_height', 'max_width'}), got 224. Converted to {'height': 224, 'width': 224}.\n",
            "[INFO|image_processing_base.py:423] 2025-08-06 00:55:43,503 >> Image processor ViTImageProcessor {\n",
            "  \"do_convert_rgb\": null,\n",
            "  \"do_normalize\": true,\n",
            "  \"do_rescale\": true,\n",
            "  \"do_resize\": true,\n",
            "  \"image_mean\": [\n",
            "    0.5,\n",
            "    0.5,\n",
            "    0.5\n",
            "  ],\n",
            "  \"image_processor_type\": \"ViTImageProcessor\",\n",
            "  \"image_std\": [\n",
            "    0.5,\n",
            "    0.5,\n",
            "    0.5\n",
            "  ],\n",
            "  \"resample\": 2,\n",
            "  \"rescale_factor\": 0.00392156862745098,\n",
            "  \"size\": {\n",
            "    \"height\": 224,\n",
            "    \"width\": 224\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example chest X-ray image\n",
        "image_path = \"000001-1.jpg\"\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "# Convert image to patch embeddings\n",
        "inputs = image_processor(image, return_tensors=\"pt\").to(\"cuda\")\n",
        "with torch.no_grad():\n",
        "    vit_outputs = vit_encoder(**inputs).last_hidden_state[:, 1:, :]\n",
        "    patch_embeddings = projection(vit_outputs).squeeze(0)  # [196, 4096]\n",
        "\n",
        "# Add batch dimenspn\n",
        "patch_embeddings = patch_embeddings.unsqueeze(0)  # [1, 196, 4096]\n"
      ],
      "metadata": {
        "id": "25F5SMHZ4MrF"
      },
      "id": "25F5SMHZ4MrF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Is the heart enlarged?\"\n",
        "prompt = \"<image> \" + question.strip()\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "text_embeddings = model.get_input_embeddings()(input_ids)  # [1, T, 4096]\n"
      ],
      "metadata": {
        "id": "lVO33WOf4tli"
      },
      "id": "lVO33WOf4tli",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate image + text\n",
        "full_embeddings = torch.cat([patch_embeddings, text_embeddings], dim=1)\n",
        "\n",
        "# Attention mask (1s for all, assumes model handles vision vs text masking internally)\n",
        "attn_mask = torch.ones(full_embeddings.shape[:-1], dtype=torch.long).to(\"cuda\")\n"
      ],
      "metadata": {
        "id": "g97lQlK-4w5w"
      },
      "id": "g97lQlK-4w5w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    outputs = model(\n",
        "        inputs_embeds=full_embeddings,\n",
        "        attention_mask=attn_mask,\n",
        "        return_dict=True\n",
        "    )\n",
        "    logits = outputs.logits\n",
        "    predicted_ids = logits[:, -1, :].argmax(dim=-1)\n",
        "\n",
        "print(\"Predicted token:\", tokenizer.decode(predicted_ids))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "xdjDvcWU4y3n",
        "outputId": "ffeee663-0685-47d3-c1f2-ff0fa13a5e76"
      },
      "id": "xdjDvcWU4y3n",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'vision_placeholder_index'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3423317764.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     outputs = model(\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfull_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/VoRA/models/modeling_vora.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, **batch)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;31m# -------------- Vision/Text Embedding ----------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0mvision_placeholder_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vision_placeholder_index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"frames\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"n_frames\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mvision_encode_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_vision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'vision_placeholder_index'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"BioMistral/BioMistral-7B\")\n",
        "special_tokens = {\"additional_special_tokens\": [\"<image>\"]}\n",
        "tokenizer.add_special_tokens(special_tokens)\n",
        "\n",
        "# Load base BioMistral LLM\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"BioMistral/BioMistral-7B\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"cuda\"\n",
        ")\n",
        "\n",
        "base_model.get_input_embeddings = lambda: base_model.transformer.wte\n",
        "base_model.set_input_embeddings = lambda value: setattr(base_model.transformer, \"wte\", value)\n",
        "# Resize token embeddings to match tokenizer\n",
        "base_model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Load LoRA adapter\n",
        "adapter_path = \"output_vora_biomistral_vqa_rad\"  #\n",
        "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "\n",
        "# Merge LoRA weights into base\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Save merged model + tokenizer\n",
        "save_dir = \"output_vora_biomistral_vqa_\"\n",
        "model.save_pretrained(save_dir)\n",
        "tokenizer.save_pretrained(save_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ua9UgWTz5SC5",
        "outputId": "3ac199e9-6495-4cb6-d62e-e97785700ffe"
      },
      "id": "Ua9UgWTz5SC5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO|tokenization_utils_base.py:2067] 2025-08-06 01:10:53,534 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--BioMistral--BioMistral-7B/snapshots/9a11e1ffa817c211cbb52ee1fb312dc6b61b40a5/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2067] 2025-08-06 01:10:53,535 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--BioMistral--BioMistral-7B/snapshots/9a11e1ffa817c211cbb52ee1fb312dc6b61b40a5/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2067] 2025-08-06 01:10:53,536 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2067] 2025-08-06 01:10:53,536 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--BioMistral--BioMistral-7B/snapshots/9a11e1ffa817c211cbb52ee1fb312dc6b61b40a5/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2067] 2025-08-06 01:10:53,537 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--BioMistral--BioMistral-7B/snapshots/9a11e1ffa817c211cbb52ee1fb312dc6b61b40a5/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2067] 2025-08-06 01:10:53,537 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|configuration_utils.py:752] 2025-08-06 01:10:53,741 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--BioMistral--BioMistral-7B/snapshots/9a11e1ffa817c211cbb52ee1fb312dc6b61b40a5/config.json\n",
            "[INFO|configuration_utils.py:817] 2025-08-06 01:10:53,742 >> Model config MistralConfig {\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"head_dim\": null,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.55.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1308] 2025-08-06 01:10:53,840 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--BioMistral--BioMistral-7B/snapshots/9a11e1ffa817c211cbb52ee1fb312dc6b61b40a5/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:2411] 2025-08-06 01:10:53,842 >> Instantiating MistralForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:1098] 2025-08-06 01:10:53,843 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "[INFO|safetensors_conversion.py:61] 2025-08-06 01:10:53,938 >> Attempting to create safetensors variant\n",
            "[INFO|safetensors_conversion.py:24] 2025-08-06 01:10:54,164 >> Attempting to convert .bin model on the fly to safetensors.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 896.00 KiB is free. Process 248727 has 39.55 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 83.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2923797915.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Load base BioMistral LLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m base_model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;34m\"BioMistral/BioMistral-7B\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    601\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   5059\u001b[0m                 \u001b[0moffload_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5060\u001b[0m                 \u001b[0merror_msgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5061\u001b[0;31m             \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5062\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5063\u001b[0m                 \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[1;32m   5522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5523\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5524\u001b[0;31m                 \u001b[0m_error_msgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisk_offload_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcpu_offload_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_shard_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5525\u001b[0m                 \u001b[0merror_msgs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0m_error_msgs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload_shard_file\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    972\u001b[0m     \u001b[0;31m# Skip it with fsdp on ranks other than 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_fsdp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_local_dist_rank_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_quantized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m         disk_offload_index, cpu_offload_index = _load_state_dict_into_meta_model(\n\u001b[0m\u001b[1;32m    975\u001b[0m             \u001b[0mmodel_to_load\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\u001b[0m\n\u001b[1;32m    877\u001b[0m                     \u001b[0mparam_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_local_dist_rank_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"meta\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m                 \u001b[0m_load_parameter_into_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 896.00 KiB is free. Process 248727 has 39.55 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 83.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "from transformers import CLIPImageProcessor\n",
        "\n",
        "\n",
        "# Initialize image processor with exact config\n",
        "image_processor = CLIPImageProcessor.from_pretrained(\n",
        "    \"openai/clip-vit-base-patch16\",\n",
        "    size={\"height\": 448, \"width\": 448},  #\n",
        "    do_rescale=False,  # important for medical images\n",
        "    do_normalize=True\n",
        ")\n",
        "\n",
        "def prepare_vora_inputs(image_path, question, tokenizer, image_processor):\n",
        "    # Load and process image\n",
        "    image = Image.open(image_path).convert(\"RGB\").resize((448, 448))\n",
        "    image_inputs = image_processor([image], return_tensors=\"pt\")\n",
        "\n",
        "    # Process text  ensure exactly one <image> token\n",
        "    if question.count(\"<image>\") != 1:\n",
        "        raise ValueError(\"Question must contain exactly one <image> token\")\n",
        "\n",
        "    # Tokenize with proper padding\n",
        "    text_ids = tokenizer.encode(question, add_special_tokens=False)\n",
        "    required_padding = 266 - 256 - len(text_ids)  # Total 266 - vision 256 - text\n",
        "\n",
        "    input_ids = [text_ids + [tokenizer.pad_token_id] * required_padding]\n",
        "    attention_mask = [[1]*len(text_ids) + [0]*required_padding]\n",
        "\n",
        "    return {\n",
        "        \"frames\": image_inputs.pixel_values.to(model.device),\n",
        "        \"n_frames\": torch.tensor([1]).to(model.device),\n",
        "        \"input_ids\": torch.tensor(input_ids, dtype=torch.long).to(model.device),\n",
        "        \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long).to(model.device),\n",
        "        \"vision_placeholder_index\": torch.tensor([-200]).to(model.device),\n",
        "        \"vison_placeholder_mode\": torch.tensor([1]).to(model.device),  # Changed to 1\n",
        "        \"prompt\": [question],\n",
        "        \"gt\": [\"\"],  # Empty but required\n",
        "        \"question\": [question]\n",
        "    }\n",
        "\n",
        "# Prepare inputs\n",
        "inputs = prepare_vora_inputs(\n",
        "    image_path=\"000001-1.jpg\",\n",
        "    question=\"<image> What abnormality do you see?\",  # Must contain <image>\n",
        "    tokenizer=tokenizer,\n",
        "    image_processor=image_processor\n",
        ")\n",
        "\n",
        "# Verify before generation\n",
        "print(f\"Image tokens in input: {(inputs['input_ids'][0] == -200).sum().item()}\")  # Should be 1\n",
        "print(f\"Total sequence length: {inputs['input_ids'].shape[1]}\")  # Must be 266\n",
        "\n",
        "# Generate response\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        batch=inputs,\n",
        "        max_new_tokens=128,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        temperature=0.1,\n",
        "        do_sample=False\n",
        "    )\n",
        "\n",
        "# Decode output\n",
        "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"Model Answer:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sVthwsntVkib",
        "outputId": "f6016992-743a-46f7-b58e-15921639e7e9"
      },
      "id": "sVthwsntVkib",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO|image_processing_base.py:378] 2025-08-06 00:52:52,638 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--openai--clip-vit-base-patch16/snapshots/57c216476eefef5ab752ec549e440a49ae4ae5f3/preprocessor_config.json\n",
            "[INFO|image_processing_utils.py:241] 2025-08-06 00:52:52,639 >> crop_size should be a dictionary on of the following set of keys: ({'width', 'height'}, {'shortest_edge'}, {'shortest_edge', 'longest_edge'}, {'longest_edge'}, {'max_height', 'max_width'}), got 224. Converted to {'height': 224, 'width': 224}.\n",
            "[INFO|image_processing_base.py:423] 2025-08-06 00:52:52,639 >> Image processor CLIPImageProcessor {\n",
            "  \"crop_size\": {\n",
            "    \"height\": 224,\n",
            "    \"width\": 224\n",
            "  },\n",
            "  \"do_center_crop\": true,\n",
            "  \"do_convert_rgb\": true,\n",
            "  \"do_normalize\": true,\n",
            "  \"do_rescale\": false,\n",
            "  \"do_resize\": true,\n",
            "  \"image_mean\": [\n",
            "    0.48145466,\n",
            "    0.4578275,\n",
            "    0.40821073\n",
            "  ],\n",
            "  \"image_processor_type\": \"CLIPImageProcessor\",\n",
            "  \"image_std\": [\n",
            "    0.26862954,\n",
            "    0.26130258,\n",
            "    0.27577711\n",
            "  ],\n",
            "  \"resample\": 3,\n",
            "  \"rescale_factor\": 0.00392156862745098,\n",
            "  \"size\": {\n",
            "    \"height\": 448,\n",
            "    \"width\": 448\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image tokens in input: 0\n",
            "Total sequence length: 10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Size mismatch! cur_image_num: 0, len(cur_vision_embeds): 1 1                     in <image> What abnormality do you see? &  & tensor([32000, 28705,  1824,   534, 10767,  2045,   511,   368,  1032, 28804],\n       device='cuda:0')",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2651717152.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# Generate response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     output = model.generate(\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/VoRA/models/modeling_vora.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, batch, **generate_params)\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0mvision_encode_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_vision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             inputs_embeds, attention_mask, _, _ = self._concat_embedding(\n\u001b[0m\u001b[1;32m    280\u001b[0m                 vision_encode_out, batch, vision_placeholder_index, left_padding=False)\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/VoRA/models/modeling_vora.py\u001b[0m in \u001b[0;36m_concat_embedding\u001b[0;34m(self, vision_encode_out, batch, vision_placeholder_index, left_padding)\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0mcur_vision_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcur_vision_attn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mcur_vision_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcur_vision_targets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mcur_image_num\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_vision_embeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;31m\"\u001b[0m\u001b[0mSize\u001b[0m \u001b[0mmismatch\u001b[0m\u001b[0;31m!\u001b[0m \u001b[0mcur_image_num\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcur_image_num\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_vision_embeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_vision_embeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_vision_embeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                     \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prompt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcur_batch_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcur_batch_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcur_batch_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;31m\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Size mismatch! cur_image_num: 0, len(cur_vision_embeds): 1 1                     in <image> What abnormality do you see? &  & tensor([32000, 28705,  1824,   534, 10767,  2045,   511,   368,  1032, 28804],\n       device='cuda:0')"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Image tokens in question:\", \"What abnormality do you see?\".count(\"<image>\"))\n",
        "print(\"Vision placeholder index:\", inputs[\"vision_placeholder_index\"])\n",
        "print(\"Processed image shape:\", inputs[\"frames\"].shape)  #"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "JzKNOpP03i0o",
        "outputId": "13ba5483-d717-4e28-a02a-99211c7c8527"
      },
      "id": "JzKNOpP03i0o",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image tokens in question: 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'vision_placeholder_index'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3932554777.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Image tokens in question:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"What abnormality do you see?\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<image>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Vision placeholder index:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vision_placeholder_index\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Processed image shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"frames\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Should be [1, 3, 448, 448]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'vision_placeholder_index'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model device:\", next(model.parameters()).device)\n",
        "print(\"Model config:\", model.config.model_type)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wicmSjFV25cH",
        "outputId": "413903cc-abbc-4b6d-c793-2aea86d688b0"
      },
      "id": "wicmSjFV25cH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model device: cuda:0\n",
            "Model config: mistral\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Input IDs shape: {inputs['input_ids'].shape}\")  #\n",
        "print(f\"Active tokens: {inputs['attention_mask'].sum().item()}\")  #"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjzKjMqnv_YJ",
        "outputId": "ef2b4f37-1902-4333-849c-f2f009a27898"
      },
      "id": "ZjzKjMqnv_YJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs shape: torch.Size([1, 10])\n",
            "Active tokens: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input IDs:\", inputs['input_ids'])\n",
        "print(\"Attention Mask:\", inputs['attention_mask'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yMVZftYsb2P",
        "outputId": "506a9e93-ead6-4fb1-b247-2f601d251857"
      },
      "id": "4yMVZftYsb2P",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs: tensor([[ -200, 28705,  1824,   534, 10767,  2045,   511,   368,  1032, 28804]],\n",
            "       device='cuda:0')\n",
            "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model device:\", model.device)\n",
        "print(\"First 10 weights:\", model.llm.model.embed_tokens.weight[0,:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXBOS3U1tDjg",
        "outputId": "805a6a61-6597-4da4-87ba-d86863e47550"
      },
      "id": "dXBOS3U1tDjg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model device: cuda:0\n",
            "First 10 weights: tensor([-0.0008,  0.0006,  0.0005,  0.0004,  0.0008, -0.0005, -0.0003,  0.0005,\n",
            "         0.0002,  0.0002], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input shapes:\")\n",
        "print(f\"input_ids: {batch['input_ids'].shape}\")\n",
        "print(f\"attention_mask: {batch['attention_mask'].shape}\")\n",
        "print(f\"frames: {batch['frames'].shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwYBPasLnLJ7",
        "outputId": "6bae58b0-d231-49fa-b797-599e621e8980"
      },
      "id": "NwYBPasLnLJ7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shapes:\n",
            "input_ids: torch.Size([1, 10])\n",
            "attention_mask: torch.Size([1, 10])\n",
            "frames: torch.Size([1, 3, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Model device: {model.device}\")\n",
        "print(f\"Input device: {batch['input_ids'].device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhDwp7_YnP8q",
        "outputId": "12f4f5eb-ba6a-4c44-8102-6868c46c23f0"
      },
      "id": "bhDwp7_YnP8q",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model device: cuda:0\n",
            "Input device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "b_0P6gKFV4M1"
      },
      "id": "b_0P6gKFV4M1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "UltTKBkAWvPd",
      "metadata": {
        "id": "UltTKBkAWvPd"
      },
      "source": [
        "# Leveraging the VoRA trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M72_-t_o7EL2",
      "metadata": {
        "id": "M72_-t_o7EL2"
      },
      "source": [
        "### üîß Import VoRATrainer and TrainingArguments\n",
        "\n",
        "We use VoRA‚Äôs built-in `VoRATrainer`, a wrapper around HuggingFace‚Äôs `Trainer`.\n",
        "\n",
        "It supports:\n",
        "- Vision embedding via `<image>` token\n",
        "- LoRA weight updates\n",
        "- Logging and checkpointing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EGhm3Xch7IeT",
      "metadata": {
        "id": "EGhm3Xch7IeT"
      },
      "source": [
        "### ‚öôÔ∏è Define TrainingArguments for Fine-Tuning\n",
        "\n",
        "Key settings:\n",
        "- 1 epoch (for quick demo)\n",
        "- batch size 1\n",
        "- logs every step\n",
        "- saves to `/content/drive/MyDrive/vora-checkpoints`\n",
        "\n",
        "Also enables `wandb_project` logging (optional).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nGn4cLHl7P0T",
      "metadata": {
        "id": "nGn4cLHl7P0T"
      },
      "source": [
        "### üöÄ Train VoRA + BioMistral on VQA-RAD\n",
        "\n",
        "- Uses `VoRATrainer` to fine-tune LoRA adapters on multimodal data\n",
        "- Visual features are injected automatically via `<image>` token + `frames`\n",
        "- Only LoRA + vision adapters are updated (base BioMistral is frozen)\n",
        "\n",
        "Reference: VoRA architecture (Han et al., 2025, Sec. 3.3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RCtgqrwNPJvN",
      "metadata": {
        "id": "RCtgqrwNPJvN"
      },
      "outputs": [],
      "source": [
        "print(model.config)  #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KjhxjzCZjsWr",
      "metadata": {
        "id": "KjhxjzCZjsWr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"PYTORCH_SDP_DISABLE_FLASH_ATTN\"] = \"1\"\n",
        "os.environ[\"PYTORCH_SDP_DISABLE_MEM_EFF_ATTENTION\"] = \"1\"\n",
        "os.environ[\"PYTORCH_SDP_DISABLE_CUDA_SDPA\"] = \"1\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JlBCQiWnRcl6",
      "metadata": {
        "id": "JlBCQiWnRcl6"
      },
      "source": [
        "# Testing section 1 Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1Kwk4WmkGEsS",
      "metadata": {
        "id": "1Kwk4WmkGEsS"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "image = Image.open(\"000001-1.jpg\").convert(\"RGB\")\n",
        "image.show()\n",
        "\n",
        "conversation = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"image\": image},\n",
        "            {\"type\": \"text\", \"text\": \"<image> Describe this image in a sentence.\"}\n",
        "        ]\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3G3QAv3oFsmp",
      "metadata": {
        "id": "3G3QAv3oFsmp"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoProcessor, AutoModelForCausalLM\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"LuwamMajor/vora-biomistral-vqa\", trust_remote_code=True)\n",
        "\n",
        "model_inputs = processor.apply_chat_template(\n",
        "    conversation,\n",
        "    add_generation_prompt=True,\n",
        "    tokenize=True, return_tensors='pt',\n",
        "    return_dict=True).to(model.device)\n",
        "gen_kwargs = {\"max_new_tokens\": 1024, \"eos_token_id\": processor.tokenizer.eos_token_id}\n",
        "\n",
        "with torch.inference_mode():\n",
        "    outputs = model.generate(model_inputs, **gen_kwargs)\n",
        "    output_text = processor.tokenizer.batch_decode(\n",
        "        outputs, skip_special_tokens=True\n",
        "    )\n",
        "    print(output_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JlxoqZFcEMoj",
      "metadata": {
        "id": "JlxoqZFcEMoj"
      },
      "outputs": [],
      "source": [
        "image_tensor = preprocess_image(image).float().to(\"cuda\")\n",
        "\n",
        "# Use trained VoRA adapter to get visual embeddings\n",
        "with torch.no_grad():\n",
        "    vision_embeds = model.vision_embedding(image_tensor)\n",
        "\n",
        "question = \"Based on the xray, is the heart enlarged? Answer yes or no\"\n",
        "prompt = \"<image> \" + question\n",
        "\n",
        "# Tokenize and get text embeddings\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
        "inputs = torch.cat([\n",
        "    torch.tensor([[tokenizer.bos_token_id]]),\n",
        "    inputs,\n",
        "    torch.tensor([[tokenizer.eos_token_id]])\n",
        "], dim=1).to(\"cuda\")\n",
        "\n",
        "text_embeds = model.get_input_embeddings()(inputs)  #\n",
        "\n",
        "# Combine image and text embeddings\n",
        "combined_embeds = torch.cat([vision_embeds, text_embeds], dim=1)\n",
        "attn_mask = torch.ones(combined_embeds.shape[:2], dtype=torch.long, device=\"cuda\")\n",
        "\n",
        "# Use finetuned VoRA model's generate\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        batch=batch,\n",
        "        inputs_embeds=combined_embeds,\n",
        "        vision_placeholder_index = batch.pop(\"vision_placeholder_index\"),\n",
        "        n_frames=1,\n",
        "        attention_mask=attn_mask,\n",
        "        max_new_tokens=40,\n",
        "        do_sample=False,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "# Decode new tokens only\n",
        "generated_tokens = outputs[0][combined_embeds.shape[1]:]\n",
        "answer = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "print(\"üß† VoRA says:\", answer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DrbN_5oNrvEb",
      "metadata": {
        "id": "DrbN_5oNrvEb"
      },
      "source": [
        "# **MODEL LOADING TEST 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_PpbBP_5YTDz",
      "metadata": {
        "id": "_PpbBP_5YTDz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import AutoTokenizer\n",
        "from torchvision import transforms\n",
        "\n",
        "from checkpoint_2.modeling_vora import VoRAForCausalLM\n",
        "from checkpoint_2.configuration_vora import VoRAConfig\n",
        "\n",
        "\n",
        "checkpoint_path = \"/content/vora-checkpoints/checkpoint_2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"BioMistral/BioMistral-7B\", trust_remote_code=True)\n",
        "tokenizer.add_special_tokens({'additional_special_tokens': [\"<image>\"]})\n",
        "print(f\"Tokenizer vocab size: {len(tokenizer)}\") # shoudl be 32001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k4XyAUMOmo6H",
      "metadata": {
        "id": "k4XyAUMOmo6H"
      },
      "outputs": [],
      "source": [
        "# Load config from the checkpoint we saved\n",
        "config = VoRAConfig.from_pretrained(checkpoint_path)\n",
        "\n",
        "# Init model and resize embedding\n",
        "model = VoRAForCausalLM(config)\n",
        "# Load model\n",
        "\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/vora-checkpoints/checkpoint_2\", trust_remote_code=True)\n",
        "\n",
        "#model = VoRAForCausalLM.from_pretrained(\"BioMistral/BioMistral-7B\", trust_remote_code=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aM6vC6p4ptWb",
      "metadata": {
        "id": "aM6vC6p4ptWb"
      },
      "outputs": [],
      "source": [
        "model.get_input_embeddings = lambda: model.llm.model.embed_tokens\n",
        "model.set_input_embeddings = lambda value: setattr(model.llm.model, \"embed_tokens\", value)\n",
        "model.resize_token_embeddings(len(tokenizer))  # Crucial for new tokens, in our case the <image> token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4tACcoHeqqS1",
      "metadata": {
        "id": "4tACcoHeqqS1"
      },
      "outputs": [],
      "source": [
        "# Load actual weights\n",
        "from transformers.modeling_utils import load_sharded_checkpoint\n",
        "\n",
        "load_sharded_checkpoint(model, checkpoint_path)\n",
        "model.eval().cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "udGyff1Cjmv_",
      "metadata": {
        "id": "udGyff1Cjmv_"
      },
      "outputs": [],
      "source": [
        "print(\"Tokenizer vocab size:\", len(tokenizer))\n",
        "print(\"Model embedding shape:\", model.llm.model.embed_tokens.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NGeeiDyYBbh3",
      "metadata": {
        "id": "NGeeiDyYBbh3"
      },
      "outputs": [],
      "source": [
        "def trainer_collate_fn(batch):\n",
        "    collated = {}\n",
        "\n",
        "    collated[\"input_ids\"] = torch.stack([b[\"input_ids\"] for b in batch])\n",
        "    collated[\"attention_mask\"] = torch.stack([b[\"attention_mask\"] for b in batch])\n",
        "    collated[\"labels\"] = torch.stack([b[\"labels\"] for b in batch])\n",
        "    collated[\"pixel_values\"] = torch.stack([b[\"pixel_values\"] for b in batch])\n",
        "    collated[\"frames\"] = torch.stack([b[\"frames\"] for b in batch])\n",
        "\n",
        "\n",
        "    collated[\"n_frames\"] = torch.tensor([b[\"n_frames\"] for b in batch], dtype=torch.long)\n",
        "    collated[\"vision_placeholder_index\"] = torch.tensor([b[\"vision_placeholder_index\"] for b in batch], dtype=torch.long)\n",
        "\n",
        "    #\n",
        "    collated[\"prompt\"] = [b[\"prompt\"] for b in batch]\n",
        "    collated[\"gt\"] = [b[\"gt\"] for b in batch]\n",
        "    collated[\"question\"] = [b[\"question\"] for b in batch]\n",
        "\n",
        "    return collated\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HM1GD8kP_OOg",
      "metadata": {
        "id": "HM1GD8kP_OOg"
      },
      "outputs": [],
      "source": [
        "test_data = VQARADDataset(\n",
        "    vqa_dataset,\n",
        "    split=\"test\",\n",
        "    vision_token=\"<image>\"\n",
        ")\n",
        "inference_dataset = VoRAWrappedDataset(test_data, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "APfF2JgR_aa5",
      "metadata": {
        "id": "APfF2JgR_aa5"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "inference_loader = DataLoader(inference_dataset, batch_size=1, shuffle=False, collate_fn=trainer_collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1k0-XzjbMn8O",
      "metadata": {
        "id": "1k0-XzjbMn8O"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_SDP_BACKEND\"] = \"math\"\n",
        "\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "maO7-tWOutv4",
      "metadata": {
        "id": "maO7-tWOutv4"
      },
      "source": [
        "# **Model response debug**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s4u8BHnIxDo2",
      "metadata": {
        "id": "s4u8BHnIxDo2"
      },
      "outputs": [],
      "source": [
        "# move the vision embedding to CUDA\n",
        "model.vision_embedding = model.vision_embedding.to(\"cuda\")\n",
        "model = model.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jmFiJwygRYvF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "jmFiJwygRYvF",
        "outputId": "ba1096c5-c878-4bd5-83e0-435cbdca7ada"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'image_tensor' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-566956058.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Visual embedding from the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mvision_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvision_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Token embedding from LLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'image_tensor' is not defined"
          ]
        }
      ],
      "source": [
        "question = \"is there evidence of an aortic aneurysm?\"\n",
        "prompt = \"<image> \" + question\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(\"cuda\")\n",
        "input_ids = torch.cat([\n",
        "    torch.tensor([[tokenizer.bos_token_id]], device=\"cuda\"),\n",
        "    input_ids,\n",
        "    torch.tensor([[tokenizer.eos_token_id]], device=\"cuda\")\n",
        "], dim=1)\n",
        "\n",
        "# Visual embedding from the trained model\n",
        "with torch.no_grad():\n",
        "    vision_embeds = model.vision_embedding(image_tensor)\n",
        "\n",
        "# Token embedding from LLM\n",
        "text_embeds = model.llm.model.embed_tokens(input_ids)\n",
        "\n",
        "#combine\n",
        "combined_embeds = torch.cat([vision_embeds, text_embeds], dim=1)\n",
        "attention_mask = torch.ones(combined_embeds.shape[:2], dtype=torch.long, device=\"cuda\")\n",
        "\n",
        "# Generation\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        inputs_embeds=combined_embeds,\n",
        "        attention_mask=attention_mask,\n",
        "        max_new_tokens=10,  #\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "generated_tokens = outputs[0][combined_embeds.shape[1]:]\n",
        "answer = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "print(\"üëÄ Model Answer:\", answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VxBvLyjRTHiS",
      "metadata": {
        "id": "VxBvLyjRTHiS"
      },
      "outputs": [],
      "source": [
        "print(\"Raw tokens:\", outputs[0])\n",
        "print(\"Decoded (raw):\", tokenizer.decode(outputs[0]))\n",
        "print(\"Decoded (clean):\", tokenizer.decode(outputs[0], skip_special_tokens=True).replace(\"</s>\", \"\").strip())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9XTxl61KT8v5",
      "metadata": {
        "id": "9XTxl61KT8v5"
      },
      "outputs": [],
      "source": [
        "print(model.state_dict()['llm.model.layers.0.self_attn.q_proj.lora_A.weight'][0][:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "auk_5afuUGG0",
      "metadata": {
        "id": "auk_5afuUGG0"
      },
      "outputs": [],
      "source": [
        "question = \"Is the heart enlarged? Answer yes or no.\"\n",
        "tokens = tokenizer(question, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    out = model.llm.generate(**tokens, max_new_tokens=30)\n",
        "    print(tokenizer.decode(out[0], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tLHbtzBetOY-",
      "metadata": {
        "id": "tLHbtzBetOY-"
      },
      "source": [
        "# **LORA MERGING TEST**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mlNRjOB7cYQc",
      "metadata": {
        "id": "mlNRjOB7cYQc"
      },
      "outputs": [],
      "source": [
        "print(model.__class__.__name__)\n",
        "print(model.config.llm)\n",
        "print(model.vision_embedding.__class__)\n",
        "print(model.llm.model.layers[0].mlp.gate_proj)    LoRALayer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a54CB09dJTN",
      "metadata": {
        "collapsed": true,
        "id": "8a54CB09dJTN"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "def manually_merge_custom_vora_lora(model):\n",
        "    merged_count = 0\n",
        "\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Linear) and hasattr(module, \"lora_A\") and hasattr(module, \"lora_B\"):\n",
        "            print(f\"Merging LoRA in: {name}\")\n",
        "\n",
        "            # compute delta = B @ A\n",
        "            delta = module.lora_B.weight @ module.lora_A.weight  # shape [out, in]\n",
        "\n",
        "            # merge into the base weight\n",
        "            module.weight.data += delta.to(module.weight.device, dtype=module.weight.dtype)\n",
        "\n",
        "            # remove LoRA modules\n",
        "            del module.lora_A\n",
        "            del module.lora_B\n",
        "\n",
        "            # patch forward method back to default nn.Linear\n",
        "            module.forward = nn.Linear(module.in_features, module.out_features, module.bias is not None).forward\n",
        "\n",
        "            merged_count += 1\n",
        "\n",
        "    print(f\"\\n Manually merged {merged_count} LoRA layers.\")\n",
        "\n",
        "manually_merge_custom_vora_lora(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "owoQEv36mGVT",
      "metadata": {
        "id": "owoQEv36mGVT"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"merged_vora_biomistral\", safe_serialization=False)\n",
        "tokenizer.save_pretrained(\"merged_vora_biomistral\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9FqsfgmWdS-Q",
      "metadata": {
        "id": "9FqsfgmWdS-Q"
      },
      "outputs": [],
      "source": [
        "print(model.llm.model.layers[0].mlp.gate_proj)  # should've ave been plain linear to be rechecked\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76PZTbfJmqxC",
      "metadata": {
        "id": "76PZTbfJmqxC"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load config and tokenizer\n",
        "config = VoRAConfig.from_pretrained(\"merged_vora_biomistral\", trust_remote_code=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"merged_vora_biomistral\")\n",
        "\n",
        "config.vocab_size = len(tokenizer)\n",
        "\n",
        "#\n",
        "model = VoRAForCausalLM(config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qhfcccyBtEYl",
      "metadata": {
        "id": "qhfcccyBtEYl"
      },
      "outputs": [],
      "source": [
        "model.get_input_embeddings = lambda: model.llm.model.embed_tokens\n",
        "model.set_input_embeddings = lambda value: setattr(model.llm.model, \"embed_tokens\", value)\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rr5z_2t7nxI6",
      "metadata": {
        "id": "rr5z_2t7nxI6"
      },
      "outputs": [],
      "source": [
        "print(\"Model vocab size:\", model.config.vocab_size)  #\n",
        "print(model.llm.model.embed_tokens.weight.shape)     #\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BxnnyDfItnsC",
      "metadata": {
        "id": "BxnnyDfItnsC"
      },
      "source": [
        "## Loading the sharded model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Eij4P2JatjrZ",
      "metadata": {
        "id": "Eij4P2JatjrZ"
      },
      "outputs": [],
      "source": [
        "from transformers.modeling_utils import load_sharded_checkpoint\n",
        "\n",
        "load_sharded_checkpoint(model, checkpoint_path)\n",
        "model.eval().cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W1ciOj_AnAw5",
      "metadata": {
        "id": "W1ciOj_AnAw5"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tok = AutoTokenizer.from_pretrained(\"merged_vora_biomistral\")\n",
        "print(len(tok))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XwltZeBTRcP5",
      "metadata": {
        "id": "XwltZeBTRcP5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import AutoTokenizer\n",
        "from torchvision import transforms\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# === 3. Load and preprocess image ===\n",
        "image_path = \"000001-1.jpg\"\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "vision_transform = transforms.Compose([\n",
        "    transforms.Resize((448, 448)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "image_tensor = vision_transform(image).unsqueeze(0).cuda()  # (1,3, 448, 448)\n",
        "\n",
        "# ===Prepare prompt ===\n",
        "question = \"What abnormalities do you see?\"\n",
        "vision_token = \"<image>\"\n",
        "prompt = f\"{vision_token} {question.strip()}\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(model.device)\n",
        "\n",
        "# == Insert image & vision token index ===\n",
        "inputs[\"frames\"] = image_tensor\n",
        "inputs[\"n_frames\"] = torch.tensor([1], device=model.device)\n",
        "inputs[\"prompt\"] = [prompt]\n",
        "inputs[\"gt\"] = [\"\"]  # Dummy ground truth\n",
        "inputs[\"question\"] = [question]\n",
        "inputs[\"vision_placeholder_index\"] = tokenizer.convert_tokens_to_ids(vision_token)\n",
        "\n",
        "# ==Patch attention mask to match visual tokens\n",
        "num_visual_tokens = 196  # VoRA uses 14x14 patches from ViT\n",
        "vision_mask = torch.ones((1, num_visual_tokens), dtype=torch.long).to(model.device)\n",
        "text_mask = inputs[\"attention_mask\"]\n",
        "inputs[\"attention_mask\"] = torch.cat([vision_mask, text_mask], dim=1)\n",
        "\n",
        "required_keys = [\n",
        "    \"input_ids\",\n",
        "    \"attention_mask\",\n",
        "    \"frames\",\n",
        "    \"vision_placeholder_index\",\n",
        "    \"n_frames\",\n",
        "]\n",
        "\n",
        "tensor_inputs = {k: inputs[k] for k in required_keys}\n",
        "\n",
        "\n",
        "with torch.inference_mode():\n",
        "    output = model(**tensor_inputs)\n",
        "    logits = output.logits\n",
        "    next_token = torch.argmax(logits[:, -1:], dim=-1)\n",
        "    decoded = tokenizer.decode(next_token[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"üëÅÔ∏è VoRA says:\", decoded)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bscjNeuG75cP",
      "metadata": {
        "id": "bscjNeuG75cP"
      },
      "outputs": [],
      "source": [
        "print(\"Total sequence length:\", inputs[\"attention_mask\"].shape[-1])\n",
        "print(\"Expected:\", num_visual_tokens + text_mask.shape[-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KAIAMN8N3-Y-",
      "metadata": {
        "id": "KAIAMN8N3-Y-"
      },
      "outputs": [],
      "source": [
        "print(\"input_ids:\", inputs[\"input_ids\"].shape)\n",
        "print(\"frames:\", inputs[\"frames\"].shape)\n",
        "print(\"attention_mask:\", inputs[\"attention_mask\"].shape if \"attention_mask\" in inputs else \"no mask\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JjnXacuc5lLy",
      "metadata": {
        "id": "JjnXacuc5lLy"
      },
      "source": [
        "# **Push to Hub**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PX6gthLH5W80",
      "metadata": {
        "id": "PX6gthLH5W80"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LyhT5kZp5pKU",
      "metadata": {
        "id": "LyhT5kZp5pKU"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "#\n",
        "repo_id = \"LuwamMajor/vora-biomistral-vqa\"\n",
        "\n",
        "#\n",
        "model.push_to_hub(repo_id)\n",
        "\n",
        "# Push tokenizer\n",
        "tokenizer.push_to_hub(repo_id)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9K_uHmpIdzNn",
      "metadata": {
        "id": "9K_uHmpIdzNn"
      },
      "source": [
        "#FOR TESTING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5PvcPA2hdrCN",
      "metadata": {
        "id": "5PvcPA2hdrCN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import requests\n",
        "from io import BytesIO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FmZCFcSvd5nb",
      "metadata": {
        "id": "FmZCFcSvd5nb"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(image):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((448, 448)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5], std=[0.5])  #\n",
        "    ])\n",
        "    if image.mode != \"RGB\":\n",
        "        image = image.convert(\"RGB\")\n",
        "    return transform(image).unsqueeze(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U2AT7x6Xd7lP",
      "metadata": {
        "id": "U2AT7x6Xd7lP"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6TFFIHiHfRzS",
      "metadata": {
        "id": "6TFFIHiHfRzS"
      },
      "outputs": [],
      "source": [
        "image = Image.open(\"000001-1.jpg\").convert(\"RGB\")\n",
        "image_tensor = preprocess_image(image).to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oU4KyQwNfdqP",
      "metadata": {
        "id": "oU4KyQwNfdqP"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "def preprocess_image(image):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((448, 448)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "    ])\n",
        "    if image.mode != \"RGB\":\n",
        "        image = image.convert(\"RGB\")\n",
        "    return transform(image).unsqueeze(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KccUV8RbfrRz",
      "metadata": {
        "id": "KccUV8RbfrRz"
      },
      "outputs": [],
      "source": [
        "image_tensor = preprocess_image(image).half().to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N6xUoI1efvZr",
      "metadata": {
        "id": "N6xUoI1efvZr"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    vision_embeds = model.vision_embedding(image_tensor)  # shape: [1, num_patches, 4096]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rleTrOMvgeYW",
      "metadata": {
        "id": "rleTrOMvgeYW"
      },
      "outputs": [],
      "source": [
        "question = \"Based on the xray, is the heart enlarged? Answer yes or no\"\n",
        "prompt = \"<image> \" + question\n",
        "\n",
        "# Tokenize the prompt\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
        "inputs = torch.cat([\n",
        "    torch.tensor([[tokenizer.bos_token_id]]),\n",
        "    inputs,\n",
        "    torch.tensor([[tokenizer.eos_token_id]])\n",
        "], dim=1).to(\"cuda\")\n",
        "\n",
        "# Get text token embeddings from base model\n",
        "text_embeds = model.llm.model.embed_tokens(inputs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ujc43Q0XgvkH",
      "metadata": {
        "id": "Ujc43Q0XgvkH"
      },
      "outputs": [],
      "source": [
        "combined_embeds = torch.cat([vision_embeds, text_embeds], dim=1)\n",
        "attn_mask = torch.ones(\n",
        "    combined_embeds.shape[:2],\n",
        "    dtype=torch.long,\n",
        "    device=combined_embeds.device\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7_PhrrP7hyVR",
      "metadata": {
        "id": "7_PhrrP7hyVR"
      },
      "outputs": [],
      "source": [
        "print(\"Generated token IDs:\", outputs[0][combined_embeds.shape[1]:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EFZ1cIx0iPrI",
      "metadata": {
        "id": "EFZ1cIx0iPrI"
      },
      "outputs": [],
      "source": [
        "print(\"Vision embedding shape:\", vision_embeds.shape)\n",
        "print(\"Text embedding shape:\", text_embeds.shape)\n",
        "print(\"Combined shape:\", combined_embeds.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DDjFeFGAjfYO",
      "metadata": {
        "id": "DDjFeFGAjfYO"
      },
      "outputs": [],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if \"lora_\" in name:\n",
        "        print(f\"{name}: requires_grad={param.requires_grad}, mean={param.data.mean().item():.6f}\")\n",
        "        break  #\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "piAscJNXJwKA",
      "metadata": {
        "id": "piAscJNXJwKA"
      },
      "outputs": [],
      "source": [
        "from types import MethodType\n",
        "\n",
        "def patched_generate(self, batch, **generate_params):\n",
        "    with torch.amp.autocast(enabled=(self.device != torch.device(\"cpu\")), device_type=self.device.type):\n",
        "        vision_placeholder_index = batch.pop(\"vision_placeholder_index\")\n",
        "        images, n_frames = batch[\"frames\"], batch[\"n_frames\"]\n",
        "\n",
        "        vision_encode_out = self._encode_vision(images, n_frames)\n",
        "\n",
        "        # Unpack all 4 returned items\n",
        "        vision_embeds, vision_atts, vision_targets, vision_extra = vision_encode_out\n",
        "\n",
        "        #Unpack vision embeds\n",
        "        patch_embeds, *rest = vision_embeds\n",
        "\n",
        "        # trying toCast only the tensor\n",
        "        patch_embeds = patch_embeds.to(dtype=torch.float16)\n",
        "\n",
        "        #Repack vision_embeds and vision_encode_out\n",
        "        vision_embeds = (patch_embeds, *rest)\n",
        "        vision_encode_out = (vision_embeds, vision_atts, vision_targets, vision_extra)\n",
        "\n",
        "        inputs_embeds, attention_mask, _, _ = self._concat_embedding(\n",
        "            vision_encode_out, batch, vision_placeholder_index, left_padding=False)\n",
        "\n",
        "    inputs_embeds = inputs_embeds.to(dtype=torch.float16)\n",
        "    attention_mask = attention_mask.to(dtype=torch.bool)\n",
        "\n",
        "    return self.llm.generate(\n",
        "        inputs_embeds=inputs_embeds,\n",
        "        attention_mask=attention_mask,\n",
        "        **generate_params\n",
        "    )\n",
        "\n",
        "model.generate = MethodType(patched_generate, model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4HNiSk5gfPt",
      "metadata": {
        "id": "a4HNiSk5gfPt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers.utils import ModelOutput\n",
        "from checkpoint_2.vora_generation_utils import VoraGenerationMixin\n",
        "\n",
        "def safe_update_model_kwargs_for_generation(self, outputs: ModelOutput, model_kwargs, is_encoder_decoder=False, num_new_tokens=1):\n",
        "    if \"attention_mask\" in model_kwargs and model_kwargs[\"attention_mask\"].ndim == 4:\n",
        "        attention_mask = model_kwargs.pop(\"attention_mask\")\n",
        "\n",
        "        model_kwargs = super(VoraGenerationMixin, self)._update_model_kwargs_for_generation(\n",
        "            outputs, model_kwargs,\n",
        "            is_encoder_decoder=is_encoder_decoder,\n",
        "            num_new_tokens=num_new_tokens\n",
        "        )\n",
        "\n",
        "        bs, _, seq_len, tgt_len = attention_mask.shape\n",
        "        dtype = attention_mask.dtype\n",
        "\n",
        "        try:\n",
        "            min_dtype = torch.finfo(dtype).min\n",
        "        except TypeError:\n",
        "            min_dtype = torch.finfo(torch.float32).min\n",
        "\n",
        "        new_col = attention_mask.new_zeros((bs, 1, seq_len, 1)).fill_(min_dtype)\n",
        "        new_row = attention_mask.new_zeros((bs, 1, 1, tgt_len + 1)).fill_(min_dtype)\n",
        "\n",
        "        attention_mask = torch.cat([\n",
        "            torch.cat([attention_mask, new_col], dim=-1),\n",
        "            new_row\n",
        "        ], dim=2)\n",
        "\n",
        "        model_kwargs[\"attention_mask\"] = attention_mask\n",
        "        return model_kwargs\n",
        "    else:\n",
        "        return super(VoraGenerationMixin, self)._update_model_kwargs_for_generation(\n",
        "            outputs, model_kwargs,\n",
        "            is_encoder_decoder=is_encoder_decoder,\n",
        "            num_new_tokens=num_new_tokens\n",
        "        )\n",
        "\n",
        "# Patch the method on the *class*, not instance (recheck)\n",
        "VoraGenerationMixin._update_model_kwargs_for_generation = safe_update_model_kwargs_for_generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JCfoCz4qV6Pc",
      "metadata": {
        "id": "JCfoCz4qV6Pc"
      },
      "outputs": [],
      "source": [
        "def vqa_infer(image_path, question_text, max_new_tokens=50):\n",
        "    model.eval()\n",
        "\n",
        "    # Load image and transform\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((448, 448)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    img_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    # Tokenize with <image> token\n",
        "    input_text = f\"<image> {question_text}\"\n",
        "    inputs = tokenizer(\n",
        "        input_text,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    ).to(device)\n",
        "\n",
        "    # Get position of <image> token\n",
        "    image_token_id = tokenizer.convert_tokens_to_ids(\"<image>\")\n",
        "    image_token_indices = (inputs[\"input_ids\"] == image_token_id).nonzero(as_tuple=True)\n",
        "\n",
        "    if len(image_token_indices[1]) == 0:\n",
        "        raise ValueError(\"Missing <image> token in input_ids\")\n",
        "\n",
        "    vision_placeholder_index = image_token_indices[1][0]\n",
        "\n",
        "    # build batch with required keyss\n",
        "    batch = {\n",
        "        \"input_ids\": inputs[\"input_ids\"],\n",
        "        \"attention_mask\": inputs[\"attention_mask\"],\n",
        "        \"frames\": img_tensor,\n",
        "        \"n_frames\": torch.tensor([1]).to(device),\n",
        "        \"vision_placeholder_index\": vision_placeholder_index.unsqueeze(0)  # shape [1]\n",
        "    }\n",
        "\n",
        "    # Generate\n",
        "    generated_ids = model.generate(batch=batch, max_new_tokens=max_new_tokens)\n",
        "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KzeTv2A-710U",
      "metadata": {
        "id": "KzeTv2A-710U"
      },
      "source": [
        "* Inference on a Sample from VQA-RAD\n",
        "\n",
        "We will select one test sample and ask the model to generate an answer.  \n",
        "This will be to shows how the fine-tuned VoRA model reasons over medical images + questions. **FIX THIS**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HaMlfWgNOGIM",
      "metadata": {
        "id": "HaMlfWgNOGIM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())\n",
        "#print(torch.backends.cuda.is_built_with_cudnn())\n",
        "print(torch.cuda.get_device_capability())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9sKNJsQ8V6JS",
      "metadata": {
        "id": "9sKNJsQ8V6JS"
      },
      "outputs": [],
      "source": [
        "test_sample = test[0]\n",
        "img = test_sample[\"frames\"]\n",
        "question = test_sample[\"question\"].replace(\"<image>\", \"\").strip()\n",
        "\n",
        "img_pil = transforms.ToPILImage()(img)\n",
        "img_path = \"/content/test_image.jpg\"\n",
        "img_pil.save(img_path)\n",
        "\n",
        "print(\" Question:\", question)\n",
        "print(\" Image path:\", img_path)\n",
        "\n",
        "output = vqa_infer(img_path, question)\n",
        "print(\"Answer:\", output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HUSaUve-7_sB",
      "metadata": {
        "id": "HUSaUve-7_sB"
      },
      "source": [
        "### üìâ Visualize Training Loss\n",
        "\n",
        "Since we are logging loss during training, let's visualize it to assess convergence and detect instability annd also see if the layrs are learning anything useful  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gx-mXi-AV6Ds",
      "metadata": {
        "id": "gx-mXi-AV6Ds"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Example: dummy losses for illustration\n",
        "losses = [2.5, 1.9, 1.3, 1.1, 0.9]\n",
        "steps = list(range(len(losses)))\n",
        "\n",
        "plt.plot(steps, losses, marker='o')\n",
        "plt.title(\"Training Loss over Steps\")\n",
        "plt.xlabel(\"Step\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ywBrH_un8e-q",
      "metadata": {
        "id": "ywBrH_un8e-q"
      },
      "source": [
        "### üß¨ Visualize Embedding Space: Vision vs Text Tokens\n",
        "\n",
        "We project the learned `<image>` embedding and some text token embeddings into 2D using PCA.  \n",
        "This helps understand whether vision and text modalities are distinguishable or aligned in the learned space.\n",
        "\n",
        "Inspired by (Han et al., 2025, Fig. 6).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FJ3tZ9Br8hGu",
      "metadata": {
        "id": "FJ3tZ9Br8hGu"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract embeddings\n",
        "embedding_layer = model.get_input_embeddings().weight.detach().cpu()\n",
        "pca = PCA(n_components=2)\n",
        "embedding_2d = pca.fit_transform(embedding_layer)\n",
        "\n",
        "# Get token positions\n",
        "image_token_id = tokenizer.convert_tokens_to_ids(\"<image>\")\n",
        "text_token_ids = tokenizer.convert_tokens_to_ids([\"what\", \"is\", \"the\", \"finding\", \"in\", \"this\", \"xray\"])\n",
        "\n",
        "# plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(embedding_2d[:, 0], embedding_2d[:, 1], alpha=0.05, label=\"All tokens\")\n",
        "plt.scatter(embedding_2d[text_token_ids, 0], embedding_2d[text_token_ids, 1], color=\"blue\", label=\"Text tokens\")\n",
        "plt.scatter(embedding_2d[image_token_id, 0], embedding_2d[image_token_id, 1], color=\"red\", label=\"<image> token\", s=100)\n",
        "plt.title(\"PCA of Token Embedding Space\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edTCt3RU8yvF",
      "metadata": {
        "id": "edTCt3RU8yvF"
      },
      "source": [
        "### üîç Visualize Cross Attention from Vision Token\n",
        "\n",
        "We inspect the attention weights **from the `<image>` token to other text tokens**.\n",
        "\n",
        "This wll help us understand and reveals how much influence the image has on each part of the question, after training VoRA.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50tp2AO79AzB",
      "metadata": {
        "id": "50tp2AO79AzB"
      },
      "outputs": [],
      "source": [
        "def visualize_attention_weights(input_text):\n",
        "    model.eval()\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.transformer(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            output_attentions=True,\n",
        "            return_dict=True,\n",
        "        )\n",
        "\n",
        "    # Last layer attention from all heads\n",
        "    attn = outputs.attentions[-1][0]  # spposed to beshape: (n_heads, seq_len, seq_len)\n",
        "\n",
        "    image_token_id = tokenizer.convert_tokens_to_ids(\"<image>\")\n",
        "    image_token_index = (inputs[\"input_ids\"][0] == image_token_id).nonzero(as_tuple=True)[0].item()\n",
        "    avg_attn = attn[:, image_token_index, :].mean(0).cpu()\n",
        "\n",
        "    token_labels = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "\n",
        "    plt.figure(figsize=(10, 2))\n",
        "    plt.bar(range(len(token_labels)), avg_attn)\n",
        "    plt.xticks(range(len(token_labels)), token_labels, rotation=45)\n",
        "    plt.title(\"Attention from <image> token to text tokens\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "#Runwith propt\n",
        "visualize_attention_weights(\"<image> what is the abnormality in this chest x-ray?\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A2gxBtsJ9Ex1",
      "metadata": {
        "id": "A2gxBtsJ9Ex1"
      },
      "source": [
        "## üó∫Ô∏è Visualize Positional Embeddings for Vision Tokens\n",
        "\n",
        "VoRA assigns different position embeddings for visual vs text tokens.  \n",
        "Here we plot them to confirm they are **distinct and structured**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5U_wqjxL9H62",
      "metadata": {
        "id": "5U_wqjxL9H62"
      },
      "outputs": [],
      "source": [
        "vision_embed = model.visual_embedding  # AIMv2Embedding\n",
        "pos_embed = vision_embed.pos_embed.detach().cpu()\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.imshow(pos_embed, aspect=\"auto\", cmap=\"viridis\")\n",
        "plt.title(\"Positional Embedding Matrix for Vision Tokens\")\n",
        "plt.xlabel(\"Embedding Dim\")\n",
        "plt.ylabel(\"Patch Index\")\n",
        "plt.colorbar()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cxn7ZNVG9M22",
      "metadata": {
        "id": "cxn7ZNVG9M22"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LYv1_2dB9MuR",
      "metadata": {
        "id": "LYv1_2dB9MuR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4DGGFD3I9MmK",
      "metadata": {
        "id": "4DGGFD3I9MmK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "P-kYHqvK9Maz",
      "metadata": {
        "id": "P-kYHqvK9Maz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "viOTmekw9MIb",
      "metadata": {
        "id": "viOTmekw9MIb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "P9dTvoLMV19W",
      "metadata": {
        "id": "P9dTvoLMV19W"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# Below is a manual training setup  to be complete if the preious appraoch fails"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M4-Cn7Rx3PrA",
      "metadata": {
        "id": "M4-Cn7Rx3PrA"
      },
      "source": [
        "# **Vision Encoder + Projection**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OAdhsG-N3yOB",
      "metadata": {
        "id": "OAdhsG-N3yOB"
      },
      "source": [
        "### Setup ViT Embeddings (Visual Distillation)\n",
        "\n",
        "We load a pre-trained ViT (Base, 16-patch) model as a **visual feature teacher**.\n",
        "\n",
        "Then, a `vision_to_text_proj` linear layer maps ViT‚Äôs 768-dim features to BioMistral‚Äôs 4096-dim space.\n",
        "\n",
        "This mirrors VoRA‚Äôs distillation idea: using a frozen ViT to inject strong visual priors into a text-only LLM (Han et al., 2025, Sec. 3.3).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uq3TnkDa3M6_",
      "metadata": {
        "id": "uq3TnkDa3M6_"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoImageProcessor, AutoModel\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "image_encoder_name = \"google/vit-base-patch16-224\"\n",
        "image_processor = AutoImageProcessor.from_pretrained(image_encoder_name)\n",
        "vit_model = AutoModel.from_pretrained(image_encoder_name).eval().to(device)\n",
        "\n",
        "vision_to_text_proj = nn.Linear(768, model.config.hidden_size).to(device).to(torch.float16)\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_image_embeddings(image_pil):\n",
        "    inputs = image_processor(image_pil, return_tensors=\"pt\").to(device)\n",
        "    print(f\"Inputs dtype before vit_model: {inputs['pixel_values'].dtype}\")\n",
        "    with torch.no_grad():\n",
        "        outputs = vit_model(**inputs) # vit_model is float32, inputs should be float32\n",
        "    # Cast the output of vit_model to float16 before projection\n",
        "    patch_embeds = outputs.last_hidden_state[:, 1:, :].to(torch.float16)\n",
        "    patch_embeds = vision_to_text_proj(patch_embeds.squeeze(0))\n",
        "    return patch_embeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zXYEvHZYUyjB",
      "metadata": {
        "id": "zXYEvHZYUyjB"
      },
      "outputs": [],
      "source": [
        "# to Inspect the model architecture\n",
        "from transformers import AutoModelForCausalLM\n",
        "model = AutoModelForCausalLM.from_pretrained(\"BioMistral/BioMistral-7B\", torch_dtype=\"auto\")\n",
        "print([name for name, _ in model.named_modules()][:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wWazVTctdlFp",
      "metadata": {
        "id": "wWazVTctdlFp"
      },
      "source": [
        "# Dataset Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ouP1VeV2Spi",
      "metadata": {
        "id": "4ouP1VeV2Spi"
      },
      "outputs": [],
      "source": [
        "# Initialize the DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_data,\n",
        "    batch_size=1,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "X9UQsXObZXuP",
      "metadata": {
        "id": "X9UQsXObZXuP"
      },
      "outputs": [],
      "source": [
        "print(\"Tokenizer special tokens:\", tokenizer.special_tokens_map)\n",
        "print(\"Pad token ID:\", tokenizer.pad_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vndBlDpOcQ6C",
      "metadata": {
        "id": "vndBlDpOcQ6C"
      },
      "outputs": [],
      "source": [
        "# checksaved data\n",
        "from datasets import load_from_disk\n",
        "reloaded_ds = load_from_disk(\"vqa_rad\")\n",
        "print(reloaded_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Benchmarking and Expected Performance\n",
        "Finally, let's discuss how our BioMistral-7B + VoRA model performed compared to other\n",
        "multimodal medical models, using metrics from their papers:\n",
        "\n",
        "* LLaVA-Med (Mistral-7B) ‚Äì This model was instruction-tuned on a large biomedical image-text\n",
        "set. After fine-tuning on specific VQA datasets, it achieved 84.2% open and 83.1% closed\n",
        "accuracy on VQA-RAD , and 86.8% open / 91.2% closed on SLAKE , surpassing previous\n",
        "state-of-the-art. It particularly shined on closed (binary) questions in VQA-RAD and PathVQA,\n",
        "outperforming all earlier methods . The authors note it set new SOTA on VQA-RAD and\n",
        "PathVQA closed questions, and on SLAKE open questions . These numbers indicate that a\n",
        "strong multimodal 7B model can exceed 80% accuracy on these benchmarks, with closed\n",
        "questions often easier (yes/no questions can reach ~90%+ accuracy ).\n",
        "\n",
        "\n",
        "* PMC-VQA (MedVInT model) ‚Äì This approach leveraged a huge 227k QA dataset for pretraining\n",
        ". Fine-tuned on VQA-RAD and others, it significantly outperformed prior models. For instance,\n",
        "PMC-VQA‚Äôs best model reported around 81.2% overall accuracy on VQA-RAD and 82.0% on\n",
        "SLAKE in their leaderboard (some figures from their literature: previous SOTA was ~79%\n",
        "overall on VQA-RAD , which their model exceeded). The exact metrics vary by question type,\n",
        "but qualitatively, PMC-VQA/MedVInT improved free-form answer quality by aligning a vision\n",
        "encoder with a LLM.\n",
        "\n",
        "* Other models: Earlier approaches like M2I2 or Q2ATransformer often treated VQA as\n",
        "classification and had accuracies in the 70-80% range on these datasets . For example, one\n",
        "method achieved ~79.2% on VQA-RAD open questions . But those often cannot generate freeform\n",
        "answers outside of the training answer set. Our approach, being generative, can produce\n",
        "open-ended answers and should be compared on metrics like BLEU or recall (the VoRA paper\n",
        "might report ‚Äúrecall‚Äù for open answers , meaning the model‚Äôs answer contains the groundtruth\n",
        "phrase)."
      ],
      "metadata": {
        "id": "uRoHjjz3HPao"
      },
      "id": "uRoHjjz3HPao"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In conclusion, this implementation is a demonstration how to turn BioMistral-7B into a multimodal\n",
        "VQA model using VoRA. We detailed the coding steps to attach visual LoRA adapters, recommended the\n",
        "VQA-RAD dataset (with 1735 chest radiographs and curated QA pairs) for fine-tuning, and provided a\n",
        "minimal inference example. With sufficient training, the resulting model should be capable of\n",
        "answering questions about chest X-rays with accuracy on par with state-of-the-art medical VQA models\n",
        "(approaching or exceeding 80% overall accuracy on benchmarks) , while benefiting from\n",
        "BioMistral‚Äôs in domain knowledge and VoRA‚Äôs efficient architecture."
      ],
      "metadata": {
        "id": "OSGslwGVHz0O"
      },
      "id": "OSGslwGVHz0O"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "BioMistral/BioMistral-7B ¬∑ Hugging Face\n",
        "https://huggingface.co/BioMistral/BioMistral-7B\n",
        "\n",
        "[2503.20680] Vision as LoRA\n",
        "https://arxiv.org/abs/2503.20680\n",
        "\n",
        "\n",
        "VoRA (Vision as LoRA) introduces a novel approach to integrating visual understanding\n",
        "into Large Language Models by embedding vision directly through Low-Rank Adaptation (LoRA) layers.\n",
        "| Aleef Mahmud\n",
        "https://www.linkedin.com/posts/aleefmahmud_vision-as-lora-vora-activity-7336790456988536833-CESH\n",
        "\n",
        "\n",
        "flaviagiammarino/vqa-rad ¬∑ Datasets at Hugging Face\n",
        "https://huggingface.co/datasets/flaviagiammarino/vqa-rad\n",
        "\n",
        "\n",
        "GitHub - Hon-Wong/VoRA: [Fully open] [Encoder-free MLLM] Vision as LoRA\n",
        "https://github.com/Hon-Wong/VoRA\n",
        "\n",
        "\n",
        "[2306.00890] LLaVA-Med: Training a Large Language-and-Vision\n",
        "Assistant for Biomedicine in One Day\n",
        "https://ar5iv.labs.arxiv.org/html/2306.00890\n",
        "\n",
        "\n",
        "[2305.10415] PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering\n",
        "https://arxiv.org/abs/2305.10415\n",
        "\n",
        "\n",
        "config.json ¬∑ BioMistral/BioMistral-7B at main\n",
        "https://huggingface.co/BioMistral/BioMistral-7B/blob/main/config.\n",
        "\n",
        "\n",
        "config.json ¬∑ Hon-Wong/VoRA-7B-Base at main\n",
        "https://huggingface.co/Hon-Wong/VoRA-7B-Base/blob/main/config.json\n",
        "\n",
        "\n",
        "Hon-Wong/VoRA-7B-Instruct ¬∑ Hugging Face\n",
        "https://huggingface.co/Hon-Wong/VoRA-7B-Instruct\n"
      ],
      "metadata": {
        "id": "KsgSehfrIIZr"
      },
      "id": "KsgSehfrIIZr"
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V6E1",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fe91fee482cf4dd48db8d6995e3b7393": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1da3504f837b44da85683ba108875a52",
              "IPY_MODEL_e8a1027638b0475c8e19cd46e6cb7d81",
              "IPY_MODEL_4d557a816e6347d99330bbcd1577070a"
            ],
            "layout": "IPY_MODEL_63283258f75c4328899ca59eae50336f"
          }
        },
        "1da3504f837b44da85683ba108875a52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bcd0148fd6a046829f8d9c36814b676e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_79aee285671848af8c72d85dff0bd4fa",
            "value": "Saving‚Äáthe‚Äádataset‚Äá(1/1‚Äáshards):‚Äá100%"
          }
        },
        "e8a1027638b0475c8e19cd46e6cb7d81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eff2c2883fea4869b36093f705724de0",
            "max": 1793,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ed9fa821cd1b4f6896c79ed25130b238",
            "value": 1793
          }
        },
        "4d557a816e6347d99330bbcd1577070a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31c7a4829d824fd5baf67b01e02fd45d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f32c33fe2ec04c0baf8bf7dab08cf38a",
            "value": "‚Äá1793/1793‚Äá[00:00&lt;00:00,‚Äá3665.15‚Äáexamples/s]"
          }
        },
        "63283258f75c4328899ca59eae50336f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcd0148fd6a046829f8d9c36814b676e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79aee285671848af8c72d85dff0bd4fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eff2c2883fea4869b36093f705724de0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed9fa821cd1b4f6896c79ed25130b238": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "31c7a4829d824fd5baf67b01e02fd45d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f32c33fe2ec04c0baf8bf7dab08cf38a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a94f4d810ad6497fb2c8880f61192ce5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_79092b168d1140e1b136a8eb2c74682d",
              "IPY_MODEL_642a1fafce5e4dc799589be123e6c41c",
              "IPY_MODEL_919ce805855b4b73911931c62378f617"
            ],
            "layout": "IPY_MODEL_6e8beb4246a14d3699ed1dfaad4b999a"
          }
        },
        "79092b168d1140e1b136a8eb2c74682d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_316391425d374cc68549c92800e039be",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f68dc7cb5728457dadf8bf8d3192e441",
            "value": "Saving‚Äáthe‚Äádataset‚Äá(1/1‚Äáshards):‚Äá100%"
          }
        },
        "642a1fafce5e4dc799589be123e6c41c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b84934f916df492690f416a1f5822993",
            "max": 451,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a718a2bc3ec745a88c75301cb834b426",
            "value": 451
          }
        },
        "919ce805855b4b73911931c62378f617": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a703158f5cd48268b96afa4b6dbe20a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_65c8b5421efc4c45bf41038040a703b8",
            "value": "‚Äá451/451‚Äá[00:00&lt;00:00,‚Äá4141.95‚Äáexamples/s]"
          }
        },
        "6e8beb4246a14d3699ed1dfaad4b999a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "316391425d374cc68549c92800e039be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f68dc7cb5728457dadf8bf8d3192e441": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b84934f916df492690f416a1f5822993": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a718a2bc3ec745a88c75301cb834b426": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8a703158f5cd48268b96afa4b6dbe20a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65c8b5421efc4c45bf41038040a703b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2469e29021454e4fad31abddfabdef8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_de7af2dcea6c43e48e7c37c2cd23ed4c",
              "IPY_MODEL_b6bc1f8c14e445a3a9a57132590f8ca4",
              "IPY_MODEL_bea25870ebdb4f109ba1894294225105"
            ],
            "layout": "IPY_MODEL_2ccd59d8936542e3a52ada363c8456c0"
          }
        },
        "de7af2dcea6c43e48e7c37c2cd23ed4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fcc984cdf4349f2a31032deff551e08",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e9c98662b75c4f028be5367e5b400625",
            "value": "preprocessor_config.json:‚Äá100%"
          }
        },
        "b6bc1f8c14e445a3a9a57132590f8ca4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f356f2c6fa7f42858f5ba789c057b344",
            "max": 160,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_129e26d0c5b14e618a74ac9e4f37a13a",
            "value": 160
          }
        },
        "bea25870ebdb4f109ba1894294225105": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11ed5411b1f84ca68bd6fce51b124772",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_33d6504a16c6424eb8249bbf65d9cc94",
            "value": "‚Äá160/160‚Äá[00:00&lt;00:00,‚Äá20.2kB/s]"
          }
        },
        "2ccd59d8936542e3a52ada363c8456c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fcc984cdf4349f2a31032deff551e08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9c98662b75c4f028be5367e5b400625": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f356f2c6fa7f42858f5ba789c057b344": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "129e26d0c5b14e618a74ac9e4f37a13a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "11ed5411b1f84ca68bd6fce51b124772": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33d6504a16c6424eb8249bbf65d9cc94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}